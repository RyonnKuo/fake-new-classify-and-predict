{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2077866c",
   "metadata": {
    "id": "2077866c"
   },
   "source": [
    "# **1132_MIS581ç¤¾ç¾¤åª’é«”åˆ†æ_æœŸæœ«å°ˆæ¡ˆ_ç¬¬ä¸€çµ„**\n",
    "- æŒ‡å°æ•™æˆï¼šé»ƒä¸‰ç›Š\n",
    "- ã€€ã€€çµ„å“¡ï¼šéƒ­å±•å·N124020012ã€å‚…æ‰å®¹N124320030ã€èŠç­‘é›…N124320004ã€ææ˜å®¹N124320016\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68389ee3",
   "metadata": {},
   "source": [
    "# **å°ˆæ¡ˆåç¨±ï¼šå¤šå±¤æ¬¡ç‰¹å¾µèåˆä¹‹å‡æ–°èåµæ¸¬èˆ‡ä¸»é¡Œåˆ†æç³»çµ±**\n",
    "## **å°ˆæ¡ˆç›®æ¨™**\n",
    "1. ç²¾æº–åˆ¤æ–·æ–°èæˆ–æ¨æ–‡ä¹‹çœŸå½\n",
    "2. å‰–æå‡æ–°èå¸¸è¦‹ä¸»é¡Œèˆ‡é—œéµå­—\n",
    "3. çµåˆå¤šæ¨¡å‹å”ä½œ(å‚³çµ± ML Ã— BERT/SBERT Ã— LLM)ï¼Œé©—è­‰ä¸åŒç‰¹å¾µçµ„åˆèˆ‡åˆ†é¡å™¨æ•ˆæœï¼Œå»ºç«‹å¯æ“´å……çš„åµæ¸¬æµç¨‹\n",
    "## **å°ˆæ¡ˆæµç¨‹**\n",
    "1. è³‡æ–™è’é›†èˆ‡å‰è™•ç†\n",
    "2. å„é …ç‰¹å¾µæå–ï¼šNERç‰¹å¾µã€æƒ…ç·’ç‰¹å¾µã€ç¶œåˆç‰¹å¾µå·¥ç¨‹åŠåˆ†é¡æ¨¡å‹è©•ä¼°\n",
    "3. ä¸»é¡Œæ¨¡å‹BERTopic\n",
    "4. LLM å¤šæ¨¡å‹è£æ±º\n",
    "5. æœ€çµ‚æ•´åˆè©•ä¼°\n",
    "## **è³‡æ–™ä¾†æº**\n",
    "1. fake-and-real-news-dataset (ISOT Fake News detection dataset) https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset\n",
    "2. Twitter Dataset (Referenced from CIC Truth Seeker Dataset 2023) https://www.kaggle.com/datasets/sudishbasnet/truthseekertwitterdataset2023/data?select=Truth_Seeker_Model_Dataset.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950fcec1",
   "metadata": {
    "id": "950fcec1"
   },
   "source": [
    "# **å°ˆæ¡ˆåŸ·è¡Œ**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ade977",
   "metadata": {},
   "source": [
    "## ä¸€ã€å¥—ä»¶å®‰è£èˆ‡å¼•å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kHUIVdFdFJao",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 134968,
     "status": "ok",
     "timestamp": 1749735935449,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "kHUIVdFdFJao",
    "outputId": "deb6b7b0-f9dc-46cd-af8d-cd7666eb0045"
   },
   "outputs": [],
   "source": [
    "# ğŸ”§ å®‰è£æ ¸å¿ƒè³‡æ–™è™•ç†èˆ‡æ¨¡å‹å¥—ä»¶\n",
    "!pip install pandas numpy scikit-learn matplotlib seaborn tqdm nltk vaderSentiment empath tabulate\n",
    "# ğŸ” å®‰è£ BERT ç›¸é—œï¼ˆtransformers, pipelineï¼‰\n",
    "!pip install transformers\n",
    "# ğŸ¤– å®‰è£å‘½åå¯¦é«”è¾¨è­˜ç”¨é è¨“ç·´æ¨¡å‹\n",
    "!pip install torch\n",
    "# ğŸ§  å®‰è£æƒ…ç·’åˆ†æå¾®èª¿æ¨¡å‹\n",
    "!pip install sentence-transformers\n",
    "# ğŸ“Š å®‰è£ä¸»é¡Œå»ºæ¨¡ï¼šBERTopic + HDBSCANï¼ˆæ”¯æ´ clusteringï¼‰\n",
    "!pip install bertopic hdbscan\n",
    "# ğŸ—‚ å­—é«”è¨­å®šç”¨ï¼ˆå¦‚ä½ åŠ è¼‰äº†è‡ªè¨‚å­—é«”ï¼‰\n",
    "!pip install fonttools\n",
    "!pip install --upgrade nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6tH_lAhbB52g",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1230,
     "status": "ok",
     "timestamp": 1749735936684,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "6tH_lAhbB52g",
    "outputId": "3ae7508e-acfd-43ae-a181-896ef1253e17"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4b00e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU åŠ é€Ÿ\n",
    "\n",
    "import torch\n",
    "\n",
    "# GPU åŠ é€Ÿ\n",
    "print(\"=== PyTorch GPU åŠ é€Ÿç’°å¢ƒæª¢æŸ¥ ===\")\n",
    "print(f\"PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"PyTorch ç·¨è­¯çš„ CUDA ç‰ˆæœ¬: {torch.version.cuda}\")\n",
    "print(f\"æ˜¯å¦æ”¯æ´ CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "try:\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        current_gpu = torch.cuda.current_device()\n",
    "        device_name = torch.cuda.get_device_name(current_gpu)\n",
    "\n",
    "        print(f\"åµæ¸¬åˆ° {gpu_count} å€‹ GPU\")\n",
    "        print(f\"ç•¶å‰ä½¿ç”¨çš„ GPUï¼š{device_name}\")\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        raise RuntimeError(\"CUDA ä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨ CPU\")\n",
    "except Exception as e:\n",
    "    print(f\"ç„¡æ³•ä½¿ç”¨ GPUï¼š{e}\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"pytorchå¯ç”¨çš„GPUç‚ºï¼š{device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784bf351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === NLP / è³‡æ–™è™•ç†åŸºç¤ ===\n",
    "import os, re, ssl, json, concurrent.futures, warnings, itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# NLTK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.data.path.extend([\n",
    "    '/usr/nltk_data', '/usr/local/nltk_data', '/usr/share/nltk_data',\n",
    "    '/usr/local/share/nltk_data', '/root/nltk_data'\n",
    "])\n",
    "\n",
    "# === Scikit-learn åŸºç¤å·¥å…· ===\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, StratifiedKFold, cross_validate)\n",
    "from sklearn.feature_extraction.text import (\n",
    "    TfidfVectorizer, CountVectorizer)\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, f1_score,\n",
    "    accuracy_score, precision_score, recall_score, make_scorer)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import clone\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# å‚³çµ±åˆ†é¡å™¨\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# === é€²éš NLP / å‘é‡åŒ– ===\n",
    "from sentence_transformers import SentenceTransformer          # SBERT\n",
    "from transformers import (\n",
    "    pipeline, AutoTokenizer, AutoModelForTokenClassification)\n",
    "\n",
    "# === ä¸»é¡Œæ¨¡å‹ / èšé¡ ===\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# === è¦–è¦ºåŒ– ===\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.font_manager import fontManager\n",
    "fontManager.addfont('./public/TaipeiSansTCBeta-Regular.ttf')\n",
    "plt.rcParams['font.sans-serif'] = ['Taipei Sans TC Beta']\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "# === VADER & Empath (æƒ…ç·’ / è©å…¸) ===\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "try:\n",
    "    from empath import Empath\n",
    "    empath_available = True\n",
    "    empath_lexicon = Empath()\n",
    "except ModuleNotFoundError:\n",
    "    empath_available = False\n",
    "    warnings.warn(\"Empath å°šæœªå®‰è£ï¼Œå°‡ç•¥éç›¸é—œç‰¹å¾µã€‚\")\n",
    "\n",
    "# === å…¶ä»–è¼”åŠ© ===\n",
    "from tabulate import tabulate\n",
    "warnings.filterwarnings('ignore')  # é¿å…é›œè¨Šè¨Šæ¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b68d7c7",
   "metadata": {},
   "source": [
    "### åŒ¯å…¥fake-and-real-news-datasetçœŸå‡æ–°èè³‡æ–™é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b54574",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 248,
     "status": "ok",
     "timestamp": 1749735938072,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "59b54574",
    "outputId": "5392bcac-87d5-4327-b9fb-5b10e98dc092"
   },
   "outputs": [],
   "source": [
    "# è¼‰å…¥fake-and-real-news-datasetè³‡æ–™é›†\n",
    "fake_df = pd.read_csv('./raw_data/fake.csv')            \n",
    "true_df = pd.read_csv('./raw_data/true.csv')            \n",
    "\n",
    "# åˆä½µ title å’Œ text æˆæ–°çš„ text æ¬„ä½\n",
    "fake_df['text'] = fake_df['title'].astype(str) + \" \" + fake_df['text'].astype(str)\n",
    "true_df['text'] = true_df['title'].astype(str) + \" \" + true_df['text'].astype(str)\n",
    "\n",
    "# åŠ ä¸Š label æ¬„ä½\n",
    "fake_df['label'] = 1\n",
    "true_df['label'] = 0\n",
    "\n",
    "# å–å‰ 1000 ç­†\n",
    "# dataNum = 1000\n",
    "# news_data = pd.concat([fake_df.iloc[:dataNum], true_df.iloc[:dataNum]], ignore_index=True)\n",
    "# news_data = fake_df\n",
    "\n",
    "# å–ä¸€åŠ(è³‡æ–™é‡éå¤§)\n",
    "fake_half = fake_df.sample(frac=0.1, random_state=42)\n",
    "true_half = true_df.sample(frac=0.1, random_state=42)\n",
    "news_data = pd.concat([fake_half, true_half], ignore_index=True)\n",
    "\n",
    "# ç§»é™¤ç©ºå€¼ä¸¦åªä¿ç•™ text å’Œ label æ¬„ä½\n",
    "news_data = news_data[news_data['text'].notna()].reset_index(drop=True)\n",
    "news_data = news_data[['text', 'label']]\n",
    "\n",
    "# æª¢æŸ¥å„é¡åˆ¥æ•¸é‡\n",
    "print(news_data['label'].value_counts())\n",
    "print(news_data.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a8c468",
   "metadata": {
    "id": "18a8c468"
   },
   "source": [
    "### åŒ¯å…¥Twitter Datasetæ¨ç‰¹çœŸå‡æ¨æ–‡è³‡æ–™é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7806f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1749735938208,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "1c7806f2",
    "outputId": "ef86ba43-79ee-452b-e3c0-ed6fc9b7522e"
   },
   "outputs": [],
   "source": [
    "# æ¨ç‰¹è³‡æ–™è™•ç†(ç‚ºäº†å˜—è©¦è§£æ±º åŠ å…¥TF-IDFéæ“¬åˆ&embedéå¼· å¯èƒ½æ˜¯å› ç‚ºæ–‡æœ¬ç‰¹å¾µå¤ªæ˜é¡¯çš„å•é¡Œ)\n",
    "tweet_df = pd.read_csv(\n",
    "    './raw_data/Truth_Seeker_Model_Dataset_unindex.csv', encoding='ISO-8859-1')\n",
    "tweet_data = tweet_df[['BinaryNumTarget',\n",
    "                       'tweet', '5_label_majority_answer']].copy()\n",
    "\n",
    "# æ¸…ç†\n",
    "tweet_data = tweet_data.dropna()\n",
    "tweet_data = tweet_data[~tweet_data['tweet'].str.contains('#REF!', na=False)]\n",
    "valid_labels = ['Agree', 'Mostly Agree']\n",
    "tweet_data = tweet_data[tweet_data['5_label_majority_answer'].isin(\n",
    "    valid_labels)]\n",
    "\n",
    "# ç§»é™¤ 5_label_majority_answer æ¬„ä½ï¼Œä¸¦é‡æ–°å‘½åæ¬„ä½\n",
    "tweet_data = tweet_data.rename(\n",
    "    columns={'BinaryNumTarget': 'label', 'tweet': 'text'})\n",
    "tweet_data = tweet_data[['text', 'label']]\n",
    "\n",
    "tweet_data_num = 50  # å–nç­†\n",
    "# å–æ¯”ä¾‹\n",
    "min_half = int(min(tweet_data['label'].value_counts()) / 10)\n",
    "# tweet_data = tweet_data.groupby('label').apply(\n",
    "#     lambda x: x.sample(n=min(len(x), min_half), random_state=42)\n",
    "# ).reset_index(drop=True)\n",
    "tweet_data = tweet_data.groupby('label').apply(\n",
    "    lambda x: x.sample(n=min_half, random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(tweet_data['label'].value_counts())\n",
    "print(tweet_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3ab861",
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1749735938240,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "4c3ab861"
   },
   "outputs": [],
   "source": [
    "# åˆä½µå…©ä»½ä¸åŒä¾†æºè³‡æ–™é›†\n",
    "data = pd.concat([news_data, tweet_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7b5ac9",
   "metadata": {
    "id": "3b7b5ac9"
   },
   "source": [
    "**éœ€è¦åšæ–‡æœ¬é è™•ç†å—?**\n",
    "\n",
    "ç›®çš„:\n",
    "- å»ºç«‹åˆ†é¡å™¨ä¾†é æ¸¬çœŸå‡æ–°è -> (TF-IDF + åˆ†é¡æ¨¡å‹éœ€è¦ä¹¾æ·¨çš„è³‡æ–™ï¼Œæœ‰å¹«åŠ©)\n",
    "- åˆ†æNER çµæœèˆ‡èªæ„åˆ†ä½ˆ -> (æœƒç ´å£èªæ„)\n",
    "- å»ºç«‹ä¸»é¡Œæ¨¡å‹ä¾†æ¢ç´¢èªæ„ä¸»é¡Œï¼ˆBERTopic -> (æœƒç ´å£èªæ„)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e324aa",
   "metadata": {
    "executionInfo": {
     "elapsed": 1467,
     "status": "ok",
     "timestamp": 1749735939726,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "d5e324aa"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "punct_pattern = re.compile(r\"[^a-z ]\")\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = punct_pattern.sub(\" \", text)\n",
    "    # ç”¨ preserve_line=True é¿é–‹ punkt_tab\n",
    "    tokens = word_tokenize(text, preserve_line=True)\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(w)\n",
    "        for w in tokens\n",
    "        if w not in stop_words and len(w) > 1\n",
    "    ]\n",
    "    return tokens\n",
    "\n",
    "data['tokens'] = data['text'].astype(str).apply(preprocess)\n",
    "data['clean_text'] = data['tokens'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68ddd12",
   "metadata": {
    "id": "e68ddd12"
   },
   "source": [
    "## äºŒã€å„é …ç‰¹å¾µè™•ç†èˆ‡åˆ†é¡å™¨è©•ä¼°è¡¨ç¾\n",
    "### NERç‰¹å¾µè™•ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29def4a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 328342,
     "status": "ok",
     "timestamp": 1749736268119,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "29def4a1",
    "outputId": "0c7820aa-6dfd-44e1-be4e-230ce207a5b5"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from hdbscan import HDBSCAN\n",
    "from transformers import BertTokenizerFast, AutoTokenizer, AutoModelForTokenClassification, AutoModelForSequenceClassification, pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# è¼‰å…¥æ¨¡å‹èˆ‡ tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", device=0)\n",
    "\n",
    "# å»ºç«‹ NER çµæœåˆ—è¡¨\n",
    "ner_rows = []\n",
    "\n",
    "# åˆ†åˆ‡å­—ä¸²\n",
    "def split_text(text, chunk_size=512):\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# é‡å°æ¯ç¯‡æ–‡ç« è·‘ NERï¼ˆå¯ç”¨ tqdm é¡¯ç¤ºé€²åº¦æ¢ï¼‰\n",
    "for idx, text in tqdm(data['text'].astype(str).items()):\n",
    "    try:\n",
    "        chunks = split_text(text)\n",
    "        all_ents = []\n",
    "        for chunk in chunks:\n",
    "            all_ents.extend(ner_pipeline(chunk))  # å°æ¯æ®µè·‘ NER\n",
    "        for ent in all_ents:\n",
    "            ner_rows.append({\n",
    "                \"index\": idx,\n",
    "                \"entity\": ent['entity_group'],  # e.g., PER, LOC\n",
    "                \"word\": ent['word'],\n",
    "                \"score\": ent['score']\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error at idx {idx}: {e}\")\n",
    "\n",
    "# å»ºç«‹ DataFrame\n",
    "ner_df = pd.DataFrame(ner_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875c0283",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1749736268179,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "875c0283",
    "outputId": "46797349-fcf9-44d0-ae61-13cfa2979d86"
   },
   "outputs": [],
   "source": [
    "ner_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbf7dd1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "executionInfo": {
     "elapsed": 454,
     "status": "ok",
     "timestamp": 1749736268646,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "2cbf7dd1",
    "outputId": "210344d9-aaee-415d-f07d-d334f88c2125"
   },
   "outputs": [],
   "source": [
    "# æ•´åˆ label\n",
    "merged_df = ner_df.merge(data[['label']], left_on='index', right_index=True)\n",
    "\n",
    "# èšåˆæ‰€æœ‰ entity é¡å‹çš„å‡ºç¾æ¬¡æ•¸\n",
    "entity_counts_all = (\n",
    "    merged_df.groupby(['index', 'entity'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)  # å¾—åˆ°æ¯ç¯‡æ–‡ç« å„é¡å¯¦é«”æ•¸\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# åˆä½µ label\n",
    "entity_counts_all = entity_counts_all.merge(data[['label']], left_on='index', right_index=True)\n",
    "\n",
    "# å»ºæ¨¡æ¬„ä½é¸æ“‡ï¼šæ‰€æœ‰å¯¦é«”é¡åˆ¥æ¬„ä½ï¼ˆæ’é™¤ index, labelï¼‰\n",
    "feature_cols = [col for col in entity_counts_all.columns if col not in ['index', 'label']]\n",
    "kmeans_fit_pred_data = entity_counts_all[feature_cols]\n",
    "\n",
    "# åš KMeans èšé¡\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "entity_counts_all['cluster'] = kmeans.fit_predict(kmeans_fit_pred_data)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(kmeans_fit_pred_data)\n",
    "entity_counts_all['PC1'] = X_pca[:, 0]\n",
    "entity_counts_all['PC2'] = X_pca[:, 1]\n",
    "# è¦–è¦ºåŒ–èšé¡çµæœ\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=entity_counts_all,\n",
    "    x='PC1', y='PC2', hue='cluster', style='label',\n",
    "    palette='Set2', s=100\n",
    ")\n",
    "\n",
    "plt.title('NER ç‰¹å¾µçš„ä¸»æˆåˆ†åˆ†æ + KMeans èšé¡')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ssZp0y9GbvTr",
   "metadata": {
    "id": "ssZp0y9GbvTr"
   },
   "source": [
    "1. ä½¿ç”¨NERç‰¹å¾µé€²è¡ŒKMeansèšé¡ï¼Œç„¡ç›£ç£å­¸ç¿’è‡ªå‹•åˆ†æˆå…©ç¾¤ï¼Œä¸Šåœ–ç‚ºæ¨¡å‹å‰çš„æ¢ç´¢æ€§è³‡æ–™åˆ†æ(EDA)çµæœï¼Œè§€å¯Ÿçµæœ:KMeansèšé¡æœ‰éƒ¨åˆ†æˆåŠŸèšå‡ºå‡æ–°èç¾¤ï¼Œç¶ è‰²cluster1å¹¾ä¹éƒ½æ˜¯å‰å‰ç‚ºå‡æ–°èç¾¤ï¼Œæ©˜è‰²cluster0åŒ…å«è¼ƒå¤šçœŸæ–°èèˆ‡éƒ¨åˆ†å‡æ–°èã€‚\n",
    "2. çµè«–ï¼šåˆ†ç¾¤é‡ç–Šæ˜é¡¯ï¼Œæ•´é«”åˆ†ç¾¤æ•ˆæœä¸ç®—éå¸¸å¥½ï¼Œä½†åˆæ­¥åˆ¤æ–·NERæœ‰å€åˆ¥èƒ½åŠ›ï¼Œéœ€çµåˆæ›´å¤šç‰¹å¾µé€²è¡Œå¤šæ¨¡æ…‹èšé¡æ¨¡å‹è©•ä¼°!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c9def6",
   "metadata": {},
   "source": [
    "### å¤šæ¨¡æ…‹ç‰¹å¾µ+åˆ†é¡å™¨è©•ä¼°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e6e5bf",
   "metadata": {
    "id": "74e6e5bf"
   },
   "source": [
    "#### A. ä½¿ç”¨ã€Œå‘½åå¯¦é«”è­˜åˆ¥(NER)ã€ä¾†è¾¨è­˜çœŸå‡æ–°è\n",
    "ä½¿ç”¨NERç•¶ä½œç‰¹å¾µï¼Œæå–å‡ºçš„`äººå`ã€`çµ„ç¹”`ã€`åœ°åæ•¸é‡`ä½œç‚ºè©å½™ç‰¹å¾µï¼ŒåŠ ä¸Š`4å€‹åˆ†é¡å™¨(LogisticRegressionã€DecisionTreeã€SVMã€RandomForest)`è©•ä¼°çµæœä¾†åˆ†é¡çœŸå‡æ–°èï¼Œé æ¸¬é€™ç¯‡æ–°èæ˜¯çœŸ/å‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d807d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1002,
     "status": "ok",
     "timestamp": 1749736269642,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "a8d807d7",
    "outputId": "656d47cd-a303-4e63-d0e1-d01087815c65"
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# â”€â”€ Part 1ï¼šNERç‰¹å¾µæ¨¡å‹æ¯”è¼ƒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# === å»ºç«‹ç‰¹å¾µï¼ˆNER ä¾‹å­ï¼‰ ===\n",
    "entity_counts = ner_df.groupby(['index', 'entity']).size().unstack(fill_value=0)\n",
    "data_with_ner = data.copy()\n",
    "data_with_ner = data_with_ner.join(entity_counts, how='left').fillna(0)\n",
    "\n",
    "X = data_with_ner[['PER', 'ORG', 'LOC']]\n",
    "y = data_with_ner['label']\n",
    "\n",
    "# === åˆ†é¡å™¨åˆ—è¡¨ ===\n",
    "# classifiers = {\n",
    "#     \"LogReg\": LogisticRegression(max_iter=1000),\n",
    "#     \"DecisionTree\": DecisionTreeClassifier(),\n",
    "#     \"SVM\": svm.SVC(probability=True),\n",
    "#     \"RandomForest\": RandomForestClassifier(random_state=42)\n",
    "# }\n",
    "classifiers = {\n",
    "    \"LogReg\": LogisticRegression(max_iter=1000),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(),\n",
    "    \"SVM\": svm.SVC(probability=True),\n",
    "    \"RandomForest\": RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# === K-fold è¨­å®š ===\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# å„²å­˜å¹³å‡ f1-score çµæœ\n",
    "results = []\n",
    "\n",
    "# === åŸ·è¡Œäº¤å‰é©—è­‰ä¸¦å°å‡ºæ¯å€‹æ¨¡å‹å ±å‘Š ===\n",
    "for name, model in classifiers.items():\n",
    "    print(f\"\\n=== {name} åˆ†é¡çµæœï¼ˆ5-foldï¼‰ ===\")\n",
    "    y_true_all, y_pred_all = [], []\n",
    "    fold_f1_scores = []\n",
    "\n",
    "    for train_idx, test_idx in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        clf = clone(model)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        y_true_all.extend(y_test)\n",
    "        y_pred_all.extend(y_pred)\n",
    "\n",
    "        fold_f1_scores.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "    avg_f1 = np.mean(fold_f1_scores)\n",
    "    print(classification_report(\n",
    "        y_true_all, y_pred_all,\n",
    "        target_names=[\"çœŸæ–°è\", \"å‡æ–°è\"],\n",
    "        digits=2\n",
    "    ))\n",
    "\n",
    "    results.append({\n",
    "        \"classifier\": name,\n",
    "        \"f1_weighted\": avg_f1\n",
    "    })\n",
    "\n",
    "# === æ¯”è¼ƒçµæœè¡¨æ ¼ ===\n",
    "result_df = pd.DataFrame(results).sort_values(by=\"f1_weighted\", ascending=False).reset_index(drop=True)\n",
    "print(\"ğŸ å„æ¨¡å‹æ¯”è¼ƒï¼š\")\n",
    "print(tabulate(result_df, headers=\"keys\", tablefmt=\"fancy_grid\"))\n",
    "\n",
    "# === æ‰¾å‡ºæœ€ä½³æ¨¡å‹ ===\n",
    "best = result_df.iloc[0]\n",
    "print(f\"\\nğŸ† æœ€ä½³åˆ†é¡å™¨ç‚ºï¼š{best['classifier']}ï¼Œweighted F1 = {best['f1_weighted']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1e1c16",
   "metadata": {
    "id": "7b1e1c16"
   },
   "source": [
    "+ NERæå–ç‰¹å¾µé æ¸¬çµæœå°šå¯ï¼ŒNERç‰¹å¾µå°çœŸå‡æ–°èè¾¨è­˜æœ‰ä¸€å®šç¨‹åº¦ä½œç”¨ï¼Œ`æœ€ä½³åˆ†é¡å™¨ï¼šSVM`ï¼Œ`Weighted F1 = 0.5968`ã€‚\n",
    "+ çµè«–ï¼šç‰¹å¾µå¤ªå°‘ï¼Œåªæœ‰ä¸‰ç¶­ã€Œäººå/åœ°å/çµ„ç¹”ã€ï¼Œè³‡è¨Šé‡å¤ªä½ã€‚å¾ˆå¤šæ–°èæˆ–æ¨æ–‡ä¸ä¸€å®šåŒ…å«é€™ä¸‰é¡å¯¦é«”ï¼Œè®Šæˆç„¡æ•ˆè³‡æ–™ã€‚ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400ff9f4",
   "metadata": {
    "id": "400ff9f4"
   },
   "source": [
    "#### B. ä½¿ç”¨ã€Œæƒ…ç·’åˆ†æã€ä¾†è¾¨è­˜çœŸå‡æ–°è\n",
    "\n",
    "åˆ©ç”¨`å¤šå€‹æƒ…ç·’ç‰¹å¾µæ¨¡å‹é€²è¡Œ`ï¼ŒåŠ ä¸Š`4å€‹åˆ†é¡å™¨(LogisticRegressionã€DecisionTreeã€SVMã€RandomForest)`æ¯”è¼ƒï¼Œè©•ä¼°æœ€ä½³çµæœä¾†åˆ†é¡çœŸå‡æ–°èï¼Œé æ¸¬é€™ç¯‡æ–°èæ˜¯çœŸ/å‡ã€‚\n",
    "+ **æƒ…ç·’æ¨¡å‹ä»‹ç´¹ï¼š**\n",
    "1. distilbertï¼šä¸€æ¬¾åŸºæ–¼SST-2å¾®èª¿çš„è¼•é‡ç´šBERTæ¨¡å‹ï¼Œå¸¸ç”¨æ–¼è‹±æ–‡ç”¢å“è©•è«–æˆ–å®¢æœå°è©±ä¸­çš„æƒ…ç·’æ­£è² åˆ†é¡ä»»å‹™ã€‚\n",
    "2. roberta-twitterï¼šå°ˆç‚ºTwitterè³‡æ–™è¨“ç·´çš„RoBERTaæ¨¡å‹ï¼Œå»£æ³›æ‡‰ç”¨æ–¼ç¤¾ç¾¤è²¼æ–‡çš„è¼¿æƒ…åˆ†æèˆ‡ç¤¾æœƒäº‹ä»¶æƒ…ç·’åµæ¸¬ã€‚\n",
    "3. bertweetï¼šä»¥æµ·é‡æ¨æ–‡èªæ–™è¨“ç·´çš„BERTæ¨¡å‹ï¼Œç‰¹åˆ¥é©ç”¨æ–¼ç¤¾ç¾¤åª’é«”ä¸Šçš„å³æ™‚æƒ…ç·’è¿½è¹¤èˆ‡ç”¨æˆ¶åæ‡‰åˆ†æã€‚\n",
    "4. nlptownï¼šä¸€å€‹æ”¯æ´å¤šèªè¨€ã€å¯è¼¸å‡º1ï½5æ˜Ÿç­‰ç´šçš„æƒ…ç·’å¼·åº¦æ¨¡å‹ï¼Œå¸¸ç”¨æ–¼å¤šèªè©•è«–è©•ç­‰ã€é¡§å®¢æ»¿æ„åº¦åˆ†æç­‰ä»»å‹™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d64c13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 899833,
     "status": "ok",
     "timestamp": 1749737169500,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "d3d64c13",
    "outputId": "7eded0ad-c018-4758-8b68-fd6ba73ca4b7"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# â”€â”€ Part 2ï¼šæƒ…ç·’ç‰¹å¾µæ¨¡å‹æ¯”è¼ƒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "sentiment_models = {\n",
    "    \"distilbert\"      : \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    \"roberta-twitter\" : \"cardiffnlp/twitter-roberta-base-sentiment\",\n",
    "    \"bertweet\"        : \"finiteautomata/bertweet-base-sentiment-analysis\",\n",
    "    \"nlptown\"         : \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "}\n",
    "# sentiment_models = {\n",
    "#     \"bertweet\"        : \"finiteautomata/bertweet-base-sentiment-analysis\"\n",
    "# }\n",
    "\n",
    "classifiers = {\n",
    "    \"LogReg\"       : LogisticRegression(max_iter=1000),\n",
    "    \"DecisionTree\" : DecisionTreeClassifier(),\n",
    "    \"LinearSVC\"    : LinearSVC(),\n",
    "    \"RandomForest\" : RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# classifiers = {\n",
    "#     \"RandomForest\" : RandomForestClassifier(random_state=42)\n",
    "# }\n",
    "\n",
    "# ä½¿ç”¨ tokenizer åˆ†æ®µï¼Œæ¯æ®µä¸è¶…é max_tokensï¼ˆä¸å« special tokensï¼‰\n",
    "def tokenize_chunks(text, tokenizer, max_tokens=512):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return [text]\n",
    "    chunked_tokens = [tokens[i:i + max_tokens] for i in range(0, len(tokens), max_tokens)]\n",
    "    return [tokenizer.decode(chunk) for chunk in chunked_tokens]\n",
    "\n",
    "def split_chunks(txt, size=512):\n",
    "    return [txt[i:i+size] for i in range(0, len(txt), size)]\n",
    "\n",
    "def senti_score(txt, pipe):\n",
    "    try:\n",
    "        res = pipe(split_chunks(txt))\n",
    "        pos = [r['score'] for r in res if 'POS' in r['label'].upper()]\n",
    "        neg = [r['score'] for r in res if 'NEG' in r['label'].upper()]\n",
    "        return (sum(pos)/len(pos)) if pos and sum(pos) > sum(neg) \\\n",
    "               else -(sum(neg)/len(neg)) if neg else 0.0\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "    \n",
    "# æ‰¹æ¬¡è™•ç† + æƒ…ç·’å¾—åˆ†\n",
    "def senti_score_batch(texts, model_name, batch_size=16, max_tokens=512):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    pipe = pipeline(\"sentiment-analysis\", model=model_name, device=0 if torch.cuda.is_available() else -1)\n",
    "    \n",
    "    all_scores = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Token-based Sentiment Inference\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        batch_scores = []\n",
    "\n",
    "        for text in batch:\n",
    "            try:\n",
    "                chunks = tokenize_chunks(text, tokenizer, max_tokens=max_tokens)\n",
    "                results = [pipe(chunk)[0] for chunk in chunks]  # æ¯æ®µä¸Ÿé€² sentiment pipeline\n",
    "\n",
    "                pos = [r['score'] for r in results if 'POS' in r['label'].upper()]\n",
    "                neg = [r['score'] for r in results if 'NEG' in r['label'].upper()]\n",
    "                score = (sum(pos) / len(pos)) if pos and sum(pos) > sum(neg) \\\n",
    "                        else -(sum(neg) / len(neg)) if neg else 0.0\n",
    "            except:\n",
    "                score = 0.0\n",
    "            batch_scores.append(score)\n",
    "\n",
    "        all_scores.extend(batch_scores)\n",
    "    return all_scores\n",
    "\n",
    "def compute_sentiment_scores(texts, model_name, batch_size=32):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(texts), batch_size)):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\", max_length=512).to(device)\n",
    "            outputs = model(**inputs)\n",
    "            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            # ä¾æ¨¡å‹æ¨™ç±¤æ ¼å¼ä¾†è™•ç†ï¼š\n",
    "            if probs.shape[1] == 3:  # eg: [NEG, NEU, POS]\n",
    "                score = (probs[:, 2] - probs[:, 0]).cpu().numpy()\n",
    "            elif probs.shape[1] == 2:  # eg: [NEG, POS]\n",
    "                score = (probs[:, 1] - probs[:, 0]).cpu().numpy()\n",
    "            else:\n",
    "                score = probs[:, 1].cpu().numpy()  # fallback\n",
    "            scores.extend(score)\n",
    "    return scores\n",
    "\n",
    "all_results, model_mean = [], []\n",
    "\n",
    "for m_key, m_name in sentiment_models.items():\n",
    "    print(f\"\\nğŸ” Sentiment Model: {m_key}\")\n",
    "    data_tmp = data.copy()\n",
    "    # ä¿®æ­£æ–¹æ³•\n",
    "    texts = data_tmp['text'].astype(str).tolist()\n",
    "    data_tmp['sentiment_score'] = senti_score_batch(\n",
    "        texts, model_name=m_name, batch_size=16  # å¯ä¾ç…§ GPU èª¿æ•´\n",
    "    )\n",
    "\n",
    "    X_senti = data_tmp[['sentiment_score']]\n",
    "    y       = data_tmp['label']\n",
    "\n",
    "    f1_collect = []\n",
    "    for clf_key, clf in classifiers.items():\n",
    "        X_tr, X_te, y_tr, y_te = train_test_split(X_senti, y, test_size=0.2, random_state=42)\n",
    "        clf.fit(X_tr, y_tr)\n",
    "        y_pred = clf.predict(X_te)\n",
    "        f1_val = f1_score(y_te, y_pred, average='weighted')\n",
    "        f1_collect.append(f1_val)\n",
    "        all_results.append({\"model\": m_key, \"classifier\": clf_key, \"f1\": f1_val})\n",
    "\n",
    "    model_mean.append({\"model\": m_key, \"mean_f1\": np.mean(f1_collect)})\n",
    "\n",
    "# âœ æ‰¾å¹³å‡ F1 æœ€é«˜çš„æƒ…ç·’æ¨¡å‹\n",
    "model_df = pd.DataFrame(model_mean).sort_values('mean_f1', ascending=False)\n",
    "best_senti = model_df.iloc[0]['model']\n",
    "print(\"\\nğŸ“Š  æƒ…ç·’æ¨¡å‹å¹³å‡ F1ï¼š\")\n",
    "print(tabulate(model_df, headers=\"keys\", tablefmt=\"fancy_grid\"))\n",
    "print(f\"\\nğŸ† æœ€ä½³æƒ…ç·’æ¨¡å‹ï¼š{best_senti}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcccc87d",
   "metadata": {
    "id": "bcccc87d"
   },
   "source": [
    "+ çµè«–ï¼š æƒ…ç·’é æ¸¬çœŸå‡æ–°èè¡¨ç¾ä¸å¥½ï¼Œæ•´é«”åˆ†é¡æ•ˆæœåå¼±ï¼Œ`æœ€ä½³æƒ…ç·’æ¨¡å‹ï¼šdistilbert`ï¼Œ`æœ€ä½³åˆ†é¡å™¨ï¼šRandomForest`ï¼Œ`Weighted F1 =0.74 `ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e0b838",
   "metadata": {
    "id": "55e0b838"
   },
   "source": [
    "#### C. ä½¿ç”¨ã€ŒNER+æƒ…ç·’ã€ä¾†è¾¨è­˜çœŸå‡æ–°è\n",
    "å»¶çºŒæ­¥é©ŸBé¸å‡ºæœ€ä½³æƒ…ç·’æ¨¡å‹`distilbert`å¾Œï¼ŒåŠ ä¸Š`4å€‹åˆ†é¡å™¨(LogisticRegressionã€DecisionTreeã€SVMã€RandomForest)`æ¯”è¼ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7yrPG8mXmvfH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 232013,
     "status": "ok",
     "timestamp": 1749737401520,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "7yrPG8mXmvfH",
    "outputId": "0fc62356-f179-4a19-aca2-26bef50a20a4"
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Part 3ï¼šNER + Best Sentiment ç‰¹å¾µè¨“ç·´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1ï¸âƒ£ ç”¨æœ€ä½³æƒ…ç·’æ¨¡å‹é‡æ–°è¨ˆç®— sentiment_score\n",
    "# best_pipe = pipeline(\"sentiment-analysis\", model=sentiment_models[best_senti], truncation=True, device=0 if device.type == \"cuda\" else -1)\n",
    "# tqdm.pandas()\n",
    "# data['sentiment_score'] = data['text'].astype(str).progress_apply(lambda t: senti_score(t, best_pipe))\n",
    "texts = data['text'].astype(str).tolist()\n",
    "data['sentiment_score'] = senti_score_batch(\n",
    "    texts, model_name=sentiment_models[best_senti], batch_size=16  # GPU + batch\n",
    ")\n",
    "\n",
    "# 2ï¸âƒ£ åˆä½µ NER (PER/ORG/LOC) + sentiment_score\n",
    "feature_df = data_with_ner[['PER', 'ORG', 'LOC']].join(data['sentiment_score'])\n",
    "X_full = feature_df\n",
    "y_full = data['label']\n",
    "\n",
    "# 3ï¸âƒ£ å››å€‹åˆ†é¡å™¨\n",
    "final_clfs = {\n",
    "    \"LogReg\"       : LogisticRegression(max_iter=1000),\n",
    "    \"DecisionTree\" : DecisionTreeClassifier(),\n",
    "    \"SVM\"          : svm.SVC(probability=True),\n",
    "    \"RandomForest\" : RandomForestClassifier(random_state=42)\n",
    "}\n",
    "# final_clfs = {\n",
    "#     \"RandomForest\" : RandomForestClassifier(random_state=42)\n",
    "# }\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "final_res = []\n",
    "\n",
    "for clf_key, base_clf in final_clfs.items():\n",
    "    y_all_t, y_all_p, f1_list = [], [], []\n",
    "\n",
    "    for tr_idx, te_idx in kf.split(X_full, y_full):\n",
    "        X_tr, X_te = X_full.iloc[tr_idx], X_full.iloc[te_idx]\n",
    "        y_tr, y_te = y_full.iloc[tr_idx], y_full.iloc[te_idx]\n",
    "\n",
    "        clf = clone(base_clf)\n",
    "        clf.fit(X_tr, y_tr)\n",
    "        y_pr = clf.predict(X_te)\n",
    "\n",
    "        y_all_t.extend(y_te); y_all_p.extend(y_pr)\n",
    "        f1_list.append(f1_score(y_te, y_pr, average='weighted'))\n",
    "\n",
    "    print(f\"\\n=== {clf_key} æ•´é«”å ±å‘Š ===\")\n",
    "    print(classification_report(y_all_t, y_all_p, target_names=[\"çœŸæ–°è\",\"å‡æ–°è\"], digits=2))\n",
    "\n",
    "    final_res.append({\"classifier\": clf_key, \"f1_weighted\": np.mean(f1_list)})\n",
    "\n",
    "# 4ï¸âƒ£ æ¯”è¼ƒè¡¨ & æœ€ä½³åˆ†é¡å™¨\n",
    "final_df = pd.DataFrame(final_res).sort_values('f1_weighted', ascending=False).reset_index(drop=True)\n",
    "print(\"\\nğŸ“Š  æœ€çµ‚ 4 åˆ†é¡å™¨æ¯”è¼ƒï¼š\")\n",
    "print(tabulate(final_df, headers=\"keys\", tablefmt=\"fancy_grid\"))\n",
    "\n",
    "best_cls = final_df.iloc[0]\n",
    "print(f\"\\nğŸ† æœ€çµ‚æœ€ä½³çµ„åˆï¼šæƒ…ç·’æ¨¡å‹={best_senti} + åˆ†é¡å™¨={best_cls['classifier']}ï¼Œweighted F1={best_cls['f1_weighted']:.4f}\")\n",
    "\n",
    "# 5ï¸âƒ£ (å¯é¸) è¦–è¦ºåŒ–\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x='classifier', y='f1_weighted', data=final_df, palette='Set2')\n",
    "plt.title(f'NER + Sentiment({best_senti})  4 åˆ†é¡å™¨æ¯”è¼ƒ')\n",
    "plt.ylabel('Weighted F1')\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26858d1",
   "metadata": {},
   "source": [
    "+ çµè«–ï¼šå¯åœ¨é€²è¡Œå„ªåŒ–ï¼Œ`æœ€ä½³åˆ†é¡å™¨ï¼šSVM`ï¼Œ`Weighted F1 =0.6014 `ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2OZ3X91gvzZA",
   "metadata": {
    "id": "2OZ3X91gvzZA"
   },
   "source": [
    "#### D. ä½¿ç”¨ã€ŒNER+æƒ…ç·’+TFIDFã€ä¾†è¾¨è­˜çœŸå‡æ–°è\n",
    "å»¶çºŒæ­¥é©ŸC.åŠ å…¥`TFIDF`ï¼Œï¼ŒåŠ ä¸Š`4å€‹åˆ†é¡å™¨(LogisticRegressionã€DecisionTreeã€SVMã€RandomForest)`æ¯”è¼ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8U7BNkkiv8yI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 230421,
     "status": "ok",
     "timestamp": 1749737631960,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "8U7BNkkiv8yI",
    "outputId": "d121b30c-9bcd-4cbb-b5d0-72ea0d4f5d6e"
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Part 4ï¼šTF-IDF + NER + Best Sentiment æ¨¡å‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# âœ… ä½¿ç”¨ Part 2 æœ€ä½³æƒ…ç·’æ¨¡å‹é‡æ–°ç”Ÿæˆ sentiment_score\n",
    "senti_model_name = sentiment_models[best_senti]\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\", model=senti_model_name, truncation=True, device=0 if device.type == \"cuda\" else -1)\n",
    "tqdm.pandas()\n",
    "data['sentiment_score'] = data['text'].astype(str).progress_apply(lambda t: senti_score(t, sentiment_pipe))\n",
    "\n",
    "# 1ï¸âƒ£ å»ºç«‹ TF-IDF ç‰¹å¾µ\n",
    "tfidf_vec = TfidfVectorizer(max_features=200, ngram_range=(1, 2))\n",
    "tfidf_mat = tfidf_vec.fit_transform(data['clean_text'].fillna(''))\n",
    "tfidf_df = pd.DataFrame(tfidf_mat.toarray(),\n",
    "                        columns=tfidf_vec.get_feature_names_out(),\n",
    "                        index=data.index)\n",
    "\n",
    "# 2ï¸âƒ£ å–å¾— NER ç‰¹å¾µ + æœ€æ–° sentiment åˆ†æ•¸\n",
    "ner_df     = data_with_ner[['PER', 'ORG', 'LOC']].copy()\n",
    "senti_df   = data[['sentiment_score']]\n",
    "X_features = pd.concat([ner_df, senti_df, tfidf_df], axis=1)\n",
    "y_target   = data['label']\n",
    "\n",
    "# 3ï¸âƒ£ å®šç¾©åˆ†é¡å™¨\n",
    "classifiers = {\n",
    "    \"LogReg\"       : LogisticRegression(max_iter=1000),\n",
    "    \"DecisionTree\" : DecisionTreeClassifier(),\n",
    "    \"SVM\"          : svm.SVC(probability=True),\n",
    "    \"RandomForest\" : RandomForestClassifier(random_state=42)\n",
    "}\n",
    "# classifiers = {\n",
    "#     \"RandomForest\" : RandomForestClassifier(random_state=42)\n",
    "# }\n",
    "\n",
    "# 4ï¸âƒ£ Cross-Validation è¨“ç·´\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = []\n",
    "\n",
    "for clf_name, clf_model in classifiers.items():\n",
    "    print(f\"\\n=== {clf_name} åˆ†é¡çµæœï¼ˆ5-foldï¼‰ ===\")\n",
    "    y_all_true, y_all_pred, f1s = [], [], []\n",
    "\n",
    "    for train_idx, test_idx in kf.split(X_features, y_target):\n",
    "        X_train, X_test = X_features.iloc[train_idx], X_features.iloc[test_idx]\n",
    "        y_train, y_test = y_target.iloc[train_idx], y_target.iloc[test_idx]\n",
    "\n",
    "        clf = clone(clf_model)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        y_all_true.extend(y_test)\n",
    "        y_all_pred.extend(y_pred)\n",
    "        f1s.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "    print(classification_report(y_all_true, y_all_pred,\n",
    "                                target_names=[\"çœŸæ–°è\", \"å‡æ–°è\"], digits=2))\n",
    "\n",
    "    results.append({\n",
    "        \"classifier\": clf_name,\n",
    "        \"f1_weighted\": np.mean(f1s)\n",
    "    })\n",
    "\n",
    "# 5ï¸âƒ£ è¼¸å‡ºç¸½çµ\n",
    "result_df = pd.DataFrame(results).sort_values(by='f1_weighted', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nğŸ“Š TF-IDF + NER + Sentiment åˆ†é¡å™¨æ¯”è¼ƒï¼š\")\n",
    "print(tabulate(result_df, headers=\"keys\", tablefmt=\"fancy_grid\"))\n",
    "\n",
    "best = result_df.iloc[0]\n",
    "print(f\"\\nğŸ† æœ€ä½³åˆ†é¡å™¨ç‚ºï¼š{best['classifier']}ï¼Œweighted F1 = {best['f1_weighted']:.4f}\")\n",
    "\n",
    "# â• å¯é¸è¦–è¦ºåŒ–\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x='classifier', y='f1_weighted', data=result_df, palette='Set3')\n",
    "plt.title(f\"TF-IDF + NER + Sentiment({best_senti}) åˆ†é¡å™¨æ¯”è¼ƒ\")\n",
    "plt.ylabel('Weighted F1')\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3xHOBUT211bp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 142,
     "status": "ok",
     "timestamp": 1749737632097,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "3xHOBUT211bp",
    "outputId": "f0ab62f9-a352-44a7-de19-5e04ffaa6af8"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# ä¾ label åˆ†ç¾¤\n",
    "true_texts = data[data['label'] == 0]['clean_text'].fillna('')\n",
    "fake_texts = data[data['label'] == 1]['clean_text'].fillna('')\n",
    "\n",
    "# å»ºç«‹ TF-IDF å‘é‡å™¨ï¼ˆå¯ä½¿ç”¨ç›¸åŒè¨­å®šä»¥ä¾¿æ¯”è¼ƒï¼‰\n",
    "tfidf = TfidfVectorizer(max_features=2000, ngram_range=(1, 2))\n",
    "\n",
    "# æ“¬åˆæ–¼çœŸæ–°è\n",
    "true_tfidf_matrix = tfidf.fit_transform(true_texts)\n",
    "true_feature_names = tfidf.get_feature_names_out()\n",
    "true_scores = true_tfidf_matrix.mean(axis=0).A1\n",
    "true_top30 = sorted(zip(true_feature_names, true_scores), key=lambda x: x[1], reverse=True)[:30]\n",
    "\n",
    "# æ“¬åˆæ–¼å‡æ–°èï¼ˆéœ€é‡æ–°å»ºä¸€å€‹ vectorizer æ‰ä¸æœƒå…±ç”¨å­—å…¸ï¼‰\n",
    "tfidf_fake = TfidfVectorizer(max_features=2000, ngram_range=(1, 2))\n",
    "fake_tfidf_matrix = tfidf_fake.fit_transform(fake_texts)\n",
    "fake_feature_names = tfidf_fake.get_feature_names_out()\n",
    "fake_scores = fake_tfidf_matrix.mean(axis=0).A1\n",
    "fake_top30 = sorted(zip(fake_feature_names, fake_scores), key=lambda x: x[1], reverse=True)[:30]\n",
    "\n",
    "# å°‡å…©å€‹ DataFrame åŠ ä¸Š index ä¸¦ reset\n",
    "true_df = pd.DataFrame(true_top30, columns=[\"çœŸæ–°èè©\", \"çœŸ_TF-IDF\"]).reset_index(drop=True)\n",
    "fake_df = pd.DataFrame(fake_top30, columns=[\"å‡æ–°èè©\", \"å‡_TF-IDF\"]).reset_index(drop=True)\n",
    "\n",
    "# åˆä½µç‚ºä¸€å€‹è¡¨æ ¼ï¼ˆå·¦å³æ¯”å°ï¼‰\n",
    "compare_df = pd.concat([true_df, fake_df], axis=1)\n",
    "\n",
    "# é¡¯ç¤ºçµæœ\n",
    "from IPython.display import display\n",
    "print(\"ğŸ“Š çœŸæ–°è vs å‡æ–°è å‰ 30 å¸¸è¦‹é—œéµè©ï¼ˆTF-IDF åˆ†æ•¸ï¼‰\")\n",
    "display(compare_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc1a61f",
   "metadata": {},
   "source": [
    "+ åˆ©ç”¨TFIDFæ‰¾å‡ºçœŸå‡æ–°èå¸¸è¦‹é—œéµå­—ã€‚\n",
    "+ çµè«–ï¼šåŠ å…¥TFIDFå¾Œï¼Œåˆ†é¡å™¨æ•ˆæœæå‡ï¼Œ`æœ€ä½³åˆ†é¡å™¨ï¼šRandomForest`ï¼Œ`Weighted F1 = 0.8604`ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "S1F-x4xHLhyi",
   "metadata": {
    "id": "S1F-x4xHLhyi"
   },
   "source": [
    "#### E. ä½¿ç”¨ã€ŒNER+æƒ…ç·’+TFIDFã€åŠ å…¥å…¶ä»–VADER&æ–‡å­—Styleä¾†è¾¨è­˜çœŸå‡æ–°è\n",
    "å»¶çºŒæ­¥é©ŸD.åŠ å…¥`VADER(+Empath)èˆ‡æ–‡å­—è¡¨é”Styleç‰¹å¾µ`ï¼Œï¼ŒåŠ ä¸Š`4å€‹åˆ†é¡å™¨(LogisticRegressionã€DecisionTreeã€SVMã€RandomForest)`æ¯”è¼ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFDPH3NGLf7k",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 5075,
     "status": "ok",
     "timestamp": 1749737722134,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "SFDPH3NGLf7k",
    "outputId": "ebc5fc07-1527-4bcd-ea51-d8ca1ad15c32"
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Part 5ï¼šTF-IDF+NER+Sentimentç‰¹å¾µä¸Šï¼Œå†åŠ å…¥VADER(+Empath)èˆ‡æ–‡å­—è¡¨é”Styleç‰¹å¾µ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ------------------------------------------------------------------\n",
    "# 1. é€éå¡æ–¹æª¢å®š (chi-square) æ‰¾å‡ºã€Œå‡æ–°è > çœŸæ–°èã€æœ€å…·å€è¾¨åŠ›çš„ n-gram\n",
    "# ------------------------------------------------------------------\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np, pandas as pd, re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ï¼ˆ1ï¼‰å»ºä¸€å€‹è©è¢‹æ¨¡å‹ï¼ˆunigram+bigramï¼Œéæ¿¾è‹±æ–‡åœç”¨å­—, min_df=5 é¿å…å¤ªç¨€æœ‰ï¼‰\n",
    "cv = CountVectorizer(stop_words='english', ngram_range=(1,2), min_df=5)\n",
    "X_bow = cv.fit_transform(data['clean_text'])\n",
    "feature_names = np.array(cv.get_feature_names_out())\n",
    "\n",
    "# ï¼ˆ2ï¼‰åšå¡æ–¹æª¢å®šï¼›label=1 ä»£è¡¨å‡æ–°è\n",
    "chi_scores, _ = chi2(X_bow, data['label'])\n",
    "\n",
    "# ï¼ˆ3ï¼‰åªä¿ç•™åœ¨å‡æ–°èå‡ºç¾æ¬¡æ•¸ > çœŸæ–°èçš„è©ï¼Œå†å–å‰ 30 å\n",
    "fake_mask = (X_bow[data['label'].values==1].sum(axis=0) >\n",
    "             X_bow[data['label'].values==0].sum(axis=0)).A1\n",
    "candidate_words = feature_names[fake_mask]\n",
    "candidate_scores= chi_scores[fake_mask]\n",
    "\n",
    "top_k = 30\n",
    "top_idx = np.argsort(candidate_scores)[::-1][:top_k]\n",
    "auto_clickbait = set(candidate_words[top_idx])\n",
    "\n",
    "print(f\"ğŸ” è‡ªå‹•åµæ¸¬åˆ° {len(auto_clickbait)} å€‹å‡æ–°èé«˜ç›¸é—œè©ï¼ˆå‰ {top_k}ï¼‰ï¼š\")\n",
    "print(sorted(auto_clickbait))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. å»ºç«‹ VADER / Empath / Style ç‰¹å¾µï¼ˆå«ã€Œå‹•æ…‹ click-baitã€ï¼‰\n",
    "# ------------------------------------------------------------------\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "try:\n",
    "    from empath import Empath\n",
    "    lexicon = Empath(); use_empath = True\n",
    "except ModuleNotFoundError:\n",
    "    use_empath = False\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# 2-1 VADER\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "vader_df = (data['text'].progress_apply(vader.polarity_scores)\n",
    "                       .apply(pd.Series).add_prefix('vader_'))\n",
    "print(\"\\nğŸ§  VADER æƒ…ç·’æ¨å‹•ç‰¹å¾µï¼ˆå‰å¹¾ç­†ï¼‰ï¼š\")\n",
    "print(vader_df.head())\n",
    "\n",
    "# 2-2 Empathï¼ˆé¸æ“‡å¹¾å€‹å¸¸ç”¨æƒ…ç·’ç¤¾æœƒé¢å‘ï¼‰\n",
    "if use_empath:\n",
    "    empath_raw = data['text'].progress_apply(lambda t: lexicon.analyze(t, normalize=True))\n",
    "    empath_keep = ['positive_emotion','negative_emotion','anger','sadness',\n",
    "                   'fear','politics','money','fun','love']\n",
    "    empath_df = (pd.DataFrame(empath_raw.tolist())\n",
    "                   [empath_keep].add_prefix('empath_'))\n",
    "\n",
    "    print(\"\\nğŸ¯ NRC-Empath æƒ…ç·’å‘é‡ï¼ˆå‰å¹¾ç­†ï¼‰ï¼š\")\n",
    "    print(empath_df.head())\n",
    "else:\n",
    "    empath_df = pd.DataFrame(index=data.index)   # ç©º DF\n",
    "    print(\"\\nâš ï¸ æœªå•Ÿç”¨ Empathï¼ˆéœ€ pip install empathï¼‰\")\n",
    "\n",
    "# 2-3 Style featuresï¼ˆå¤§å¯«æ¯”ä¾‹ / ! å¯†åº¦ / è‡ªå‹• click-bait å‘½ä¸­ç‡ï¼‰\n",
    "def style_feats(txt:str):\n",
    "    L = max(len(txt),1)\n",
    "    txt_low = txt.lower()\n",
    "    hit_cnt = sum(1 for w in auto_clickbait if w in txt_low)\n",
    "    return pd.Series({\n",
    "        'caps_ratio'      : sum(c.isupper() for c in txt)/L,\n",
    "        'excl_ratio'      : txt.count('!')/L,\n",
    "        'clickbait_ratio' : hit_cnt / len(auto_clickbait)\n",
    "    })\n",
    "\n",
    "style_df = data['text'].progress_apply(style_feats)\n",
    "\n",
    "print(\"\\nğŸ“ æ–‡å­—è¡¨é”æ–¹å¼ç‰¹å¾µï¼ˆå¤§å¯«æ¯”ä¾‹ / æ„Ÿå˜†è™Ÿå¯†åº¦ / Click-bait å‘½ä¸­ç‡ï¼‰\")\n",
    "print(style_df.describe())\n",
    "\n",
    "print(\"\\nğŸ“Š å‡æ–°èèˆ‡çœŸæ–°èçš„ Style ç‰¹å¾µå¹³å‡æ¯”è¼ƒï¼š\")\n",
    "print(pd.concat([style_df, data['label']], axis=1)\n",
    "        .groupby('label').mean()\n",
    "        .rename(index={0: \"çœŸæ–°è\", 1: \"å‡æ–°è\"}))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. æŠŠæ–°ç‰¹å¾µæ¥åˆ°æ—¢æœ‰ X_featuresï¼ˆTF-IDF + NER + Best-Sentimentï¼‰\n",
    "# ------------------------------------------------------------------\n",
    "X_final = pd.concat([X_features, vader_df, empath_df, style_df], axis=1)\n",
    "y_final = data['label']\n",
    "print(\"ğŸ”¢ æ–°å¢å¾Œç‰¹å¾µç¶­åº¦ :\", X_final.shape)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4. å››å€‹åˆ†é¡å™¨ Ã— 5-fold äº¤å‰é©—è­‰\n",
    "# ------------------------------------------------------------------\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import clone\n",
    "from tabulate import tabulate\n",
    "import seaborn as sns, matplotlib.pyplot as plt\n",
    "\n",
    "clfs = {\n",
    "    \"LogReg\"      : LogisticRegression(max_iter=1000),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(),\n",
    "    \"SVM\"         : SVC(probability=True),\n",
    "    \"RandomForest\": RandomForestClassifier(random_state=42)\n",
    "}\n",
    "# clfs = {\n",
    "#     \"RandomForest\": RandomForestClassifier(random_state=42)\n",
    "# }\n",
    "\n",
    "kf = StratifiedKFold(5, shuffle=True, random_state=42)\n",
    "rows = []\n",
    "\n",
    "for name, base in clfs.items():\n",
    "    y_t, y_p, f1s = [], [], []\n",
    "    for tr, te in kf.split(X_final, y_final):\n",
    "        mdl = clone(base).fit(X_final.iloc[tr], y_final.iloc[tr])\n",
    "        pred = mdl.predict(X_final.iloc[te])\n",
    "        y_t.extend(y_final.iloc[te]); y_p.extend(pred)\n",
    "        f1s.append(f1_score(y_final.iloc[te], pred, average='weighted'))\n",
    "    print(f\"\\n=== {name} å ±å‘Š (åŠ  VADER / Style) ===\")\n",
    "    print(classification_report(y_t, y_p, target_names=['çœŸæ–°è','å‡æ–°è'], digits=2))\n",
    "    rows.append({\"classifier\": name, \"f1_weighted\": np.mean(f1s)})\n",
    "\n",
    "res_df = pd.DataFrame(rows).sort_values('f1_weighted', ascending=False)\n",
    "print(\"\\nğŸ“Š  åŠ  VADER / Style / å‹•æ…‹ Click-bait å¾Œåˆ†é¡å™¨æ¯”è¼ƒ\")\n",
    "print(tabulate(res_df, headers=\"keys\", tablefmt=\"fancy_grid\"))\n",
    "\n",
    "best_cls = res_df.iloc[0]\n",
    "print(f\"\\nğŸ†  æ–°æœ€ä½³æ¨¡å‹ï¼š{best_cls['classifier']}  (Weighted F1 = {best_cls['f1_weighted']:.4f})\")\n",
    "\n",
    "# ï¼ˆå¯é¸ï¼‰é•·æ¢åœ–\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.barplot(x='classifier', y='f1_weighted', data=res_df, palette='Set2')\n",
    "plt.title('åŠ å…¥ VADER / Style ç‰¹å¾µå¾Œçš„åˆ†é¡å™¨æ¯”è¼ƒ')\n",
    "plt.ylabel('Weighted F1')\n",
    "plt.ylim(0,1); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903532f7",
   "metadata": {},
   "source": [
    "+ çµè«–ï¼šå¼•å…¥VADERæƒ…ç·’åˆ†æèˆ‡æ–‡å­—é¢¨æ ¼ç‰¹å¾µï¼Œä»¥å¼·åŒ–å‡æ–°èèˆ‡çœŸæ–°èçš„èªè¨€è¡¨å¾µå·®ç•°ï¼Œç¶“ä»¥ä¸Šæ¨¡å‹æ¸¬è©¦ï¼Œ**ä½¿ç”¨ã€ŒNER+æƒ…ç·’+TFIDF+VADER+STYLEã€ç‰¹å¾µçš„çµæœæœ€å¥½**ï¼Œ`æœ€ä½³åˆ†é¡å™¨ï¼šRandomForest`ï¼Œ`Weighted F1 =0.8675 `ã€‚\n",
    "+ å¾VADERç‰¹å¾µçµæœå¯è¦‹ï¼Œå‡æ–°èæ•´é«”åå‘ç”¢ç”Ÿè¼ƒé«˜çš„è² å‘æƒ…ç·’åˆ†æ•¸èˆ‡æ¥µç«¯æƒ…æ„Ÿï¼ˆcompound åˆ†æ•¸è¼ƒè² ï¼‰ï¼Œé¡¯ç¤ºå‡æ–°èå‚¾å‘ä»¥æƒ…ç·’æ€§èªè¨€æ¿€ç™¼è®€è€…åæ‡‰ã€‚å¦ä¸€æ–¹é¢ï¼Œæ–‡å­—è¡¨é”æ–¹å¼æ–¹é¢çš„æ¯”è¼ƒé¡¯ç¤ºï¼Œå‡æ–°èåœ¨å¤§å¯«å­—æ¯ä½¿ç”¨æ¯”ä¾‹ï¼ˆcaps_ratioï¼‰ã€æ„Ÿå˜†è™Ÿå¯†åº¦ï¼ˆexcl_ratioï¼‰ã€ä»¥åŠ clickbait é—œéµè©å‘½ä¸­ç‡ï¼ˆclickbait_ratioï¼‰å‡é«˜æ–¼çœŸæ–°èï¼Œé¡¯ç¤ºå‡æ–°èå¸¸é€éèª‡å¼µæ¨™é¡Œèˆ‡è¦–è¦ºå¼·èª¿ä»¥å¸å¼•æ³¨æ„åŠ›ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kXCGLyxag7DL",
   "metadata": {
    "id": "kXCGLyxag7DL"
   },
   "source": [
    "#### F. ä½¿ç”¨ã€ŒNER+æƒ…ç·’+SBERTã€ä¾†è¾¨è­˜çœŸå‡æ–°è\n",
    "+ åˆ©ç”¨SBERTå‘é‡åŒ–æ–¹å¼èˆ‡æ­¥é©ŸDçš„TFIDFæ¯”è¼ƒï¼Œï¼Œå†åŠ ä¸Š`4å€‹åˆ†é¡å™¨(LogisticRegressionã€DecisionTreeã€SVMã€RandomForest)`è©•ä¼°æœ€ä½³çµæœã€‚\n",
    "+ ç¶“å‰æ¸¬çµæœï¼Œæ¡ç”¨æ­¤æ¨¡å‹`sentence-transformers/paraphrase-MiniLM-L6-v2`é€²è¡Œã€‚\n",
    "1. ä½¿ç”¨all-MiniLM-L6-v2ï¼Œæœ€ä½³åˆ†é¡å™¨ç‚ºï¼šRandomForestï¼ŒWeighted F1 = 0.7197\n",
    "2. ä½¿ç”¨sentence-transformers/paraphrase-MiniLM-L6-v2ï¼Œæœ€ä½³åˆ†é¡å™¨ç‚ºï¼šLogRegï¼ŒWeighted F1 = 0.7467"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RRiE1c9WgFLd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "1bd81346eb5446ae86c8992ea87a8979",
      "3aa0234db1f4447da06ae0ba78593276",
      "aaefd6793ed644b3afbf183bec859bff",
      "e108b8551f0e41b69ce386e7cbdf3945",
      "9c0fc22fc33a4388b5cde67492782329",
      "a7bd128141e746019bbd9eeed16eda0c",
      "c9da9c06c90a475ab7e45abc95f3dec3",
      "953dd787d72045bc9382fef65869ced1",
      "30cfa6df7235440a8aaf12b38e7c1a24",
      "2ff3e28618eb412c9f1879564137be6c",
      "07b0a05b872c48ec8bc00ec063304866"
     ]
    },
    "executionInfo": {
     "elapsed": 13099,
     "status": "ok",
     "timestamp": 1749738724842,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "RRiE1c9WgFLd",
    "outputId": "3b0c94d7-cadd-4537-9858-96bf4b02f141"
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€ Part 6ï¼šNER+Sentimentç‰¹å¾µ+SBERTå‘é‡åŒ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# â–1. SBERT å‘é‡åŒ–\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')  # å¯æ”¹å…¶ä»–å¦‚ 'paraphrase-MiniLM-L6-v2'\n",
    "sbert_embeddings = model.encode(data['clean_text'].fillna(''), show_progress_bar=True)\n",
    "\n",
    "sbert_df = pd.DataFrame(sbert_embeddings, index=data.index)\n",
    "sbert_df.columns = sbert_df.columns.astype(str)\n",
    "print(\"ğŸ“ å‘é‡ç¶­åº¦ï¼š\", sbert_df.shape)\n",
    "\n",
    "# â–2. åˆä½µå…¶ä»–ç‰¹å¾µï¼ˆNER + Sentimentï¼‰\n",
    "ner_df    = data_with_ner[['PER', 'ORG', 'LOC']].copy()\n",
    "senti_df  = data[['sentiment_score']]\n",
    "X_sbert   = pd.concat([sbert_df, ner_df, senti_df], axis=1)\n",
    "y_target  = data['label']\n",
    "\n",
    "# â–3. å»ºç«‹åˆ†é¡å™¨çµ„åˆ\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.base import clone\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "classifiers = {\n",
    "    \"LogReg\"       : LogisticRegression(max_iter=1000),\n",
    "    \"DecisionTree\" : DecisionTreeClassifier(),\n",
    "    \"SVM\"          : SVC(probability=True),\n",
    "    \"RandomForest\" : RandomForestClassifier(random_state=42)\n",
    "}\n",
    "# classifiers = {\n",
    "#     \"RandomForest\" : RandomForestClassifier(random_state=42)\n",
    "# }\n",
    "\n",
    "# â–4. Cross-validation æ¯”è¼ƒè¡¨ç¾\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = []\n",
    "\n",
    "for clf_name, clf_model in classifiers.items():\n",
    "    print(f\"\\n=== {clf_name} åˆ†é¡çµæœï¼ˆ5-foldï¼‰ ===\")\n",
    "    y_all_true, y_all_pred, f1s = [], [], []\n",
    "\n",
    "    for train_idx, test_idx in kf.split(X_sbert, y_target):\n",
    "        X_train, X_test = X_sbert.iloc[train_idx], X_sbert.iloc[test_idx]\n",
    "        y_train, y_test = y_target.iloc[train_idx], y_target.iloc[test_idx]\n",
    "\n",
    "        clf = clone(clf_model)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        y_all_true.extend(y_test)\n",
    "        y_all_pred.extend(y_pred)\n",
    "        f1s.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "    print(classification_report(y_all_true, y_all_pred, target_names=[\"çœŸæ–°è\", \"å‡æ–°è\"], digits=2))\n",
    "\n",
    "    results.append({\n",
    "        \"classifier\": clf_name,\n",
    "        \"f1_weighted\": np.mean(f1s)\n",
    "    })\n",
    "\n",
    "# â–5. é¡¯ç¤ºæ¯”è¼ƒçµæœ\n",
    "result_df = pd.DataFrame(results).sort_values(by='f1_weighted', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nğŸ“Š SBERT + NER + Sentiment åˆ†é¡å™¨æ¯”è¼ƒï¼š\")\n",
    "print(tabulate(result_df, headers=\"keys\", tablefmt=\"fancy_grid\"))\n",
    "\n",
    "best = result_df.iloc[0]\n",
    "print(f\"\\nğŸ† æœ€ä½³åˆ†é¡å™¨ç‚ºï¼š{best['classifier']}ï¼Œweighted F1 = {best['f1_weighted']:.4f}\")\n",
    "\n",
    "# â–6. è¦–è¦ºåŒ–çµæœ\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x='classifier', y='f1_weighted', data=result_df, palette='Set2')\n",
    "plt.title(\"BERT å‘é‡ + NER + Sentiment åˆ†é¡å™¨æ¯”è¼ƒ\")\n",
    "plt.ylabel('Weighted F1')\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HbkktXE-jqDV",
   "metadata": {
    "id": "HbkktXE-jqDV"
   },
   "source": [
    "+ çµè«–ï¼šè¡¨ç¾æˆæœæ²’æœ‰æ­¥é©ŸEä½¿ç”¨ã€ŒNER+æƒ…ç·’+TFIDF+VADER+æ–‡å­—Styleã€çš„å¥½ï¼Œ`æœ€ä½³åˆ†é¡å™¨ï¼šSVM`ï¼Œ`Weighted F1 =0.8612 `ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff48406",
   "metadata": {},
   "source": [
    "#### **G. æœ€ä½³å¤šç‰¹å¾µæ¨¡å‹ç‚ºï¼šä½¿ç”¨ã€ŒNER+æƒ…ç·’åˆ†æ•¸+TFIDF+VADER+æ–‡å­—Styleã€æœ€å¥½**\n",
    "`æœ€ä½³åˆ†é¡å™¨ï¼šRandomForest`ï¼Œ`Weighted F1 =0.8675 `ã€‚åˆ©ç”¨æ­¤çµæœå¾€ä¸‹é€²è¡ŒBertopicä½œæ¥­"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa66961",
   "metadata": {
    "id": "2aa66961"
   },
   "source": [
    "## ä¸‰ã€ä¸»é¡Œæ¨¡å‹ï¼šBERTopic \n",
    "+ ä¾æœ€ä½³å‘é‡åŒ–ç­–ç•¥(TFIDF+Style)å»ºç«‹HDBSCANèšé¡ï¼Œä¸¦è¼¸å‡ºã€ŒçœŸå‡æ¯”ä¾‹æœ€é«˜/æœ€ä½ã€ä¸»é¡Œèˆ‡é—œéµå­—ã€‚\n",
    "+ æ­¥é©Ÿï¼šä½¿ç”¨æœ€ä½³å‘é‡åŒ–çµæœ(TFIDF+VADER+æ–‡å­—Style)â†’BERTopicåˆ†ç¾¤â†’ç–ŠåŠ çœŸå‡æ¨™ç±¤â†’çœ‹æ¯å€‹ä¸»é¡Œå“ªé‚Šå‡æ–°èé«˜ã€å“ªé‚ŠçœŸæ–°èé«˜ï¼Œé€™æ¨£å°±èƒ½å¾—åˆ°ï¼š\n",
    "  1. å‡æ–°èæœ€å¸¸è¦‹çš„ä¸»é¡Œæœ‰å“ªäº›?\n",
    "  2. çœŸæ–°èè£¡å“ªäº›ä¸»é¡Œç‰¹åˆ¥çªå‡º?\n",
    "  3. å„ä¸»é¡Œçš„ä»£è¡¨é—œéµè©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gmCYJY1vm9EW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1611,
     "status": "ok",
     "timestamp": 1749741036530,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "gmCYJY1vm9EW",
    "outputId": "5b1a8b04-f301-4c61-8191-b9a0d51b4ed1"
   },
   "outputs": [],
   "source": [
    "# -------- 1. é¸æ“‡å‘é‡åŒ–æ–¹å¼ -------------------------------------Part 4 / 5 / 6 çµæœè¼¸å…¥\n",
    "VEC_CHOICE = \"tfidf_style\"       # â† è¼¸å…¥ # Part 4- \"tfidf\" / Part 5- \"tfidf_style\" / Part 6- \"sbert\"\n",
    "texts = data['clean_text'].fillna('')\n",
    "\n",
    "if VEC_CHOICE == \"tfidf\":\n",
    "    vec_model = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), stop_words=\"english\")\n",
    "    embeddings = vec_model.fit_transform(texts)\n",
    "\n",
    "elif VEC_CHOICE == \"tfidf_style\":\n",
    "    if \"X_final\" not in globals():\n",
    "        raise RuntimeError(\"âš ï¸ æ‰¾ä¸åˆ° X_finalï¼Œè«‹å…ˆåŸ·è¡Œ Part 5 å»ºç«‹ç‰¹å¾µ\")\n",
    "    embeddings = X_final.values\n",
    "\n",
    "elif VEC_CHOICE == \"sbert\":\n",
    "    emb_model = SentenceTransformer(\"sentence-transformers/paraphrase-MiniLM-L6-v2\")\n",
    "    embeddings = emb_model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"VEC_CHOICE åƒ…èƒ½ç‚º 'tfidf' / 'tfidf_style' / 'sbert'\")\n",
    "\n",
    "# -------- 2. å»ºç«‹ BERTopic æ¨¡å‹ ----------------------------------\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=None if VEC_CHOICE.startswith(\"tfidf\") else emb_model,\n",
    "    hdbscan_model=HDBSCAN(min_cluster_size=10, min_samples=30),\n",
    "    vectorizer_model=CountVectorizer(ngram_range=(1, 2), stop_words=\"english\"),\n",
    "    calculate_probabilities=False,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "topics, _ = topic_model.fit_transform(texts, embeddings)\n",
    "data['topic'] = topics\n",
    "\n",
    "# âœ… é˜²å‘†ï¼šç¢ºèªæ˜¯å¦æœ‰æœ‰æ•ˆä¸»é¡Œï¼ˆé -1ï¼‰\n",
    "valid_topics = [t for t in set(topics) if t != -1]\n",
    "if len(valid_topics) == 0:\n",
    "    print(\"âš ï¸ ç„¡æœ‰æ•ˆä¸»é¡Œï¼ˆå…¨éƒ¨ç‚º outlierï¼‰ï¼Œè«‹æª¢æŸ¥è³‡æ–™ç­†æ•¸æˆ–é™ä½ min_cluster_size è¨­å®šã€‚\")\n",
    "else:\n",
    "    # -------- 3. ä¸»é¡Œ Ã— çœŸï¼å‡ åˆ†ä½ˆ -------------------------------\n",
    "    data['label_name'] = data['label'].map({0: \"True\", 1: \"Fake\"})\n",
    "    topic_dist = (data.groupby(['topic', 'label_name']).size().unstack(fill_value=0))\n",
    "    topic_dist['Total'] = topic_dist.sum(axis=1)\n",
    "    topic_dist['Fake_Ratio'] = topic_dist['Fake'] / topic_dist['Total']\n",
    "\n",
    "    print(\"â–¶ å„ Topic çœŸï¼å‡ç­†æ•¸èˆ‡å‡æ–°èæ¯”ä¾‹ (å‰ 10)ï¼š\")\n",
    "    display(topic_dist.sort_values('Fake_Ratio', ascending=False).head(10))\n",
    "\n",
    "    # -------- 4. å–ä¸»é¡Œé—œéµå­—ä¸¦ä¾çœŸå‡æ¯”ä¾‹æ’åº ----------------------\n",
    "    kw_rows = []\n",
    "    for tid, word_scores in topic_model.get_topics().items():\n",
    "        if tid == -1:\n",
    "            continue\n",
    "        for word, score in word_scores:\n",
    "            kw_rows.append({\"topic\": tid, \"word\": word, \"c_tf_idf\": score})\n",
    "\n",
    "    kw_df = pd.DataFrame(kw_rows)\n",
    "\n",
    "    merged_kw = kw_df.merge(topic_dist.reset_index(), on=\"topic\")\n",
    "\n",
    "    fake_top_kw = (merged_kw.sort_values(['Fake_Ratio', 'c_tf_idf'], ascending=[False, False])\n",
    "                            .groupby('topic')\n",
    "                            .head(30))\n",
    "\n",
    "    true_top_kw = (merged_kw.sort_values(['Fake_Ratio', 'c_tf_idf'], ascending=[True, False])\n",
    "                            .groupby('topic')\n",
    "                            .head(30))\n",
    "\n",
    "    print(\"\\nğŸŸ¥ å‡æ–°èé«˜æ¯”ä¾‹ä¸»é¡Œé—œéµå­— TOP 30\")\n",
    "    display(fake_top_kw[['topic', 'word', 'c_tf_idf', 'Fake_Ratio']])\n",
    "\n",
    "    print(\"\\nğŸŸ¦ çœŸæ–°èé«˜æ¯”ä¾‹ä¸»é¡Œé—œéµå­— TOP 30\")\n",
    "    display(true_top_kw[['topic', 'word', 'c_tf_idf', 'Fake_Ratio']])\n",
    "\n",
    "    # -------- 5. è¦–è¦ºåŒ–æ¯å€‹ä¸»é¡Œçš„å‡æ–°èæ¯”ä¾‹ ------------------------\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    sns.barplot(x=topic_dist.index, y=topic_dist['Fake_Ratio'], palette=\"coolwarm\")\n",
    "    plt.title(\"Fake-News Ratio per Topic\")\n",
    "    plt.ylabel(\"Fake Ratio\")\n",
    "    plt.xlabel(\"Topic ID\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "osakI7pZsZuG",
   "metadata": {
    "id": "osakI7pZsZuG"
   },
   "source": [
    "+ **çµè«–**\n",
    "+ é€éBERTopicä¸»é¡Œå»ºæ¨¡ï¼Œæ¡ç”¨åŠ å…¥æƒ…ç·’èˆ‡Styleç‰¹å¾µçš„å‘é‡é€²è¡Œåˆ†æï¼Œæ­éœ²å‡æ–°èèˆ‡çœŸæ–°èåˆ†å¸ƒæ–¼ä¸»é¡Œç©ºé–“çš„é¡¯è‘—å·®ç•°ã€‚çµæœé¡¯ç¤ºï¼Œå¤šå€‹ä¸»é¡Œå‘ˆç¾é«˜åº¦åå‘å‡æ–°èï¼ˆFake Ratio = 1.0ï¼‰ï¼Œå…¶ä¸­åŒ…å«èˆ‡æ”¿æ²»äººç‰©ï¼ˆå¦‚ rick perryã€american maleï¼‰æˆ–å…·çˆ­è­°æ€§çš„ä¸»å¼µï¼ˆå¦‚ birthright citizenshipï¼‰ç›¸é—œä¹‹ä¸»é¡Œï¼Œå‡¸é¡¯å‡æ–°èå¸¸èšç„¦æ–¼ç‰¹å®šæ•æ„Ÿè­°é¡Œä»¥å¸å¼•è®€è€…æ³¨æ„ã€‚\n",
    "+ çœŸæ–°èæ¯”ä¾‹è¼ƒé«˜ä¹‹ä¸»é¡Œå‰‡è¼ƒå¤šæ¶µè“‹ä¸­æ€§äº‹ä»¶èˆ‡è³‡è¨Šå ±å°ï¼ˆå¦‚ joe exoticã€wildfireã€pardonedï¼‰ï¼Œé¡¯ç¤ºå…¶å…§å®¹å‚¾å‘å®¢è§€æè¿°ä¸”æƒ…ç·’æ“å¼„ç¨‹åº¦è¼ƒä½ã€‚\n",
    "+ å¾ã€Œæ¯å€‹ä¸»é¡Œçš„å‡æ–°èæ¯”ä¾‹è¦–è¦ºåŒ–åœ–ã€å¯é€²ä¸€æ­¥è§€å¯Ÿä¸»é¡Œåˆ†å¸ƒçš„æ¥µåŒ–ç¾è±¡ï¼Œå…¶ä¸­éƒ¨åˆ†ä¸»é¡Œé›†ä¸­å‘ˆç¾æ¥µé«˜æˆ–æ¥µä½çš„å‡æ–°èæ¯”ä¾‹ï¼Œé©—è­‰ä¸»é¡Œæ¨¡å‹èƒ½æœ‰æ•ˆæ•æ‰æ–°èçœŸå½å‚¾å‘ã€‚\n",
    "+ ç¶œåˆä¸Šè¿°ï¼ŒåŠ å…¥æƒ…ç·’èˆ‡Styleå‘é‡ç‰¹å¾µå¾Œæ‰€å»ºæ§‹ä¹‹ä¸»é¡Œæ¨¡å‹ï¼Œæå‡äº†ä¸»é¡ŒTopicçš„å­—å½™å€è¾¨èƒ½åŠ›ï¼Œä¹Ÿæœ‰åŠ©æ–¼å¾ä¸»é¡Œè§’åº¦ç†è§£çœŸå‡æ–°èå¸¸å‡ºç¾çš„å­—å½™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5762b28",
   "metadata": {},
   "source": [
    "## å››ã€LLM ##\n",
    "\n",
    "ä¸‰å€‹æœ¬åœ°æ¨¡å‹åˆ†åˆ¥æ‰®æ¼”ä¸åŒè§’è‰²ï¼š\n",
    "- logic_llmï¼šé‚è¼¯é¢åˆ¤æ–·è€…(LLaMA)\n",
    "- debater_llmï¼šè¾¯è«–ç«‹å ´æå‡ºè€…(Mistral)\n",
    "- judge_llmï¼šåŸå§‹ä»²è£è€…ï¼ŒåŒæ™‚ä¹Ÿè² è²¬æœ€çµ‚æ±ºç­–(Phi-3)\n",
    "\n",
    "\n",
    "é€™ä¸‰å€‹æ¨¡å‹æœƒåŒæ™‚é‡å°åŒä¸€å‰‡è¨Šæ¯é€²è¡Œåˆ¤æ–·ï¼Œè¼¸å‡º verdict, confidence, reason ä¸‰æ¬„ JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a3a756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama  # ä½¿ç”¨ Ollama å°è£çš„ LLaMA æ¨¡å‹\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# å®šç¾©è¼¸å‡ºçµæ§‹\n",
    "\n",
    "\n",
    "class MessageClassification(BaseModel):\n",
    "    verdict: str = Field(\n",
    "        description=\"Verdict whether the message is Real or Fake\")\n",
    "    confidence: str = Field(\n",
    "        description=\"Confidence level of the judgment (e.g., High, Medium, Low)\")\n",
    "    reason: str = Field(description=\"Brief explanation of the judgment\")\n",
    "\n",
    "\n",
    "# ä½¿ç”¨æœ¬åœ° LLaMA æ¨¡å‹\n",
    "judge_llm = ChatOllama(model=\"llama3:8B\")\n",
    "logic_llm = ChatOllama(model=\"phi3:3.8B\")\n",
    "debater_llm = ChatOllama(model=\"mistral:7B\")\n",
    "\n",
    "# Json è¼¸å‡ºæ ¼å¼è§£æå™¨\n",
    "parser = JsonOutputParser(pydantic_object=MessageClassification)\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "\n",
    "# å–®ä¸€ LLM æ¨ç†çš„ Prompt\n",
    "llm_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are a professional fact-checker. Analyze the following message and determine if it is real or fake.\n",
    "\n",
    "Message:\n",
    "\\\"\\\"\\\"{message}\\\"\\\"\\\"\n",
    "\n",
    "Respond **strictly** in the following JSON format:\n",
    "\n",
    "{{\n",
    "  \"verdict\": \"\",        // Only \"Real\" or \"Fake\"\n",
    "  \"confidence\": \"\",     // Only \"High\", \"Medium\", or \"Low\"\n",
    "  \"reason\": \"\"          // A short explanation (1-2 sentences)\n",
    "}}\n",
    "\n",
    "â— Instructions:\n",
    "- Your response MUST be valid JSON. Do not include any other text.\n",
    "- Do NOT wrap the output in markdown (e.g., do NOT use ```json or any backticks).\n",
    "- Do NOT explain your answer outside the JSON.\n",
    "- Output only the JSON object with exactly three string fields: verdict, confidence, and reason.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# è®“ judge_llm åŒ¯ç¸½æ‰€æœ‰æ¨¡å‹è§€é»çš„ Prompt\n",
    "summary_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are the final arbiter. Three experts have evaluated the message. Please summarize their opinions and give your final decision.\n",
    "\n",
    "Message:\n",
    "\\\"\\\"\\\"{message}\\\"\\\"\\\"\n",
    "\n",
    "Expert 1 (Logic-focused model):\n",
    "{logic_opinion}\n",
    "\n",
    "Expert 2 (Debate-focused model):\n",
    "{debate_opinion}\n",
    "\n",
    "Expert 3 (Your own opinion):\n",
    "{your_opinion}\n",
    "\n",
    "Now summarize the opinions, resolve any conflicts, and provide a final classification in this JSON format:\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fc0a6b",
   "metadata": {},
   "source": [
    "ä»²è£æ¨¡å‹è§’è‰²èªªæ˜ï¼š\n",
    "è¼¸å…¥: åŸå§‹è¨Šæ¯ data[text]\n",
    "\n",
    "å°è±¡: llama3:8Bã€phi3:3.8Bã€mistral:7B\n",
    "\n",
    "å¾—åˆ°: ä¸‰å€‹æ¨¡å‹çš„è§€é»\n",
    "\n",
    "è¦‹æ•´åˆé€™ä¸‰ä»½è§€é»ï¼Œæ ¹æ“šsummary promptå›å‚³ä¸€çµ„æ–°çš„ JSONä½œç‚ºæœ€å¾Œç­”æ¡ˆ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a57b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import re\n",
    "import json\n",
    "\n",
    "def extract_json(text: str) -> dict:\n",
    "    try:\n",
    "        \n",
    "        # æ‰¾å‡ºç¬¬ä¸€çµ„çµæ§‹ç‚º { ... } çš„JSONå€å¡Š\n",
    "        match = re.search(r'{[\\s\\S]*?}', text)\n",
    "        if not match:\n",
    "            print(\"âš ï¸ ç„¡æœ‰æ•ˆ JSON å€å¡Šï¼Œè·³éæ­¤è¼¸å‡ºã€‚\")\n",
    "            print(\"åŸå§‹è¼¸å‡ºï¼š\", text)\n",
    "            return {\n",
    "                \"verdict\": \"Unknown\",\n",
    "                \"confidence\": \"Low\",\n",
    "                \"reason\": \"Model did not return valid JSON block.\"\n",
    "            }\n",
    "        json_str = match.group()\n",
    "        return json.loads(json_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"\\n JSON è§£æå¤±æ•—ï¼š{e}\")\n",
    "        print(\"åŸå§‹è¼¸å‡ºï¼š\", text)\n",
    "        return {\n",
    "            \"verdict\": \"Unknown\",\n",
    "            \"confidence\": \"Low\",\n",
    "            \"reason\": \"Model did not return valid JSON.\"\n",
    "        }\n",
    "\n",
    "def call_llm(llm, prompt):\n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content if hasattr(response, \"content\") else response\n",
    "\n",
    "# å®šç¾©åˆ†æå‡½å¼\n",
    "def analyze_message_with_multi_llm(message: str):\n",
    "    logic_input = llm_prompt.format(message=message, format_instructions=format_instructions)\n",
    "    debate_input = llm_prompt.format(message=message, format_instructions=format_instructions)\n",
    "    judge_input = llm_prompt.format(message=message, format_instructions=format_instructions)\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = {\n",
    "            executor.submit(call_llm, logic_llm, logic_input): \"logic\",\n",
    "            executor.submit(call_llm, debater_llm, debate_input): \"debate\",\n",
    "            executor.submit(call_llm, judge_llm, judge_input): \"judge\"\n",
    "        }\n",
    "        results = {}\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            key = futures[future]\n",
    "            results[key] = future.result()\n",
    "\n",
    "    summary_input = summary_prompt.format(\n",
    "        message=message,\n",
    "        logic_opinion=results[\"logic\"],\n",
    "        debate_opinion=results[\"debate\"],\n",
    "        your_opinion=results[\"judge\"],\n",
    "        format_instructions=format_instructions\n",
    "    )\n",
    "\n",
    "    final_response = judge_llm.invoke(summary_input)\n",
    "    result = extract_json(final_response.content)\n",
    "    return result\n",
    "\n",
    "# 6/17 batchå¯¦ä½œ\n",
    "# ========== Batchå‘¼å«llm ==========\n",
    "\n",
    "def batch_call_llm(llm, messages: list, batch_size: int, model_name: str) -> list:\n",
    "    outputs = []\n",
    "    for i in tqdm(range(0, len(messages), batch_size), desc=f\"Batching {model_name}\"):\n",
    "        batch = messages[i:i+batch_size]\n",
    "        prompts = [llm_prompt.format(message=msg, format_instructions=format_instructions) for msg in batch]\n",
    "        results = llm.batch(prompts)\n",
    "        outputs.extend([res.content if hasattr(res, \"content\") else res for res in results])\n",
    "    return outputs\n",
    "\n",
    "# ========== batchä¸»æµç¨‹ ==========\n",
    "# å…ˆç²—ç•¥æ ¹æ“šæ¨¡å‹é¤æ•¸é‡å¤§å°åˆ†batché‡\n",
    "def process_data_in_batches(df: pd.DataFrame, batch_logic=32, batch_debate=16, batch_judge=8) -> pd.DataFrame:\n",
    "    messages = df[\"text\"].tolist()\n",
    "    logic_results = batch_call_llm(logic_llm, messages, batch_logic, model_name=\"logic_llm\")\n",
    "    debate_results = batch_call_llm(debater_llm, messages, batch_debate, model_name=\"debater_llm\")\n",
    "    judge_results = batch_call_llm(judge_llm, messages, batch_judge, model_name=\"judge_llm\")\n",
    "\n",
    "    final_results = []\n",
    "    for i in tqdm(range(len(messages)), desc=\"Final summarization\"):\n",
    "        summary_input = summary_prompt.format(\n",
    "            message=messages[i],\n",
    "            logic_opinion=logic_results[i],\n",
    "            debate_opinion=debate_results[i],\n",
    "            your_opinion=judge_results[i],\n",
    "            format_instructions=format_instructions\n",
    "        )\n",
    "        final_response = judge_llm.invoke(summary_input)\n",
    "        parsed = extract_json(final_response.content)\n",
    "        final_results.append(parsed)\n",
    "\n",
    "    return pd.DataFrame(final_results)\n",
    "\n",
    "# ========== encode ==========\n",
    "\n",
    "def encode_verdict(verdict: str) -> int:\n",
    "    if verdict.strip().lower() == 'real':\n",
    "        return 1\n",
    "    elif verdict.strip().lower() == 'fake':\n",
    "        return 0\n",
    "    else:\n",
    "        return 2 # ä»£è¡¨ Unknown æˆ–å…¶ä»–ç„¡æ•ˆè¼¸å…¥\n",
    "\n",
    "def encode_confidence(conf: str) -> int:\n",
    "    mapping = {'low': 0, 'medium': 1, 'high': 2}\n",
    "    return mapping.get(conf.strip().lower(), 1)  # é è¨­çµ¦ä¿¡å¿ƒç¨‹åº¦1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4378c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# # è§€å¯Ÿæ¸¬è©¦ç”¨!!!\n",
    "# sample_texts = data['text'].sample(10, random_state=42)\n",
    "\n",
    "# for i, text in enumerate(sample_texts):\n",
    "#     result = analyze_message_with_multi_llm(text)\n",
    "#     print(\"æ¨è«–çµæœï¼š\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09167dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰€éœ€æ™‚é–“å¤ªé•·ï¼Œæ•…ä½¿ç”¨è¼ƒå°è³‡æ–™é›†\n",
    "\n",
    "# === éš¨æ©ŸæŠ½æ¨£ 200 ç­†åŸå§‹è³‡æ–™ ===\n",
    "# sampled_indices = data.sample(n=200, random_state=42).index\n",
    "\n",
    "# sampled_data = data.loc[sampled_indices]\n",
    "# X_final_sampled = X_final.loc[sampled_indices]\n",
    "# y_final_sampled = y_final.loc[sampled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aad227",
   "metadata": {},
   "source": [
    "### æ ¹æ“š LLM response æ–°å¢è³‡æ–™æ¬„ä½ ###\n",
    "\n",
    "ç¶“épromptå®šç¾©ã€JSONéæ¿¾å™¨ï¼Œæ¯ä¸€ç­†è³‡æ–™é æœŸå¾—åˆ°é¡ä¼¼ä»¥ä¸‹çµæ§‹response:\n",
    "{\n",
    "    \"verdict\": \"Real\", \n",
    "    \"confidence\": \"High\", \n",
    "    \"reason\": \"\"\n",
    "}\n",
    "\n",
    "æå‡ºverdictã€confidenceæ¬„ä½ä¸¦å„è‡ªåšencodeï¼Œå†æ–°å¢é€²è¦æ‹¿ä¾†è¨“ç·´çš„è³‡æ–™åšç‚ºæ–°çš„æ¬„ä½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569cba3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-LLM\n",
    "# llm_results = data['text'].progress_apply(analyze_message_with_multi_llm)\n",
    "# llm_df = pd.DataFrame(llm_results.tolist())\n",
    "\n",
    "# æ”¹æˆæ‰¹æ¬¡è™•ç†ç‰ˆæœ¬process_data_in_batches\n",
    "llm_df = process_data_in_batches(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a57b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and \n",
    "def clean_verdict(v):\n",
    "    if isinstance(v, list):\n",
    "        # å°‹æ‰¾ç¬¬ä¸€å€‹éç©ºå­—ä¸²\n",
    "        for item in v:\n",
    "            if isinstance(item, str) and item.strip():\n",
    "                return item\n",
    "        return 'Unknown'\n",
    "    elif isinstance(v, str):\n",
    "        return v\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "    \n",
    "def clean_confidence(c):\n",
    "    if isinstance(c, list):\n",
    "        for item in c:\n",
    "            if isinstance(item, str) and item.lower() in ['low', 'medium', 'high']:\n",
    "                return item\n",
    "        return 'medium'  # fallback\n",
    "    elif isinstance(c, str):\n",
    "        return c\n",
    "    else:\n",
    "        return 'medium'\n",
    "\n",
    "llm_df['verdict_clean'] = llm_df['verdict'].apply(clean_verdict)\n",
    "llm_df['verdict_encoded'] = llm_df['verdict_clean'].apply(encode_verdict)\n",
    "\n",
    "llm_df['confidence_clean'] = llm_df['confidence'].apply(clean_confidence)\n",
    "llm_df['confidence_encoded'] = llm_df['confidence_clean'].apply(encode_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b62fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜ multi-llm è¨è«–çš„çµæœ\n",
    "# ç¯©æ‰ verdict_encoded == 2 (multi-llm å›å‚³æœ‰å•é¡Œ)çš„æ¨£æœ¬\n",
    "valid_mask = llm_df['verdict_encoded'] != 2\n",
    "\n",
    "llm_df_filtered = llm_df.loc[valid_mask].reset_index(drop=True)\n",
    "X_final_filtered = X_final_sampled.iloc[valid_mask.values].reset_index(drop=True)\n",
    "y_final_filtered = y_final_sampled.iloc[valid_mask.values].reset_index(drop=True)\n",
    "\n",
    "X_final_filtered = pd.concat([X_final_filtered, llm_df_filtered[['verdict_encoded', 'confidence_encoded']]], axis=1)\n",
    "\n",
    "# å¦‚æœ X_final_filtered å·²æœ‰ 'verdict_encoded' å…ˆåˆªæ‰\n",
    "if 'verdict_encoded' in X_final_filtered.columns:\n",
    "    X_final_filtered = X_final_filtered.drop(columns=['verdict_encoded'])\n",
    "if 'confidence_encoded' in X_final_filtered.columns:\n",
    "    X_final_filtered = X_final_filtered.drop(columns=['confidence_encoded'])\n",
    "\n",
    "# å†åŠ ä¸Šæ–°çš„æ¬„ä½\n",
    "X_final_filtered = pd.concat([X_final_filtered, llm_df_filtered[['verdict_encoded', 'confidence_encoded']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f44577",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_final_filtered.columns[X_final_filtered.columns.duplicated()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fa6e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_final_filtered.shape)\n",
    "print(y_final_filtered.shape)\n",
    "print(X_final_filtered.isna().sum().sort_values(ascending=False).head(10)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf8bfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "# from sklearn.metrics import make_scorer, accuracy_score, f1_score, precision_score, recall_score\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# # classifier models\n",
    "# classifier_model = {\n",
    "#     \"LogReg\"       : LogisticRegression(max_iter=1000),\n",
    "#     \"DecisionTree\" : DecisionTreeClassifier(),\n",
    "#     \"SVM\"          : SVC(kernel='linear', probability=True),\n",
    "#     \"RandomForest\" : RandomForestClassifier(random_state=42)\n",
    "# }\n",
    "\n",
    "# scoring = {\n",
    "#     'accuracy': make_scorer(accuracy_score),\n",
    "#     'f1': make_scorer(f1_score),\n",
    "#     'precision': make_scorer(precision_score),\n",
    "#     'recall': make_scorer(recall_score)\n",
    "# }\n",
    "\n",
    "# # === init result ===\n",
    "# results = []\n",
    "\n",
    "# # === å»ºç«‹ Stratified K-Fold ===\n",
    "# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# # === è¨“ç·´æ¯å€‹æ¨¡å‹ ===\n",
    "# for name, model in classifier_model.items():\n",
    "#     print(f\"\\n è¨“ç·´æ¨¡å‹: {name}\")\n",
    "#     pipeline = Pipeline([\n",
    "#         ('scaler', StandardScaler()),  # å°æ‰€æœ‰ç‰¹å¾µæ¨™æº–åŒ–\n",
    "#         ('clf', model)\n",
    "#     ])\n",
    "#     scores = cross_validate(pipeline, X_final, y_final, cv=cv, scoring=scoring)\n",
    "#     result = {\n",
    "#         'model': name,\n",
    "#         'accuracy': np.mean(scores['test_accuracy']),\n",
    "#         'f1': np.mean(scores['test_f1']),\n",
    "#         'precision': np.mean(scores['test_precision']),\n",
    "#         'recall': np.mean(scores['test_recall'])\n",
    "#     }\n",
    "#     results.append(result)\n",
    "\n",
    "# # === æ•´ç†æˆ DataFrame é¡¯ç¤º ===\n",
    "# result_df = pd.DataFrame(results)\n",
    "# print(\"\\nå„æ¨¡å‹è©•ä¼°çµæœï¼š\")\n",
    "# print(result_df.sort_values(by='f1', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe4b334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# === Init ===\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "metrics_list = []\n",
    "\n",
    "# === é–‹å§‹äº¤å‰é©—è­‰ ===\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(X_final_filtered, y_final_filtered)):\n",
    "    X_train, X_test = X_final_filtered.iloc[train_idx], X_final_filtered.iloc[test_idx]\n",
    "    y_train, y_test = y_final_filtered.iloc[train_idx], y_final_filtered.iloc[test_idx]\n",
    "    \n",
    "    # åˆå§‹åŒ– LGBM æ¨¡å‹\n",
    "    clf = lgb.LGBMClassifier(\n",
    "        n_estimators=200,\n",
    "        random_state=42,\n",
    "        class_weight='balanced',  # è‹¥ label ä¸å¹³è¡¡å¯ä»¥é–‹å•Ÿ\n",
    "    )\n",
    "    \n",
    "    # æ¨¡å‹è¨“ç·´\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # é æ¸¬èˆ‡è©•ä¼°\n",
    "    y_pred = clf.predict(X_test)\n",
    "    metrics = {\n",
    "        'fold': fold + 1,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred)\n",
    "    }\n",
    "    print(f\"\\nğŸ“‹ Fold {fold+1} classification report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=[\"çœŸæ–°è\", \"å‡æ–°è\"]))\n",
    "    metrics_list.append(metrics)\n",
    "\n",
    "# === çµæœç¸½è¦½ ===\n",
    "df_result = pd.DataFrame(metrics_list)\n",
    "print(\"\\nğŸ“Š LightGBM - K-Fold æ•ˆèƒ½æ¯”è¼ƒï¼š\")\n",
    "print(tabulate(df_result, headers='keys', tablefmt='fancy_grid'))\n",
    "\n",
    "# === å¹³å‡è¡¨ç¾ ===\n",
    "avg_metrics = df_result.drop('fold', axis=1).mean().to_dict()\n",
    "print(\"\\nğŸ LightGBM å¹³å‡æ•ˆèƒ½:\")\n",
    "for m, v in avg_metrics.items():\n",
    "    print(f\"{m:10}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9daad09",
   "metadata": {},
   "source": [
    "+ **çµè«–**\n",
    "+ ç¶“éä¸‰å€‹LLM(LLaMA3:8b,Phi-3:3.8b,Mistral:7b)Multi-LLMä»²è£æ¨¡å¼ï¼ˆArbitration Modeï¼‰çµæœï¼Œä½¿ç”¨LightGBMåˆ†é¡å™¨ç”¢å‡ºæœ€çµ‚çµæœï¼ŒF1ç‚º0.6620ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60babe86",
   "metadata": {},
   "source": [
    "## **äº”ã€æœ€çµ‚çµè«–** ##\n",
    "+ çœŸå‡æ–°èçš„é æ¸¬ï¼Œ**ä½¿ç”¨ã€ŒNER+æƒ…ç·’+TFIDF+VADER+STYLEã€ç‰¹å¾µçš„çµæœæœ€å¥½**ï¼Œ`æœ€ä½³åˆ†é¡å™¨ï¼šRandomForest`ï¼Œ`Weighted F1 =0.8675 `ã€‚\n",
    "+ åœ¨åˆ†é¡å™¨è¡¨ç¾ä¸Šï¼Œå‚³çµ±çš„åˆ†é¡å™¨è¡¨ç¾ï¼Œæ¯”LLMè¡¨ç¾çµæœä½³ï¼Œæ¯”èµ·åŸæœ¬é æœŸLLMçš„è¡¨ç¾å¯èƒ½æœƒå¤§å¹…å„ªæ–¼å‚³çµ±åˆ†é¡å™¨çš„è¨“ç·´è¡¨ç¾ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2b2b75",
   "metadata": {},
   "source": [
    "## å…­ã€æœªä¾†å±•æœ› ##\n",
    "\n",
    "+ åœ¨ç‰¹å¾µé¸æ“‡æ–¹é¢ï¼Œç›®å‰æˆ‘å€‘ä»¥äººå·¥è§€å¯Ÿèˆ‡ç¶“é©—åˆ¤æ–·æ–¹å¼æ±ºå®šç‰¹å¾µçš„ä¿ç•™èˆ‡å¦ï¼Œä½†é€™æ¨£ä¸æ˜¯ç³»çµ±æ€§çš„å°‹æ‰¾æœ€å„ªç‰¹å¾µçµ„åˆã€‚æœªä¾†å¯è€ƒæ…®ä½¿ç”¨å¦‚ sklearn.feature_selection ä¸­çš„ RFE æ–¹æ³•ï¼Œæ ¹æ“šæ¨¡å‹æ¬Šé‡é€æ­¥å‰”é™¤ä¸é‡è¦çš„ç‰¹å¾µã€‚éœ€æ³¨æ„æ­¤æ”¹å‹•å¯èƒ½è€—è²»å¤§é‡æ™‚é–“ï¼›ä»¥æœ¬å°ˆæ¡ˆç‚ºä¾‹ï¼Œç›®å‰ä½¿ç”¨å…¨éƒ¨è³‡æ–™é›†èˆ‡ GPU é‹è¡Œä¸‹ï¼Œç´„éœ€ 12 å°æ™‚å·¦å³ï¼Œæ”¹å‹•å¾Œå¯èƒ½è€—è²»æ›´å¤šæ™‚é–“ã€‚\n",
    "\n",
    "+ æˆ‘å€‘æœ€çµ‚æ¡ç”¨é¡ä¼¼ Multi-Agent çš„æ¦‚å¿µï¼Œä½¿ç”¨å¤šå€‹ LLM å»ºæ§‹ã€Œä»²è£æ¨¡å¼ã€ä»¥æå‡åˆ¤æ–·æº–ç¢ºæ€§ã€‚è€ƒé‡é‹ç®—æ•ˆç‡ï¼Œç›®å‰æ¡ç”¨çš„åšæ³•ç‚ºå°‡å¤šç­†è³‡æ–™æ‰¹æ¬¡è¼¸å…¥åŒä¸€å€‹ LLMï¼Œé›–èƒ½ç¨å¾®åŠ å¿«é€Ÿåº¦ï¼Œå»å¯èƒ½é™ä½æ¨ç†å“è³ªï¼ˆå› éé€ç­†è¨è«–ã€ä¸Šä¸‹æ–‡ã€å‰å¾Œå°è©±é †åºå½±éŸ¿ï¼‰ã€‚æ­¤å¤–ï¼Œè§’è‰²è¨­å®šçš„ prompt è¼ƒç‚ºç°¡ç•¥ï¼Œå¾ŒçºŒå¯è€ƒæ…®åŠ å…¥æ›´å¤šè§’è‰²ã€å¤šæ®µæ€è€ƒï¼ˆå¦‚ CoTï¼‰ã€ä»¥åŠæ–°å¢ä¸€å€‹ LLM å°ˆé–€ç”¨ä¾†å¯©æŸ¥å‰æ®µæ¨ç†é‚è¼¯çš„æ­£ç¢ºæ€§ï¼Œé€²ä¸€æ­¥å¼·åŒ–æ•´é«”åˆ¤æ–·æµç¨‹çš„åš´è¬¹æ€§ã€‚\n",
    "\n",
    "+ åœ¨å¯¦ä½œä»²è£æ¨¡å¼çš„éç¨‹ä¸­ï¼Œå› æ¨ç†è€—æ™‚éé•·ï¼Œæœ€çµ‚æ”¹ä»¥éš¨æ©ŸæŠ½æ¨£ 200 ç­†è³‡æ–™é€²è¡Œæ¨è«–ï¼ˆä½¿ç”¨æŒ‡ä»¤ï¼š`sampled_indices = data.sample(n=200, random_state=42).index`ï¼‰ã€‚\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07b0a05b872c48ec8bc00ec063304866": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1bd81346eb5446ae86c8992ea87a8979": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3aa0234db1f4447da06ae0ba78593276",
       "IPY_MODEL_aaefd6793ed644b3afbf183bec859bff",
       "IPY_MODEL_e108b8551f0e41b69ce386e7cbdf3945"
      ],
      "layout": "IPY_MODEL_9c0fc22fc33a4388b5cde67492782329"
     }
    },
    "2ff3e28618eb412c9f1879564137be6c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30cfa6df7235440a8aaf12b38e7c1a24": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3aa0234db1f4447da06ae0ba78593276": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a7bd128141e746019bbd9eeed16eda0c",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c9da9c06c90a475ab7e45abc95f3dec3",
      "value": "Batches:â€‡100%"
     }
    },
    "953dd787d72045bc9382fef65869ced1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c0fc22fc33a4388b5cde67492782329": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7bd128141e746019bbd9eeed16eda0c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aaefd6793ed644b3afbf183bec859bff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_953dd787d72045bc9382fef65869ced1",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_30cfa6df7235440a8aaf12b38e7c1a24",
      "value": 5
     }
    },
    "c9da9c06c90a475ab7e45abc95f3dec3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e108b8551f0e41b69ce386e7cbdf3945": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2ff3e28618eb412c9f1879564137be6c",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_07b0a05b872c48ec8bc00ec063304866",
      "value": "â€‡5/5â€‡[00:09&lt;00:00,â€‡â€‡1.28s/it]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
