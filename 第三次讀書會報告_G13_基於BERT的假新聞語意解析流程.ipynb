{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4da74db",
   "metadata": {},
   "source": [
    "## 如果需要補全 期末專案 可參考 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11629ac2",
   "metadata": {
    "id": "FteSnH9FROVI"
   },
   "source": [
    "\n",
    "# 主題：新聞文本NLP處理與真假新聞分類\n",
    "#### 第十三組 第三次讀書會專案\n",
    "組員：\n",
    "N124020001 林坤郁\n",
    "N124020004 陳忠泰\n",
    "N124020005 陳莘惠\n",
    "N124020006 張祐倫\n",
    "N124020012 郭展州"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cb5555",
   "metadata": {},
   "source": [
    "<h2>一、資料簡介與目標</h2>\n",
    "<h3>1.1資料簡介</h3>\n",
    "資料來源Kaggle:https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset\n",
    "\n",
    "本專案使用的資料集為 Fake and Real News Dataset，由 Kaggle 提供，原始資料包含兩個檔案：\n",
    "<ol>\n",
    "<li>Fake.csv：包含假新聞（Fake News）之標題與內文</li>\n",
    "<li>True.csv：包含真新聞（Real News）之標題與內文</li>\n",
    "</ol>\n",
    "每筆資料具有以下欄位：\n",
    "<ol>\n",
    "<li>title：新聞標題</li>\n",
    "<li>text：新聞文字內容</li>\n",
    "<li>subject：主題類別，如政治、世界新聞等</li>\n",
    "<li>date：新聞發布時間</li>\n",
    "</ol>\n",
    "\n",
    "在資料前處理階段，我們將兩份資料加上標籤（label），其中：\n",
    "假新聞（Fake）標記為 0;真新聞（True）標記為 1\n",
    "並合併成一份可供分類任務使用的整合資料集。\n",
    "\n",
    "<h3>1.2目標</h3>\n",
    "本次專案主要任務為：\n",
    "<ol>\n",
    "<li>觀察資料特別與分布</li>\n",
    "<li>利用新聞文本內容來辨識新聞真偽（即真假新聞分類任務）\n",
    "這是一個典型的二元分類問題（Binary Classification），我們將嘗試透過不同的文本表示方式（Text Embedding），來建立分類模型，判斷一篇新聞是「真」還是「假」</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d5205d",
   "metadata": {
    "id": "fZy215-S1Kiv"
   },
   "source": [
    "<h3>二、引用套件與資料處理</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6387ca5",
   "metadata": {},
   "source": [
    "<h4>2.1載入套件</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115d3253",
   "metadata": {
    "executionInfo": {
     "elapsed": 9707,
     "status": "ok",
     "timestamp": 1745841542183,
     "user": {
      "displayName": "呂育真",
      "userId": "02881314335061949853"
     },
     "user_tz": -480
    },
    "id": "e4zJh4IklUkI"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import multiprocessing\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, util, models\n",
    "\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "from matplotlib.font_manager import fontManager\n",
    "import plotly.express as px\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt','stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7b01c0",
   "metadata": {
    "id": "JyP_HVbPvxdL"
   },
   "source": [
    "<h4>2.2資料處理</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5cf3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#讀檔\n",
    "trueNews = pd.read_csv('./raw_data/true.csv')\n",
    "fakeNews = pd.read_csv('./raw_data/fake.csv')\n",
    "#新增新聞類別欄位\n",
    "trueNews[\"label\"] = 1\n",
    "fakeNews[\"label\"] = 0\n",
    "#合併兩份檔案\n",
    "df = pd.concat([trueNews,fakeNews],axis = 0 ).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a453f7",
   "metadata": {},
   "source": [
    "把文章的title+text合併成為content欄位，做後續探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041c1c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"content\"] = df[\"title\"].astype(str) + \" : \" + df[\"text\"].astype(str)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d68397",
   "metadata": {},
   "source": [
    "移除移除標點符號、數字、小寫化等基本清理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895bc35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()                          # 小寫化\n",
    "    text = re.sub(r'\\d+', '', text)              # 移除數字\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)          # 移除標點符號\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()     # 移除多餘空白\n",
    "    return text\n",
    "\n",
    "df[\"content_clean\"] = df[\"content\"].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1756c42",
   "metadata": {},
   "source": [
    "使用 NLTK 進行英文斷詞（tokenization）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d0bd37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2916,
     "status": "ok",
     "timestamp": 1745841574140,
     "user": {
      "displayName": "呂育真",
      "userId": "02881314335061949853"
     },
     "user_tz": -480
    },
    "id": "0pYmTSvfmBZd",
    "outputId": "fd5a5e64-9ee7-45b4-e557-bd070c81f44f"
   },
   "outputs": [],
   "source": [
    "df[\"tokens\"] = df[\"content_clean\"].apply(word_tokenize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6950c574",
   "metadata": {},
   "source": [
    "去除停用詞（stopwords）與詞幹還原（stemming）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad67b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_tokens(tokens):\n",
    "    return [\n",
    "        stemmer.stem(word) \n",
    "        for word in tokens \n",
    "        if word not in stop_words and len(word) > 1\n",
    "    ]\n",
    "\n",
    "df[\"tokens_clean\"] = df[\"tokens\"].apply(preprocess_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c4e649",
   "metadata": {
    "id": "N_klM4cOcI3K"
   },
   "source": [
    "<h2>三、word2vec</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6317de",
   "metadata": {},
   "source": [
    "<h3>3.1依照資料訓練自己的模型</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50f01a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用tokens_clean 當input\n",
    "sentences = df[\"tokens_clean\"].tolist()\n",
    "\n",
    "# 建bigram模型\n",
    "bigram = Phrases(sentences, min_count=1, threshold=2)\n",
    "bigram_phraser = Phraser(bigram)\n",
    "\n",
    "# 用bigram 模型，把threshold以上的詞合併\n",
    "df[\"tokens_bigram\"] = df[\"tokens_clean\"].apply(lambda x: bigram_phraser[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6140ce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看機器的core\n",
    "cores = multiprocessing.cpu_count()\n",
    "print(f\"number of cores: {cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45bf11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只保tokens_bigram欄位的資料\n",
    "sentences = df[\"tokens_bigram\"].tolist()\n",
    "\n",
    "# 8:2切資料\n",
    "sentences_train, sentences_test = train_test_split(sentences, test_size=0.2, random_state=42)\n",
    "\n",
    "# 訓練 Word2Vec 模型\n",
    "w2v_model = Word2Vec(\n",
    "    sentences = sentences_train,    # 使用訓練語料\n",
    "    vector_size = 128,              # vector 的維度\n",
    "    alpha = 0.0005, #learning rate\n",
    "    window = 2,                    # 上下文視窗大小\n",
    "    min_count = 30,                  # 忽略出現次數少於30次的詞\n",
    "    workers= cores-2,                  # CPU core\n",
    "    sg = 1,                         # 1 = skip-gram, 0 = CBOW\n",
    "    epochs = 30,\n",
    "    hs = 1,\n",
    "    seed = 8888\n",
    ")\n",
    "\n",
    "# 訓練模型\n",
    "w2v_model.train(sentences_train, total_examples=len(sentences_train), epochs=10)\n",
    "\n",
    "# 儲存模型\n",
    "w2v_model.save(\"w2v_fake_news.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb402da",
   "metadata": {},
   "source": [
    "查看結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f91bfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢查最相關的字 (cosine similarity)\n",
    "w2v_model.wv.most_similar('trump',topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3824cb",
   "metadata": {},
   "source": [
    "Trump 的相關詞包含 presid_trump、trump_campaign、trump_administr、mr_trump。\n",
    "\n",
    "語意模型能成功捕捉語料中的政治語意脈絡。\n",
    "可以說明：這些關聯詞與 Trump 的政治背景、高頻議題高度關聯，模型語意學習是符合預期且有效的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31827d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar('trump','biden',topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e97cf0",
   "metadata": {},
   "source": [
    "跟兩個字最不相關"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495f8797",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(negative=['trump','biden'],topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a801e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算兩個字之間的關係\n",
    "w2v_model.wv.similarity(\"china\",\"america\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc052468",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.similarity(\"china\",\"taiwan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228d3214",
   "metadata": {},
   "source": [
    "china 與 america 的相似度約 0.34，代表兩者語意在語料中是相對較遠。\n",
    "china 與 taiwan 的相似度高達 0.81，顯示語料中的這兩個詞出現脈絡極為接近。\n",
    "\n",
    "雖然這三個詞都是地名 但在 Word2Vec 看語意距離有所差距，說明了語意相似不等於實體分類相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a685a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取得所有的字\n",
    "words = w2v_model.wv.key_to_index.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1bb438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 降維：利用PCA tSNE\n",
    "\n",
    "def reduceDim(mat,method:str='PCA',dim:str=2,perplexity = 25,learning_rate = 400):\n",
    "\n",
    "    method_dict = {\n",
    "        \"PCA\":PCA(n_components=dim,iterated_power = 1000,random_state=0),\n",
    "        \"TSNE\":TSNE(n_components=dim,random_state=0,perplexity=perplexity,learning_rate=learning_rate),\n",
    "    }\n",
    "    new_feat = method_dict[method].fit_transform(mat)\n",
    "\n",
    "    return new_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6db07f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拿到list of words 的vector\n",
    "def getVecs(model,words:list):\n",
    "    vecs = []\n",
    "    for i in words:\n",
    "        vecs.append(model.wv[i])\n",
    "    return np.vstack(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d01de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "getVecs(w2v_model,['taiwan','china'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c18c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 擴展相似的字詞\n",
    "def expandPosWord(model, words:list, top_n:int, split = True):\n",
    "\n",
    "    if split == False:\n",
    "        wp = model.wv.most_similar(words,topn = top_n)\n",
    "        return wp\n",
    "    expand = []\n",
    "\n",
    "    for w in words:\n",
    "        wp = model.wv.most_similar(w,topn = top_n)\n",
    "        for i in wp:\n",
    "            expand.append(i[0])\n",
    "\n",
    "    return list(set(expand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7cc75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "expandPosWord(w2v_model,['china','taiwan'],top_n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7c683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 畫出兩維的散佈圖\n",
    "def plotScatter(vec_df):\n",
    "    \"\"\"\n",
    "    vec_df: 字詞及其兩個維度的值\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15,15))\n",
    "    fontManager.addfont('./TaipeiSansTCBeta-Regular.ttf')\n",
    "    plt.rcParams['font.sans-serif'] = ['Taipei Sans TC Beta']\n",
    "    plt.rcParams['font.size'] = '16'\n",
    "\n",
    "    p = sns.scatterplot(x=\"dim1\", y=\"dim2\",\n",
    "                  data=vec_df)\n",
    "    for line in range(0, vec_df.shape[0]):\n",
    "         p.text(vec_df[\"dim1\"][line],\n",
    "                 vec_df['dim2'][line],\n",
    "                 '  ' + vec_df[\"word\"][line].title(),\n",
    "                 horizontalalignment='left',\n",
    "                 verticalalignment='bottom', size='medium',\n",
    "                 weight='normal'\n",
    "                ).set_size(15)\n",
    "    plt.show()\n",
    "\n",
    "# 畫出三維的散佈圖\n",
    "def plotScatter3D(vec_df):\n",
    "    vec_df['size'] = .5\n",
    "    if 'color' not in vec_df.columns:\n",
    "        vec_df['color'] = 'blue'\n",
    "    fig = px.scatter_3d(\n",
    "        vec_df,'dim1','dim2','dim3',text = 'word',width=800, height=800,color = 'color',size = 'size'\n",
    "\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da40fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_words = np.random.choice(list(words),150,replace=False).tolist()\n",
    "\n",
    "feat = getVecs(model=w2v_model,words=sample_words)\n",
    "print(feat.shape)\n",
    "new_feat = reduceDim(feat,method='TSNE',perplexity=20,learning_rate = 800)\n",
    "print(new_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88b7ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = pd.DataFrame({\n",
    "    \"word\":sample_words,\n",
    "    \"dim1\":new_feat[:,0],\n",
    "    \"dim2\":new_feat[:,1],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822b8909",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotScatter(word_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42c6ce3",
   "metadata": {},
   "source": [
    "3D 散狀圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5070ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feat = reduceDim(feat,dim = 3,method = 'PCA' )\n",
    "print(new_feat.shape)\n",
    "word_df = pd.DataFrame({\n",
    "    \"word\":sample_words,\n",
    "    \"dim1\":new_feat[:,0],\n",
    "    \"dim2\":new_feat[:,1],\n",
    "    \"dim3\":new_feat[:,2],\n",
    "})\n",
    "plotScatter3D(word_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38587acb",
   "metadata": {},
   "source": [
    "將字分群"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616e1f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn-extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ffe137",
   "metadata": {},
   "outputs": [],
   "source": [
    "#只使用word vector 去分群\n",
    "def cluster(X,method = 'kmeans',n = 2):\n",
    "\n",
    "    method_dict = {\n",
    "        'kmeans':KMeans(n_clusters=n, random_state=0),\n",
    "        'kmedos':KMedoids(n_clusters=n, random_state=0)\n",
    "    }\n",
    "    method_dict[method].fit(X)\n",
    "    result = method_dict[method].predict(X)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f36a2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feat = reduceDim(feat,method='PCA',dim = 20)\n",
    "d3_feat = reduceDim(feat,method='PCA',dim = 3)\n",
    "word_df = pd.DataFrame({\n",
    "    \"word\":sample_words,\n",
    "    \"color\":cluster(new_feat,n=4),\n",
    "    \"dim1\":d3_feat[:,0],\n",
    "    \"dim2\":d3_feat[:,1],\n",
    "    \"dim3\":d3_feat[:,2],\n",
    "\n",
    "})\n",
    "plotScatter3D(word_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acfaeee",
   "metadata": {},
   "source": [
    "<h3>3.2 Transformers Embeddings</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1db24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d08944",
   "metadata": {},
   "source": [
    "<h4>小模型，以BERT為範例</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084447f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 英文 bert-base-uncased\n",
    "bert_en = SentenceTransformer('google-bert/bert-base-uncased')\n",
    "\n",
    "bert_en.tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d1ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_en.get_max_seq_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb3994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回傳cosine similarity分數\n",
    "def get_result_df(sentences, cosine_scores):\n",
    "\n",
    "  result = []\n",
    "  for i in range(len(sentences)):\n",
    "      for j in range(i+1, len(sentences)):\n",
    "          result.append([sentences[i], sentences[j], cosine_scores[i][j].item()])\n",
    "\n",
    "  result_df = pd.DataFrame(result, columns=[\"sentence1\", \"sentence2\", \"score\"])\n",
    "  result_df = result_df.sort_values(\"score\", ascending = False)\n",
    "\n",
    "  return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69318d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多語言 bert-base-multilingual-cased\n",
    "bert_multi = SentenceTransformer('google-bert/bert-base-multilingual-cased')\n",
    "\n",
    "bert_multi.tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f761c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示範句子\n",
    "sentences = [\n",
    "    \"Trump's treatening the world under the tariff policy.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"川普正在藉由關稅政策威脅世界。\"\n",
    "]\n",
    "\n",
    "# 使用 encode() 對資料做embedding\n",
    "embeddings_multi = bert_multi.encode(sentences)\n",
    "\n",
    "# Compute cosine-similarities\n",
    "cosine_scores = util.cos_sim(embeddings_multi, embeddings_multi)\n",
    "\n",
    "# 印出句子間的cosine similarity分數\n",
    "result_df = get_result_df(sentences, cosine_scores)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccc2617",
   "metadata": {},
   "source": [
    "<h3>四、使用embedding做NLP任務</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b323c8cc",
   "metadata": {},
   "source": [
    "<h4>4.1相似文件</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ebbda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用英文預訓練 BERT 模型（適合英文句子）\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 將所有新聞內容轉換為向量\n",
    "corpus_embeddings = model.encode(df['content'].tolist(), convert_to_tensor=True, batch_size=32)\n",
    "\n",
    "# 任選一篇新聞作為查詢對象（你可自行更換 query_num 數值）\n",
    "query_num = 10\n",
    "query_embedding = model.encode(df['content'][query_num], convert_to_tensor=True)\n",
    "\n",
    "# 計算與其他新聞的 cosine similarity\n",
    "cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "\n",
    "# 取相似度前五高的文章\n",
    "top_k = 5\n",
    "top_results = torch.topk(cos_scores, k=top_k)\n",
    "\n",
    "# 顯示結果\n",
    "print(\"\\n\\n======================\")\n",
    "print(\"查詢新聞標題:\\n\", df['title'][query_num])\n",
    "print(\"\\n最相似的新聞:\")\n",
    "\n",
    "for score, idx in zip(top_results[0], top_results[1]):\n",
    "    print(f\"{df['title'][idx.item()]} (Score: {score:.4f})\")\n",
    "\n",
    "print(\"======================\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50002bb",
   "metadata": {},
   "source": [
    "<h4>4.2分類任務</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5fb0f8",
   "metadata": {},
   "source": [
    "轉成向量（用 title+text 合併的 content 欄位）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c5ca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df[\"content\"].tolist()\n",
    "embeddings = model.encode(corpus, batch_size=32, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64197e36",
   "metadata": {},
   "source": [
    "建立分類器（以 sklearn的邏輯回歸 ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652baf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, df[\"label\"], test_size=0.2, random_state=42)\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a985928",
   "metadata": {},
   "source": [
    "包成function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005dbee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_news_authenticity(title: str, content: str, model_embed, clf):\n",
    "    \"\"\"\n",
    "    輸入新聞標題與內文，使用已訓練的嵌入模型與分類器預測真假。\n",
    "    \n",
    "    參數：\n",
    "    - title: 新聞標題（字串）\n",
    "    - content: 新聞內文（字串）\n",
    "    - model_embed: SentenceTransformer 模型\n",
    "    - clf: 訓練好的分類模型（如 LogisticRegression）\n",
    "\n",
    "    輸出：\n",
    "    - 預測結果（True/Fake）\n",
    "    - True 機率\n",
    "    - Fake 機率\n",
    "    \"\"\"\n",
    "    full_text = f\"{title.strip()} : {content.strip()}\"\n",
    "    embedding = model_embed.encode(full_text).reshape(1, -1)\n",
    "    pred_label = clf.predict(embedding)[0]\n",
    "    pred_prob = clf.predict_proba(embedding)[0]\n",
    "\n",
    "    result = \"True\" if pred_label == 1 else \"Fake\"\n",
    "    true_prob = round(pred_prob[1], 4)\n",
    "    fake_prob = round(pred_prob[0], 4)\n",
    "\n",
    "    print(f\"預測結果：{result}\")\n",
    "    print(f\"True 機率：{true_prob}\")\n",
    "    print(f\"Fake 機率：{fake_prob}\")\n",
    "    \n",
    "    return result, true_prob, fake_prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2077866c",
   "metadata": {},
   "source": [
    "# 5.BERT (Encoder-only-model)\n",
    "\n",
    "- Token classification \n",
    "    NER\n",
    "- Sequence classification\n",
    "    Sentiment Classification\n",
    "    Relation Extraction (RE)\n",
    "- Text Clustering (BERTopic)\n",
    "    Embedding model\n",
    "    Clustering model\n",
    "    使用Representation方法去微調主題表示\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b54574",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 載入資料集\n",
    "fake_df = pd.read_csv('./raw_data/fake.csv')\n",
    "true_df = pd.read_csv('./raw_data/true.csv')\n",
    "\n",
    "# 加上 label 欄位\n",
    "fake_df['label'] = 1\n",
    "true_df['label'] = 0\n",
    "\n",
    "# 取前 10000 筆\n",
    "data = pd.concat([fake_df.iloc[:10000], true_df.iloc[:10000]], ignore_index=True)\n",
    "data = data[data['text'].notna()].reset_index(drop=True)\n",
    "\n",
    "# 檢查各類別數量\n",
    "print(data['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7b5ac9",
   "metadata": {},
   "source": [
    "## 需要做文本預處理嗎?\n",
    "\n",
    "目的:\n",
    "- 建立分類器來預測真假新聞 -> (TF-IDF + 分類模型需要乾淨的資料，有幫助)\n",
    "- 分析NER 結果與語意分佈 -> (會破壞語意)\n",
    "- 建立主題模型來探索語意主題（BERTopic -> (會破壞語意)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e324aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z ]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "data['tokens'] = data['text'].astype(str).apply(preprocess)\n",
    "data['clean_text'] = data['tokens'].apply(lambda x: ' '.join(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68ddd12",
   "metadata": {},
   "source": [
    "## NER 預測新聞真假"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81df51b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.font_manager import fontManager\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fontManager.addfont('./TaipeiSansTCBeta-Regular.ttf')\n",
    "plt.rcParams['font.sans-serif'] = ['Taipei Sans TC Beta']\n",
    "plt.rcParams['font.size'] = '16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29def4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from hdbscan import HDBSCAN\n",
    "from transformers import BertTokenizerFast, AutoTokenizer, AutoModelForTokenClassification, AutoModelForSequenceClassification, pipeline\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# 設定裝置：優先使用 Apple GPU (MPS)，否則用 CPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\" 使用裝置：{device}\")\n",
    "\n",
    "# 載入 BERT 模型與 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\").to(device)\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\",\n",
    "                        device=0 if device.type != \"cpu\" else -1)\n",
    "\n",
    "# 分段函數（確保不超過 512 token）\n",
    "def split_text(text, chunk_size=512):\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# 初始化結果列表\n",
    "ner_rows = []\n",
    "\n",
    "# 逐篇文章處理，加雙層進度條\n",
    "for idx, text in tqdm(data['text'].astype(str).items(), desc=\"🔍 處理文章進度\"):\n",
    "    try:\n",
    "        chunks = split_text(text)\n",
    "        all_ents = []\n",
    "        with torch.no_grad():  # 加速推論用\n",
    "            for chunk in tqdm(chunks, leave=False, desc=f\"文章 {idx} 分段中\"):\n",
    "                all_ents.extend(ner_pipeline(chunk))\n",
    "        for ent in all_ents:\n",
    "            ner_rows.append({\n",
    "                \"index\": idx,\n",
    "                \"entity\": ent['entity_group'],\n",
    "                \"word\": ent['word'],\n",
    "                \"score\": ent['score']\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\" 第 {idx} 筆資料錯誤: {e}\")\n",
    "\n",
    "# 轉為 DataFrame\n",
    "ner_df = pd.DataFrame(ner_rows)\n",
    "ner_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875c0283",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbf7dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整合 label\n",
    "merged_df = ner_df.merge(data[['label']], left_on='index', right_index=True)\n",
    "\n",
    "# 聚合所有 entity 類型的出現次數\n",
    "entity_counts_all = (\n",
    "    merged_df.groupby(['index', 'entity'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)  # 得到每篇文章各類實體數\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 合併 label\n",
    "entity_counts_all = entity_counts_all.merge(data[['label']], left_on='index', right_index=True)\n",
    "\n",
    "# 建模欄位選擇：所有實體類別欄位（排除 index, label）\n",
    "feature_cols = [col for col in entity_counts_all.columns if col not in ['index', 'label']]\n",
    "kmeans_fit_pred_data = entity_counts_all[feature_cols]\n",
    "\n",
    "# 做 KMeans 聚類\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "entity_counts_all['cluster'] = kmeans.fit_predict(kmeans_fit_pred_data)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(kmeans_fit_pred_data)\n",
    "entity_counts_all['PC1'] = X_pca[:, 0]\n",
    "entity_counts_all['PC2'] = X_pca[:, 1]\n",
    "# 視覺化聚類結果\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=entity_counts_all,\n",
    "    x='PC1', y='PC2', hue='cluster', style='label',\n",
    "    palette='Set2', s=100\n",
    ")\n",
    "\n",
    "plt.title('NER 特徵的主成分分析 + KMeans 聚類')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e6e5bf",
   "metadata": {},
   "source": [
    "#### 嘗試用 NER 提取出的'人名'、'組織'、'地名數量'作為詞彙特徵，再餵給 TF-IDF + 模型來預測這篇新聞是真/假"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d807d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 聚合 ner_df 結果為特徵表（以 index = 文章編號為 key）\n",
    "entity_counts = ner_df.groupby(['index', 'entity']).size().unstack(fill_value=0)\n",
    "\n",
    "# 合併回原資料集\n",
    "data_with_ner = data.copy()\n",
    "data_with_ner = data_with_ner.join(entity_counts, how='left').fillna(0)\n",
    "\n",
    "# 建立特徵：人名、組織、地名數量\n",
    "ner_pred_X = data_with_ner[['PER', 'ORG', 'LOC']]\n",
    "ner_pred_y = data_with_ner['label']\n",
    "\n",
    "# 建模\n",
    "# Logistic Regression\n",
    "X_train, X_test, y_train, y_test = train_test_split(ner_pred_X, ner_pred_y, test_size=0.2, random_state=42)\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train, y_train)\n",
    "lr_pred = lr_clf.predict(X_test)\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "rf_y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "# Logistic Regression 結果\n",
    "print(\"=== LogisticRegression 分類結果 ===\")\n",
    "print(classification_report(y_test, lr_pred))\n",
    "\n",
    "# Random Forest 結果\n",
    "print(\"=== RandomForestClassifier 分類結果 ===\")\n",
    "print(classification_report(y_test, rf_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1e1c16",
   "metadata": {},
   "source": [
    "### NER提取特徵預測結果尚可\n",
    "小結:\n",
    "預測真新聞:  LR      RF\n",
    "precision   0.71    0.75\n",
    "recall      0.62    0.74\n",
    "f1          0.66    0.74\n",
    "\n",
    "預測假新聞:\n",
    "precision   0.66    0.74\n",
    "recall      0.74    0.75\n",
    "f1          0.70    0.74\n",
    "\n",
    "NER 特徵對真假新聞辨識有一定程度作用，且用RandomForest的結果較優"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400ff9f4",
   "metadata": {},
   "source": [
    "## 使用情緒分析辨識真假新聞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d64c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入情緒分析模型\n",
    "model = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "# 因為這個語言也是BERT = 效果仰賴'自然語言語序與上下文' = 使用data['text']即可\n",
    "\n",
    "# 切割文字 每段不超過 512 字\n",
    "def split_text(text, chunk_size=512):\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# 整合段落的情緒分數\n",
    "def analyze_long_text(text):\n",
    "    try:\n",
    "        chunks = split_text(text)\n",
    "        results = model(chunks)\n",
    "\n",
    "        # 統計情緒\n",
    "        pos_scores = [r['score'] for r in results if r['label'] == 'POSITIVE']\n",
    "        neg_scores = [r['score'] for r in results if r['label'] == 'NEGATIVE']\n",
    "\n",
    "        # 平均分數\n",
    "        avg_pos = sum(pos_scores) / len(pos_scores) if pos_scores else 0\n",
    "        avg_neg = sum(neg_scores) / len(neg_scores) if neg_scores else 0\n",
    "\n",
    "        # 決定總體情續\n",
    "        if avg_pos > avg_neg:\n",
    "            return pd.Series(['POSITIVE', avg_pos])\n",
    "        elif avg_neg > avg_pos:\n",
    "            return pd.Series(['NEGATIVE', avg_neg])\n",
    "        else:\n",
    "            return pd.Series(['NEUTRAL', 0.5])\n",
    "    except Exception:\n",
    "        return pd.Series(['ERROR', 0.0])\n",
    "\n",
    "# 執行分析\n",
    "tqdm.pandas()\n",
    "data[['sentiment_label', 'sentiment_score']] = data['text'].progress_apply(analyze_long_text)\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349aa9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pred_X = data[['sentiment_score']]\n",
    "sentiment_pred_y = data['label']\n",
    "\n",
    "# 分割訓練與測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentiment_pred_X, sentiment_pred_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 建立模型並訓練\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train, y_train)\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# 預測與評估\n",
    "y_pred = lr_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 預測與評估\n",
    "rf_y_pred = rf_clf.predict(X_test)\n",
    "print(classification_report(y_test, rf_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcccc87d",
   "metadata": {},
   "source": [
    "### 小結: 情緒預測真假新聞表現不好\n",
    "小結:\n",
    "預測真新聞:  LR      RF\n",
    "precision   0.60    0.52\n",
    "recall      0.37    0.50\n",
    "f1          0.46    0.51\n",
    "\n",
    "預測假新聞:\n",
    "precision   0.54    0.52\n",
    "recall      0.75    0.54\n",
    "f1          0.63    0.53\n",
    "\n",
    "整體分類效果偏弱，跟丟銅板差不多\n",
    "模型偏好預測為假新聞（recall 高），但也多誤判"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e0b838",
   "metadata": {},
   "source": [
    "### 嘗試整合兩者"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec497be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "combined_X = pd.concat([data_with_ner[['PER', 'ORG', 'LOC']], sentiment_pred_X], axis=1)\n",
    "combined_y = data['label']\n",
    "# 分割訓練與測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_X, combined_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression\n",
    "clf_lr = LogisticRegression()\n",
    "clf_lr.fit(X_train, y_train)\n",
    "lr_preds = clf_lr.predict(X_test)\n",
    "\n",
    "# Random Forest\n",
    "clf_rf = RandomForestClassifier(random_state=42)\n",
    "clf_rf.fit(X_train, y_train)\n",
    "rf_preds = clf_rf.predict(X_test)\n",
    "\n",
    "# 評估結果\n",
    "print(\"=== Logistic Regression 分類結果 ===\")\n",
    "print(classification_report(y_test, lr_preds))\n",
    "\n",
    "print(\"=== Random Forest 分類結果 ===\")\n",
    "print(classification_report(y_test, rf_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f3f453",
   "metadata": {},
   "source": [
    "### 整合特徵後整體預測準確度上升\n",
    "\n",
    "NER:\n",
    "=== LogisticRegression 分類結果 ===\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.71      0.62      0.66       201\n",
    "           1       0.66      0.74      0.70       199\n",
    "\n",
    "    accuracy                           0.68       400\n",
    "   macro avg       0.68      0.68      0.68       400\n",
    "weighted avg       0.68      0.68      0.68       400\n",
    "\n",
    "=== RandomForestClassifier 分類結果 ===\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.75      0.74      0.74       201\n",
    "           1       0.74      0.75      0.75       199\n",
    "\n",
    "    accuracy                           0.74       400\n",
    "   macro avg       0.75      0.75      0.74       400\n",
    "weighted avg       0.75      0.74      0.74       400\n",
    "\n",
    "\n",
    "\n",
    "情緒分析:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.60      0.37      0.46       201\n",
    "           1       0.54      0.75      0.63       199\n",
    "\n",
    "    accuracy                           0.56       400\n",
    "   macro avg       0.57      0.56      0.55       400\n",
    "weighted avg       0.57      0.56      0.55       400\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.52      0.50      0.51       201\n",
    "           1       0.52      0.54      0.53       199\n",
    "\n",
    "    accuracy                           0.52       400\n",
    "   macro avg       0.52      0.52      0.52       400\n",
    "weighted avg       0.52      0.52      0.52       400\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "整合後:\n",
    "=== Logistic Regression 分類結果 ===\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.73      0.67      0.70       201\n",
    "           1       0.69      0.75      0.72       199\n",
    "\n",
    "    accuracy                           0.71       400\n",
    "   macro avg       0.71      0.71      0.71       400\n",
    "weighted avg       0.71      0.71      0.71       400\n",
    "\n",
    "=== Random Forest 分類結果 ===\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.80      0.77      0.79       201\n",
    "           1       0.78      0.81      0.79       199\n",
    "\n",
    "    accuracy                           0.79       400\n",
    "   macro avg       0.79      0.79      0.79       400\n",
    "weighted avg       0.79      0.79      0.79       400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6497f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 嘗試增加TF-IDF欄位(clean_text)\n",
    "\n",
    "# 建立 TF-IDF 向量器（可自訂 ngram 範圍與維度限制）\n",
    "tfidf = TfidfVectorizer(max_features=200, ngram_range=(1, 2))\n",
    "tfidf_matrix = tfidf.fit_transform(data['clean_text'].fillna(''))\n",
    "\n",
    "# 轉為 DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out(), index=data.index)\n",
    "\n",
    "# 新增tf-idf欄位\n",
    "ner_sentiment_df = pd.concat([ner_pred_X, sentiment_pred_X], axis=1)\n",
    "combined_X_full = pd.concat([ner_sentiment_df, tfidf_df], axis=1)\n",
    "\n",
    "# 分割資料\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_X_full, combined_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic\n",
    "clf_lr = LogisticRegression()\n",
    "clf_lr.fit(X_train, y_train)\n",
    "lr_preds = clf_lr.predict(X_test)\n",
    "\n",
    "# Random Forest\n",
    "clf_rf = RandomForestClassifier(random_state=42)\n",
    "clf_rf.fit(X_train, y_train)\n",
    "rf_preds = clf_rf.predict(X_test)\n",
    "\n",
    "# 評估\n",
    "print(\"=== Logistic Regression(NER + Sentiment + TF-IDF) ===\")\n",
    "print(classification_report(y_test, lr_preds))\n",
    "\n",
    "print(\"=== Random Forest(NER + Sentiment + TF-IDF) ===\")\n",
    "print(classification_report(y_test, rf_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28223dfe",
   "metadata": {},
   "source": [
    "### 過擬合?\n",
    "\n",
    "=== Logistic Regression(NER + Sentiment + TF-IDF) ===\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.97      1.00      0.98       201\n",
    "           1       0.99      0.96      0.98       199\n",
    "\n",
    "    accuracy                           0.98       400\n",
    "   macro avg       0.98      0.98      0.98       400\n",
    "weighted avg       0.98      0.98      0.98       400\n",
    "\n",
    "=== Random Forest(NER + Sentiment + TF-IDF) ===\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00       201\n",
    "           1       0.99      1.00      1.00       199\n",
    "\n",
    "    accuracy                           1.00       400\n",
    "   macro avg       1.00      1.00      1.00       400\n",
    "weighted avg       1.00      1.00      1.00       400\n",
    "\n",
    "- 修正: TfidfVectorizer max_features=1000 -> 200\n",
    "- 修正: 資料各取1000筆 -> 10000筆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242e57f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(clf_rf, combined_X_full, combined_y, cv=5, scoring='f1')\n",
    "print(f\"5-fold F1 scores: {scores}\")\n",
    "print(f\"Mean F1: {scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa66961",
   "metadata": {},
   "source": [
    "### Topic model: BERTopic 主題詞來源使用c-TF-IDF頻率導向，表現方式偏向詞頻高的詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19be7f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 真假新聞進行主題建模\n",
    "docs = data['text'].astype(str).tolist()\n",
    "\n",
    "# 模型可換成 'all-MiniLM-L6-v2', 'microsoft/Phi-4-mini-instruct' 等\n",
    "embedding_model = 'all-MiniLM-L6-v2'\n",
    "\n",
    "# 可調整 測試用2000筆\n",
    "# min_cluster_size 群集最少需要包含n個點，否則會被視為雜訊（noise）\n",
    "# min_samples 包含至少n篇文章的主題才會被承認為主題\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=10, min_samples=30) # Clustering layer\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\")\n",
    "\n",
    "topic_model = BERTopic(embedding_model=embedding_model, hdbscan_model=hdbscan_model, vectorizer_model=vectorizer_model)\n",
    "topics, probs = topic_model.fit_transform(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3442db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立一個儲存所有主題關鍵詞與 TF-IDF 分數的清單\n",
    "all_topics = []\n",
    "\n",
    "# 把主題總數拿出來（排除 -1 是未分類主題）\n",
    "valid_topics = [topic for topic in topic_model.get_topic_info().Topic if topic != -1]\n",
    "\n",
    "# 對每個主題取得詞與 c-TF-IDF 分數\n",
    "for topic_id in valid_topics:\n",
    "    topic_words = topic_model.get_topic(topic_id)\n",
    "    for word, score in topic_words:\n",
    "        all_topics.append({\n",
    "            \"Topic\": topic_id,\n",
    "            \"Word\": word,\n",
    "            \"C-TF-IDF\": score\n",
    "        })\n",
    "\n",
    "# 轉換成 DataFrame 並排序\n",
    "topic_tfidf_df = pd.DataFrame(all_topics)\n",
    "topic_tfidf_df = topic_tfidf_df.sort_values(by=[\"Topic\", \"C-TF-IDF\"], ascending=[True, False])\n",
    "\n",
    "# 顯示前幾列\n",
    "topic_tfidf_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf36076",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 列出文章的BERTopic資訊\n",
    "topic_model.get_document_info(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4088184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_fake_news_ratio_by_topic(model, docs, labels, title=\"主題的假新聞比例\"):\n",
    "    doc_info = model.get_document_info(docs).copy()\n",
    "    doc_info['label'] = labels\n",
    "\n",
    "    # 計算比例與數量\n",
    "    topic_fake_ratio = (\n",
    "        doc_info[doc_info['Topic'] != -1]\n",
    "        .groupby('Topic')['label']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={'label': 'fake_news_ratio'})\n",
    "    )\n",
    "    topic_counts = (\n",
    "        doc_info[doc_info['Topic'] != -1]['Topic']\n",
    "        .value_counts()\n",
    "        .rename_axis('Topic')\n",
    "        .reset_index(name='count')\n",
    "    )\n",
    "    topic_stats = pd.merge(topic_fake_ratio, topic_counts, on='Topic')\n",
    "\n",
    "    # 加上主題名稱\n",
    "    topic_names = model.get_topic_info()[['Topic', 'Name']]\n",
    "    topic_stats_named = topic_stats.merge(topic_names, on='Topic')\n",
    "\n",
    "    # 過濾比例過低的主題\n",
    "    topic_stats_named = topic_stats_named[topic_stats_named['fake_news_ratio'] >= 0.1]\n",
    "\n",
    "    # 繪圖\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ax = sns.barplot(\n",
    "        data=topic_stats_named.sort_values(by='fake_news_ratio', ascending=False),\n",
    "        x='fake_news_ratio', y='Name', palette='Reds'\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel('假新聞比例 (label=1)')\n",
    "    plt.ylabel('主題代表詞')\n",
    "    plt.grid(True, axis='x')\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55d4a85",
   "metadata": {},
   "source": [
    "### representation topic model: 加上語意導向的KeyBERT, 表現方式是語意向量相似的詞 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281f408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic.representation import KeyBERTInspired\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model_with_st = SentenceTransformer(embedding_model)  # 或其他你指定的模型\n",
    "embeddings = embedding_model_with_st.encode(docs, show_progress_bar=True)\n",
    "\n",
    "# 關鍵詞表示模型（非生成式）\n",
    "keybert = KeyBERTInspired()\n",
    "\n",
    "# 組裝 representation model\n",
    "representation_model = {\n",
    "    \"KeyBERT\": keybert\n",
    "}\n",
    "\n",
    "# 建立 BERTopic 模型（用 KeyBERT 調整主題表示）\n",
    "representation_topic_model = BERTopic(\n",
    "    embedding_model=embedding_model_with_st,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    representation_model=representation_model,\n",
    "    top_n_words=30,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 訓練模型\n",
    "topics, probs = representation_topic_model.fit_transform(docs, embeddings)\n",
    "\n",
    "# 查看新的主題表示\n",
    "representation_topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f35ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化主題分布：圓圈大小是主題的大小，圓圈的距離是主題之間的相似度\n",
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c52eb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "representation_topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de5782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始模型的主題\n",
    "visualize_fake_news_ratio_by_topic(topic_model, docs, data['label'], title=\"原始主題的假新聞比例\")\n",
    "\n",
    "# 使用 KeyBERT 表示詞的模型主題\n",
    "visualize_fake_news_ratio_by_topic(representation_topic_model, docs, data['label'], title=\"KeyBERT 主題的假新聞比例\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
