{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4da74db",
   "metadata": {},
   "source": [
    "## å¦‚æœéœ€è¦è£œå…¨ æœŸæœ«å°ˆæ¡ˆ å¯åƒè€ƒ ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11629ac2",
   "metadata": {
    "id": "FteSnH9FROVI"
   },
   "source": [
    "\n",
    "# ä¸»é¡Œï¼šæ–°èæ–‡æœ¬NLPè™•ç†èˆ‡çœŸå‡æ–°èåˆ†é¡\n",
    "#### ç¬¬åä¸‰çµ„ ç¬¬ä¸‰æ¬¡è®€æ›¸æœƒå°ˆæ¡ˆ\n",
    "çµ„å“¡ï¼š\n",
    "N124020001 æ—å¤éƒ\n",
    "N124020004 é™³å¿ æ³°\n",
    "N124020005 é™³è˜æƒ \n",
    "N124020006 å¼µç¥å€«\n",
    "N124020012 éƒ­å±•å·"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cb5555",
   "metadata": {},
   "source": [
    "<h2>ä¸€ã€è³‡æ–™ç°¡ä»‹èˆ‡ç›®æ¨™</h2>\n",
    "<h3>1.1è³‡æ–™ç°¡ä»‹</h3>\n",
    "è³‡æ–™ä¾†æºKaggle:https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset\n",
    "\n",
    "æœ¬å°ˆæ¡ˆä½¿ç”¨çš„è³‡æ–™é›†ç‚º Fake and Real News Datasetï¼Œç”± Kaggle æä¾›ï¼ŒåŸå§‹è³‡æ–™åŒ…å«å…©å€‹æª”æ¡ˆï¼š\n",
    "<ol>\n",
    "<li>Fake.csvï¼šåŒ…å«å‡æ–°èï¼ˆFake Newsï¼‰ä¹‹æ¨™é¡Œèˆ‡å…§æ–‡</li>\n",
    "<li>True.csvï¼šåŒ…å«çœŸæ–°èï¼ˆReal Newsï¼‰ä¹‹æ¨™é¡Œèˆ‡å…§æ–‡</li>\n",
    "</ol>\n",
    "æ¯ç­†è³‡æ–™å…·æœ‰ä»¥ä¸‹æ¬„ä½ï¼š\n",
    "<ol>\n",
    "<li>titleï¼šæ–°èæ¨™é¡Œ</li>\n",
    "<li>textï¼šæ–°èæ–‡å­—å…§å®¹</li>\n",
    "<li>subjectï¼šä¸»é¡Œé¡åˆ¥ï¼Œå¦‚æ”¿æ²»ã€ä¸–ç•Œæ–°èç­‰</li>\n",
    "<li>dateï¼šæ–°èç™¼å¸ƒæ™‚é–“</li>\n",
    "</ol>\n",
    "\n",
    "åœ¨è³‡æ–™å‰è™•ç†éšæ®µï¼Œæˆ‘å€‘å°‡å…©ä»½è³‡æ–™åŠ ä¸Šæ¨™ç±¤ï¼ˆlabelï¼‰ï¼Œå…¶ä¸­ï¼š\n",
    "å‡æ–°èï¼ˆFakeï¼‰æ¨™è¨˜ç‚º 0;çœŸæ–°èï¼ˆTrueï¼‰æ¨™è¨˜ç‚º 1\n",
    "ä¸¦åˆä½µæˆä¸€ä»½å¯ä¾›åˆ†é¡ä»»å‹™ä½¿ç”¨çš„æ•´åˆè³‡æ–™é›†ã€‚\n",
    "\n",
    "<h3>1.2ç›®æ¨™</h3>\n",
    "æœ¬æ¬¡å°ˆæ¡ˆä¸»è¦ä»»å‹™ç‚ºï¼š\n",
    "<ol>\n",
    "<li>è§€å¯Ÿè³‡æ–™ç‰¹åˆ¥èˆ‡åˆ†å¸ƒ</li>\n",
    "<li>åˆ©ç”¨æ–°èæ–‡æœ¬å…§å®¹ä¾†è¾¨è­˜æ–°èçœŸå½ï¼ˆå³çœŸå‡æ–°èåˆ†é¡ä»»å‹™ï¼‰\n",
    "é€™æ˜¯ä¸€å€‹å…¸å‹çš„äºŒå…ƒåˆ†é¡å•é¡Œï¼ˆBinary Classificationï¼‰ï¼Œæˆ‘å€‘å°‡å˜—è©¦é€éä¸åŒçš„æ–‡æœ¬è¡¨ç¤ºæ–¹å¼ï¼ˆText Embeddingï¼‰ï¼Œä¾†å»ºç«‹åˆ†é¡æ¨¡å‹ï¼Œåˆ¤æ–·ä¸€ç¯‡æ–°èæ˜¯ã€ŒçœŸã€é‚„æ˜¯ã€Œå‡ã€</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d5205d",
   "metadata": {
    "id": "fZy215-S1Kiv"
   },
   "source": [
    "<h3>äºŒã€å¼•ç”¨å¥—ä»¶èˆ‡è³‡æ–™è™•ç†</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6387ca5",
   "metadata": {},
   "source": [
    "<h4>2.1è¼‰å…¥å¥—ä»¶</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115d3253",
   "metadata": {
    "executionInfo": {
     "elapsed": 9707,
     "status": "ok",
     "timestamp": 1745841542183,
     "user": {
      "displayName": "å‘‚è‚²çœŸ",
      "userId": "02881314335061949853"
     },
     "user_tz": -480
    },
    "id": "e4zJh4IklUkI"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import multiprocessing\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, util, models\n",
    "\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "from matplotlib.font_manager import fontManager\n",
    "import plotly.express as px\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt','stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7b01c0",
   "metadata": {
    "id": "JyP_HVbPvxdL"
   },
   "source": [
    "<h4>2.2è³‡æ–™è™•ç†</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5cf3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#è®€æª”\n",
    "trueNews = pd.read_csv('./raw_data/true.csv')\n",
    "fakeNews = pd.read_csv('./raw_data/fake.csv')\n",
    "#æ–°å¢æ–°èé¡åˆ¥æ¬„ä½\n",
    "trueNews[\"label\"] = 1\n",
    "fakeNews[\"label\"] = 0\n",
    "#åˆä½µå…©ä»½æª”æ¡ˆ\n",
    "df = pd.concat([trueNews,fakeNews],axis = 0 ).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a453f7",
   "metadata": {},
   "source": [
    "æŠŠæ–‡ç« çš„title+textåˆä½µæˆç‚ºcontentæ¬„ä½ï¼Œåšå¾ŒçºŒæ¢ç´¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041c1c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"content\"] = df[\"title\"].astype(str) + \" : \" + df[\"text\"].astype(str)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d68397",
   "metadata": {},
   "source": [
    "ç§»é™¤ç§»é™¤æ¨™é»ç¬¦è™Ÿã€æ•¸å­—ã€å°å¯«åŒ–ç­‰åŸºæœ¬æ¸…ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895bc35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()                          # å°å¯«åŒ–\n",
    "    text = re.sub(r'\\d+', '', text)              # ç§»é™¤æ•¸å­—\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)          # ç§»é™¤æ¨™é»ç¬¦è™Ÿ\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()     # ç§»é™¤å¤šé¤˜ç©ºç™½\n",
    "    return text\n",
    "\n",
    "df[\"content_clean\"] = df[\"content\"].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1756c42",
   "metadata": {},
   "source": [
    "ä½¿ç”¨ NLTK é€²è¡Œè‹±æ–‡æ–·è©ï¼ˆtokenizationï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d0bd37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2916,
     "status": "ok",
     "timestamp": 1745841574140,
     "user": {
      "displayName": "å‘‚è‚²çœŸ",
      "userId": "02881314335061949853"
     },
     "user_tz": -480
    },
    "id": "0pYmTSvfmBZd",
    "outputId": "fd5a5e64-9ee7-45b4-e557-bd070c81f44f"
   },
   "outputs": [],
   "source": [
    "df[\"tokens\"] = df[\"content_clean\"].apply(word_tokenize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6950c574",
   "metadata": {},
   "source": [
    "å»é™¤åœç”¨è©ï¼ˆstopwordsï¼‰èˆ‡è©å¹¹é‚„åŸï¼ˆstemmingï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad67b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_tokens(tokens):\n",
    "    return [\n",
    "        stemmer.stem(word) \n",
    "        for word in tokens \n",
    "        if word not in stop_words and len(word) > 1\n",
    "    ]\n",
    "\n",
    "df[\"tokens_clean\"] = df[\"tokens\"].apply(preprocess_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c4e649",
   "metadata": {
    "id": "N_klM4cOcI3K"
   },
   "source": [
    "<h2>ä¸‰ã€word2vec</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6317de",
   "metadata": {},
   "source": [
    "<h3>3.1ä¾ç…§è³‡æ–™è¨“ç·´è‡ªå·±çš„æ¨¡å‹</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50f01a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”¨tokens_clean ç•¶input\n",
    "sentences = df[\"tokens_clean\"].tolist()\n",
    "\n",
    "# å»ºbigramæ¨¡å‹\n",
    "bigram = Phrases(sentences, min_count=1, threshold=2)\n",
    "bigram_phraser = Phraser(bigram)\n",
    "\n",
    "# ç”¨bigram æ¨¡å‹ï¼ŒæŠŠthresholdä»¥ä¸Šçš„è©åˆä½µ\n",
    "df[\"tokens_bigram\"] = df[\"tokens_clean\"].apply(lambda x: bigram_phraser[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6140ce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹æ©Ÿå™¨çš„core\n",
    "cores = multiprocessing.cpu_count()\n",
    "print(f\"number of cores: {cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45bf11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åªä¿tokens_bigramæ¬„ä½çš„è³‡æ–™\n",
    "sentences = df[\"tokens_bigram\"].tolist()\n",
    "\n",
    "# 8:2åˆ‡è³‡æ–™\n",
    "sentences_train, sentences_test = train_test_split(sentences, test_size=0.2, random_state=42)\n",
    "\n",
    "# è¨“ç·´ Word2Vec æ¨¡å‹\n",
    "w2v_model = Word2Vec(\n",
    "    sentences = sentences_train,    # ä½¿ç”¨è¨“ç·´èªæ–™\n",
    "    vector_size = 128,              # vector çš„ç¶­åº¦\n",
    "    alpha = 0.0005, #learning rate\n",
    "    window = 2,                    # ä¸Šä¸‹æ–‡è¦–çª—å¤§å°\n",
    "    min_count = 30,                  # å¿½ç•¥å‡ºç¾æ¬¡æ•¸å°‘æ–¼30æ¬¡çš„è©\n",
    "    workers= cores-2,                  # CPU core\n",
    "    sg = 1,                         # 1 = skip-gram, 0 = CBOW\n",
    "    epochs = 30,\n",
    "    hs = 1,\n",
    "    seed = 8888\n",
    ")\n",
    "\n",
    "# è¨“ç·´æ¨¡å‹\n",
    "w2v_model.train(sentences_train, total_examples=len(sentences_train), epochs=10)\n",
    "\n",
    "# å„²å­˜æ¨¡å‹\n",
    "w2v_model.save(\"w2v_fake_news.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb402da",
   "metadata": {},
   "source": [
    "æŸ¥çœ‹çµæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f91bfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æª¢æŸ¥æœ€ç›¸é—œçš„å­— (cosine similarity)\n",
    "w2v_model.wv.most_similar('trump',topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3824cb",
   "metadata": {},
   "source": [
    "Trump çš„ç›¸é—œè©åŒ…å« presid_trumpã€trump_campaignã€trump_administrã€mr_trumpã€‚\n",
    "\n",
    "èªæ„æ¨¡å‹èƒ½æˆåŠŸæ•æ‰èªæ–™ä¸­çš„æ”¿æ²»èªæ„è„ˆçµ¡ã€‚\n",
    "å¯ä»¥èªªæ˜ï¼šé€™äº›é—œè¯è©èˆ‡ Trump çš„æ”¿æ²»èƒŒæ™¯ã€é«˜é »è­°é¡Œé«˜åº¦é—œè¯ï¼Œæ¨¡å‹èªæ„å­¸ç¿’æ˜¯ç¬¦åˆé æœŸä¸”æœ‰æ•ˆçš„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31827d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar('trump','biden',topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e97cf0",
   "metadata": {},
   "source": [
    "è·Ÿå…©å€‹å­—æœ€ä¸ç›¸é—œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495f8797",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(negative=['trump','biden'],topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a801e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨ˆç®—å…©å€‹å­—ä¹‹é–“çš„é—œä¿‚\n",
    "w2v_model.wv.similarity(\"china\",\"america\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc052468",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.similarity(\"china\",\"taiwan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228d3214",
   "metadata": {},
   "source": [
    "china èˆ‡ america çš„ç›¸ä¼¼åº¦ç´„ 0.34ï¼Œä»£è¡¨å…©è€…èªæ„åœ¨èªæ–™ä¸­æ˜¯ç›¸å°è¼ƒé ã€‚\n",
    "china èˆ‡ taiwan çš„ç›¸ä¼¼åº¦é«˜é” 0.81ï¼Œé¡¯ç¤ºèªæ–™ä¸­çš„é€™å…©å€‹è©å‡ºç¾è„ˆçµ¡æ¥µç‚ºæ¥è¿‘ã€‚\n",
    "\n",
    "é›–ç„¶é€™ä¸‰å€‹è©éƒ½æ˜¯åœ°å ä½†åœ¨ Word2Vec çœ‹èªæ„è·é›¢æœ‰æ‰€å·®è·ï¼Œèªªæ˜äº†èªæ„ç›¸ä¼¼ä¸ç­‰æ–¼å¯¦é«”åˆ†é¡ç›¸åŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a685a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å–å¾—æ‰€æœ‰çš„å­—\n",
    "words = w2v_model.wv.key_to_index.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1bb438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é™ç¶­ï¼šåˆ©ç”¨PCA tSNE\n",
    "\n",
    "def reduceDim(mat,method:str='PCA',dim:str=2,perplexity = 25,learning_rate = 400):\n",
    "\n",
    "    method_dict = {\n",
    "        \"PCA\":PCA(n_components=dim,iterated_power = 1000,random_state=0),\n",
    "        \"TSNE\":TSNE(n_components=dim,random_state=0,perplexity=perplexity,learning_rate=learning_rate),\n",
    "    }\n",
    "    new_feat = method_dict[method].fit_transform(mat)\n",
    "\n",
    "    return new_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6db07f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‹¿åˆ°list of words çš„vector\n",
    "def getVecs(model,words:list):\n",
    "    vecs = []\n",
    "    for i in words:\n",
    "        vecs.append(model.wv[i])\n",
    "    return np.vstack(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d01de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "getVecs(w2v_model,['taiwan','china'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c18c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ“´å±•ç›¸ä¼¼çš„å­—è©\n",
    "def expandPosWord(model, words:list, top_n:int, split = True):\n",
    "\n",
    "    if split == False:\n",
    "        wp = model.wv.most_similar(words,topn = top_n)\n",
    "        return wp\n",
    "    expand = []\n",
    "\n",
    "    for w in words:\n",
    "        wp = model.wv.most_similar(w,topn = top_n)\n",
    "        for i in wp:\n",
    "            expand.append(i[0])\n",
    "\n",
    "    return list(set(expand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7cc75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "expandPosWord(w2v_model,['china','taiwan'],top_n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7c683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç•«å‡ºå…©ç¶­çš„æ•£ä½ˆåœ–\n",
    "def plotScatter(vec_df):\n",
    "    \"\"\"\n",
    "    vec_df: å­—è©åŠå…¶å…©å€‹ç¶­åº¦çš„å€¼\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15,15))\n",
    "    fontManager.addfont('./TaipeiSansTCBeta-Regular.ttf')\n",
    "    plt.rcParams['font.sans-serif'] = ['Taipei Sans TC Beta']\n",
    "    plt.rcParams['font.size'] = '16'\n",
    "\n",
    "    p = sns.scatterplot(x=\"dim1\", y=\"dim2\",\n",
    "                  data=vec_df)\n",
    "    for line in range(0, vec_df.shape[0]):\n",
    "         p.text(vec_df[\"dim1\"][line],\n",
    "                 vec_df['dim2'][line],\n",
    "                 '  ' + vec_df[\"word\"][line].title(),\n",
    "                 horizontalalignment='left',\n",
    "                 verticalalignment='bottom', size='medium',\n",
    "                 weight='normal'\n",
    "                ).set_size(15)\n",
    "    plt.show()\n",
    "\n",
    "# ç•«å‡ºä¸‰ç¶­çš„æ•£ä½ˆåœ–\n",
    "def plotScatter3D(vec_df):\n",
    "    vec_df['size'] = .5\n",
    "    if 'color' not in vec_df.columns:\n",
    "        vec_df['color'] = 'blue'\n",
    "    fig = px.scatter_3d(\n",
    "        vec_df,'dim1','dim2','dim3',text = 'word',width=800, height=800,color = 'color',size = 'size'\n",
    "\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da40fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_words = np.random.choice(list(words),150,replace=False).tolist()\n",
    "\n",
    "feat = getVecs(model=w2v_model,words=sample_words)\n",
    "print(feat.shape)\n",
    "new_feat = reduceDim(feat,method='TSNE',perplexity=20,learning_rate = 800)\n",
    "print(new_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88b7ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = pd.DataFrame({\n",
    "    \"word\":sample_words,\n",
    "    \"dim1\":new_feat[:,0],\n",
    "    \"dim2\":new_feat[:,1],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822b8909",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotScatter(word_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42c6ce3",
   "metadata": {},
   "source": [
    "3D æ•£ç‹€åœ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5070ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feat = reduceDim(feat,dim = 3,method = 'PCA' )\n",
    "print(new_feat.shape)\n",
    "word_df = pd.DataFrame({\n",
    "    \"word\":sample_words,\n",
    "    \"dim1\":new_feat[:,0],\n",
    "    \"dim2\":new_feat[:,1],\n",
    "    \"dim3\":new_feat[:,2],\n",
    "})\n",
    "plotScatter3D(word_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38587acb",
   "metadata": {},
   "source": [
    "å°‡å­—åˆ†ç¾¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616e1f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn-extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ffe137",
   "metadata": {},
   "outputs": [],
   "source": [
    "#åªä½¿ç”¨word vector å»åˆ†ç¾¤\n",
    "def cluster(X,method = 'kmeans',n = 2):\n",
    "\n",
    "    method_dict = {\n",
    "        'kmeans':KMeans(n_clusters=n, random_state=0),\n",
    "        'kmedos':KMedoids(n_clusters=n, random_state=0)\n",
    "    }\n",
    "    method_dict[method].fit(X)\n",
    "    result = method_dict[method].predict(X)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f36a2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feat = reduceDim(feat,method='PCA',dim = 20)\n",
    "d3_feat = reduceDim(feat,method='PCA',dim = 3)\n",
    "word_df = pd.DataFrame({\n",
    "    \"word\":sample_words,\n",
    "    \"color\":cluster(new_feat,n=4),\n",
    "    \"dim1\":d3_feat[:,0],\n",
    "    \"dim2\":d3_feat[:,1],\n",
    "    \"dim3\":d3_feat[:,2],\n",
    "\n",
    "})\n",
    "plotScatter3D(word_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acfaeee",
   "metadata": {},
   "source": [
    "<h3>3.2 Transformers Embeddings</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1db24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d08944",
   "metadata": {},
   "source": [
    "<h4>å°æ¨¡å‹ï¼Œä»¥BERTç‚ºç¯„ä¾‹</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084447f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è‹±æ–‡ bert-base-uncased\n",
    "bert_en = SentenceTransformer('google-bert/bert-base-uncased')\n",
    "\n",
    "bert_en.tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d1ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_en.get_max_seq_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb3994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å›å‚³cosine similarityåˆ†æ•¸\n",
    "def get_result_df(sentences, cosine_scores):\n",
    "\n",
    "  result = []\n",
    "  for i in range(len(sentences)):\n",
    "      for j in range(i+1, len(sentences)):\n",
    "          result.append([sentences[i], sentences[j], cosine_scores[i][j].item()])\n",
    "\n",
    "  result_df = pd.DataFrame(result, columns=[\"sentence1\", \"sentence2\", \"score\"])\n",
    "  result_df = result_df.sort_values(\"score\", ascending = False)\n",
    "\n",
    "  return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69318d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤šèªè¨€ bert-base-multilingual-cased\n",
    "bert_multi = SentenceTransformer('google-bert/bert-base-multilingual-cased')\n",
    "\n",
    "bert_multi.tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f761c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¤ºç¯„å¥å­\n",
    "sentences = [\n",
    "    \"Trump's treatening the world under the tariff policy.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"å·æ™®æ­£åœ¨è—‰ç”±é—œç¨…æ”¿ç­–å¨è„…ä¸–ç•Œã€‚\"\n",
    "]\n",
    "\n",
    "# ä½¿ç”¨ encode() å°è³‡æ–™åšembedding\n",
    "embeddings_multi = bert_multi.encode(sentences)\n",
    "\n",
    "# Compute cosine-similarities\n",
    "cosine_scores = util.cos_sim(embeddings_multi, embeddings_multi)\n",
    "\n",
    "# å°å‡ºå¥å­é–“çš„cosine similarityåˆ†æ•¸\n",
    "result_df = get_result_df(sentences, cosine_scores)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccc2617",
   "metadata": {},
   "source": [
    "<h3>å››ã€ä½¿ç”¨embeddingåšNLPä»»å‹™</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b323c8cc",
   "metadata": {},
   "source": [
    "<h4>4.1ç›¸ä¼¼æ–‡ä»¶</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ebbda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨è‹±æ–‡é è¨“ç·´ BERT æ¨¡å‹ï¼ˆé©åˆè‹±æ–‡å¥å­ï¼‰\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# å°‡æ‰€æœ‰æ–°èå…§å®¹è½‰æ›ç‚ºå‘é‡\n",
    "corpus_embeddings = model.encode(df['content'].tolist(), convert_to_tensor=True, batch_size=32)\n",
    "\n",
    "# ä»»é¸ä¸€ç¯‡æ–°èä½œç‚ºæŸ¥è©¢å°è±¡ï¼ˆä½ å¯è‡ªè¡Œæ›´æ› query_num æ•¸å€¼ï¼‰\n",
    "query_num = 10\n",
    "query_embedding = model.encode(df['content'][query_num], convert_to_tensor=True)\n",
    "\n",
    "# è¨ˆç®—èˆ‡å…¶ä»–æ–°èçš„ cosine similarity\n",
    "cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "\n",
    "# å–ç›¸ä¼¼åº¦å‰äº”é«˜çš„æ–‡ç« \n",
    "top_k = 5\n",
    "top_results = torch.topk(cos_scores, k=top_k)\n",
    "\n",
    "# é¡¯ç¤ºçµæœ\n",
    "print(\"\\n\\n======================\")\n",
    "print(\"æŸ¥è©¢æ–°èæ¨™é¡Œ:\\n\", df['title'][query_num])\n",
    "print(\"\\næœ€ç›¸ä¼¼çš„æ–°è:\")\n",
    "\n",
    "for score, idx in zip(top_results[0], top_results[1]):\n",
    "    print(f\"{df['title'][idx.item()]} (Score: {score:.4f})\")\n",
    "\n",
    "print(\"======================\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50002bb",
   "metadata": {},
   "source": [
    "<h4>4.2åˆ†é¡ä»»å‹™</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5fb0f8",
   "metadata": {},
   "source": [
    "è½‰æˆå‘é‡ï¼ˆç”¨ title+text åˆä½µçš„ content æ¬„ä½ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c5ca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df[\"content\"].tolist()\n",
    "embeddings = model.encode(corpus, batch_size=32, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64197e36",
   "metadata": {},
   "source": [
    "å»ºç«‹åˆ†é¡å™¨ï¼ˆä»¥ sklearnçš„é‚è¼¯å›æ­¸ ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652baf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, df[\"label\"], test_size=0.2, random_state=42)\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a985928",
   "metadata": {},
   "source": [
    "åŒ…æˆfunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005dbee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_news_authenticity(title: str, content: str, model_embed, clf):\n",
    "    \"\"\"\n",
    "    è¼¸å…¥æ–°èæ¨™é¡Œèˆ‡å…§æ–‡ï¼Œä½¿ç”¨å·²è¨“ç·´çš„åµŒå…¥æ¨¡å‹èˆ‡åˆ†é¡å™¨é æ¸¬çœŸå‡ã€‚\n",
    "    \n",
    "    åƒæ•¸ï¼š\n",
    "    - title: æ–°èæ¨™é¡Œï¼ˆå­—ä¸²ï¼‰\n",
    "    - content: æ–°èå…§æ–‡ï¼ˆå­—ä¸²ï¼‰\n",
    "    - model_embed: SentenceTransformer æ¨¡å‹\n",
    "    - clf: è¨“ç·´å¥½çš„åˆ†é¡æ¨¡å‹ï¼ˆå¦‚ LogisticRegressionï¼‰\n",
    "\n",
    "    è¼¸å‡ºï¼š\n",
    "    - é æ¸¬çµæœï¼ˆTrue/Fakeï¼‰\n",
    "    - True æ©Ÿç‡\n",
    "    - Fake æ©Ÿç‡\n",
    "    \"\"\"\n",
    "    full_text = f\"{title.strip()} : {content.strip()}\"\n",
    "    embedding = model_embed.encode(full_text).reshape(1, -1)\n",
    "    pred_label = clf.predict(embedding)[0]\n",
    "    pred_prob = clf.predict_proba(embedding)[0]\n",
    "\n",
    "    result = \"True\" if pred_label == 1 else \"Fake\"\n",
    "    true_prob = round(pred_prob[1], 4)\n",
    "    fake_prob = round(pred_prob[0], 4)\n",
    "\n",
    "    print(f\"é æ¸¬çµæœï¼š{result}\")\n",
    "    print(f\"True æ©Ÿç‡ï¼š{true_prob}\")\n",
    "    print(f\"Fake æ©Ÿç‡ï¼š{fake_prob}\")\n",
    "    \n",
    "    return result, true_prob, fake_prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2077866c",
   "metadata": {},
   "source": [
    "# 5.BERT (Encoder-only-model)\n",
    "\n",
    "- Token classification \n",
    "    NER\n",
    "- Sequence classification\n",
    "    Sentiment Classification\n",
    "    Relation Extraction (RE)\n",
    "- Text Clustering (BERTopic)\n",
    "    Embedding model\n",
    "    Clustering model\n",
    "    ä½¿ç”¨Representationæ–¹æ³•å»å¾®èª¿ä¸»é¡Œè¡¨ç¤º\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b54574",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# è¼‰å…¥è³‡æ–™é›†\n",
    "fake_df = pd.read_csv('./raw_data/fake.csv')\n",
    "true_df = pd.read_csv('./raw_data/true.csv')\n",
    "\n",
    "# åŠ ä¸Š label æ¬„ä½\n",
    "fake_df['label'] = 1\n",
    "true_df['label'] = 0\n",
    "\n",
    "# å–å‰ 10000 ç­†\n",
    "data = pd.concat([fake_df.iloc[:10000], true_df.iloc[:10000]], ignore_index=True)\n",
    "data = data[data['text'].notna()].reset_index(drop=True)\n",
    "\n",
    "# æª¢æŸ¥å„é¡åˆ¥æ•¸é‡\n",
    "print(data['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7b5ac9",
   "metadata": {},
   "source": [
    "## éœ€è¦åšæ–‡æœ¬é è™•ç†å—?\n",
    "\n",
    "ç›®çš„:\n",
    "- å»ºç«‹åˆ†é¡å™¨ä¾†é æ¸¬çœŸå‡æ–°è -> (TF-IDF + åˆ†é¡æ¨¡å‹éœ€è¦ä¹¾æ·¨çš„è³‡æ–™ï¼Œæœ‰å¹«åŠ©)\n",
    "- åˆ†æNER çµæœèˆ‡èªæ„åˆ†ä½ˆ -> (æœƒç ´å£èªæ„)\n",
    "- å»ºç«‹ä¸»é¡Œæ¨¡å‹ä¾†æ¢ç´¢èªæ„ä¸»é¡Œï¼ˆBERTopic -> (æœƒç ´å£èªæ„)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e324aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z ]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "data['tokens'] = data['text'].astype(str).apply(preprocess)\n",
    "data['clean_text'] = data['tokens'].apply(lambda x: ' '.join(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68ddd12",
   "metadata": {},
   "source": [
    "## NER é æ¸¬æ–°èçœŸå‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81df51b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.font_manager import fontManager\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fontManager.addfont('./TaipeiSansTCBeta-Regular.ttf')\n",
    "plt.rcParams['font.sans-serif'] = ['Taipei Sans TC Beta']\n",
    "plt.rcParams['font.size'] = '16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29def4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from hdbscan import HDBSCAN\n",
    "from transformers import BertTokenizerFast, AutoTokenizer, AutoModelForTokenClassification, AutoModelForSequenceClassification, pipeline\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# è¨­å®šè£ç½®ï¼šå„ªå…ˆä½¿ç”¨ Apple GPU (MPS)ï¼Œå¦å‰‡ç”¨ CPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\" ä½¿ç”¨è£ç½®ï¼š{device}\")\n",
    "\n",
    "# è¼‰å…¥ BERT æ¨¡å‹èˆ‡ tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\").to(device)\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\",\n",
    "                        device=0 if device.type != \"cpu\" else -1)\n",
    "\n",
    "# åˆ†æ®µå‡½æ•¸ï¼ˆç¢ºä¿ä¸è¶…é 512 tokenï¼‰\n",
    "def split_text(text, chunk_size=512):\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# åˆå§‹åŒ–çµæœåˆ—è¡¨\n",
    "ner_rows = []\n",
    "\n",
    "# é€ç¯‡æ–‡ç« è™•ç†ï¼ŒåŠ é›™å±¤é€²åº¦æ¢\n",
    "for idx, text in tqdm(data['text'].astype(str).items(), desc=\"ğŸ” è™•ç†æ–‡ç« é€²åº¦\"):\n",
    "    try:\n",
    "        chunks = split_text(text)\n",
    "        all_ents = []\n",
    "        with torch.no_grad():  # åŠ é€Ÿæ¨è«–ç”¨\n",
    "            for chunk in tqdm(chunks, leave=False, desc=f\"æ–‡ç«  {idx} åˆ†æ®µä¸­\"):\n",
    "                all_ents.extend(ner_pipeline(chunk))\n",
    "        for ent in all_ents:\n",
    "            ner_rows.append({\n",
    "                \"index\": idx,\n",
    "                \"entity\": ent['entity_group'],\n",
    "                \"word\": ent['word'],\n",
    "                \"score\": ent['score']\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\" ç¬¬ {idx} ç­†è³‡æ–™éŒ¯èª¤: {e}\")\n",
    "\n",
    "# è½‰ç‚º DataFrame\n",
    "ner_df = pd.DataFrame(ner_rows)\n",
    "ner_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875c0283",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbf7dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•´åˆ label\n",
    "merged_df = ner_df.merge(data[['label']], left_on='index', right_index=True)\n",
    "\n",
    "# èšåˆæ‰€æœ‰ entity é¡å‹çš„å‡ºç¾æ¬¡æ•¸\n",
    "entity_counts_all = (\n",
    "    merged_df.groupby(['index', 'entity'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)  # å¾—åˆ°æ¯ç¯‡æ–‡ç« å„é¡å¯¦é«”æ•¸\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# åˆä½µ label\n",
    "entity_counts_all = entity_counts_all.merge(data[['label']], left_on='index', right_index=True)\n",
    "\n",
    "# å»ºæ¨¡æ¬„ä½é¸æ“‡ï¼šæ‰€æœ‰å¯¦é«”é¡åˆ¥æ¬„ä½ï¼ˆæ’é™¤ index, labelï¼‰\n",
    "feature_cols = [col for col in entity_counts_all.columns if col not in ['index', 'label']]\n",
    "kmeans_fit_pred_data = entity_counts_all[feature_cols]\n",
    "\n",
    "# åš KMeans èšé¡\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "entity_counts_all['cluster'] = kmeans.fit_predict(kmeans_fit_pred_data)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(kmeans_fit_pred_data)\n",
    "entity_counts_all['PC1'] = X_pca[:, 0]\n",
    "entity_counts_all['PC2'] = X_pca[:, 1]\n",
    "# è¦–è¦ºåŒ–èšé¡çµæœ\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=entity_counts_all,\n",
    "    x='PC1', y='PC2', hue='cluster', style='label',\n",
    "    palette='Set2', s=100\n",
    ")\n",
    "\n",
    "plt.title('NER ç‰¹å¾µçš„ä¸»æˆåˆ†åˆ†æ + KMeans èšé¡')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e6e5bf",
   "metadata": {},
   "source": [
    "#### å˜—è©¦ç”¨ NER æå–å‡ºçš„'äººå'ã€'çµ„ç¹”'ã€'åœ°åæ•¸é‡'ä½œç‚ºè©å½™ç‰¹å¾µï¼Œå†é¤µçµ¦ TF-IDF + æ¨¡å‹ä¾†é æ¸¬é€™ç¯‡æ–°èæ˜¯çœŸ/å‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d807d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# èšåˆ ner_df çµæœç‚ºç‰¹å¾µè¡¨ï¼ˆä»¥ index = æ–‡ç« ç·¨è™Ÿç‚º keyï¼‰\n",
    "entity_counts = ner_df.groupby(['index', 'entity']).size().unstack(fill_value=0)\n",
    "\n",
    "# åˆä½µå›åŸè³‡æ–™é›†\n",
    "data_with_ner = data.copy()\n",
    "data_with_ner = data_with_ner.join(entity_counts, how='left').fillna(0)\n",
    "\n",
    "# å»ºç«‹ç‰¹å¾µï¼šäººåã€çµ„ç¹”ã€åœ°åæ•¸é‡\n",
    "ner_pred_X = data_with_ner[['PER', 'ORG', 'LOC']]\n",
    "ner_pred_y = data_with_ner['label']\n",
    "\n",
    "# å»ºæ¨¡\n",
    "# Logistic Regression\n",
    "X_train, X_test, y_train, y_test = train_test_split(ner_pred_X, ner_pred_y, test_size=0.2, random_state=42)\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train, y_train)\n",
    "lr_pred = lr_clf.predict(X_test)\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "rf_y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "# Logistic Regression çµæœ\n",
    "print(\"=== LogisticRegression åˆ†é¡çµæœ ===\")\n",
    "print(classification_report(y_test, lr_pred))\n",
    "\n",
    "# Random Forest çµæœ\n",
    "print(\"=== RandomForestClassifier åˆ†é¡çµæœ ===\")\n",
    "print(classification_report(y_test, rf_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1e1c16",
   "metadata": {},
   "source": [
    "### NERæå–ç‰¹å¾µé æ¸¬çµæœå°šå¯\n",
    "å°çµ:\n",
    "é æ¸¬çœŸæ–°è:  LR      RF\n",
    "precision   0.71    0.75\n",
    "recall      0.62    0.74\n",
    "f1          0.66    0.74\n",
    "\n",
    "é æ¸¬å‡æ–°è:\n",
    "precision   0.66    0.74\n",
    "recall      0.74    0.75\n",
    "f1          0.70    0.74\n",
    "\n",
    "NER ç‰¹å¾µå°çœŸå‡æ–°èè¾¨è­˜æœ‰ä¸€å®šç¨‹åº¦ä½œç”¨ï¼Œä¸”ç”¨RandomForestçš„çµæœè¼ƒå„ª"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400ff9f4",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨æƒ…ç·’åˆ†æè¾¨è­˜çœŸå‡æ–°è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d64c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥æƒ…ç·’åˆ†ææ¨¡å‹\n",
    "model = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "# å› ç‚ºé€™å€‹èªè¨€ä¹Ÿæ˜¯BERT = æ•ˆæœä»°è³´'è‡ªç„¶èªè¨€èªåºèˆ‡ä¸Šä¸‹æ–‡' = ä½¿ç”¨data['text']å³å¯\n",
    "\n",
    "# åˆ‡å‰²æ–‡å­— æ¯æ®µä¸è¶…é 512 å­—\n",
    "def split_text(text, chunk_size=512):\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# æ•´åˆæ®µè½çš„æƒ…ç·’åˆ†æ•¸\n",
    "def analyze_long_text(text):\n",
    "    try:\n",
    "        chunks = split_text(text)\n",
    "        results = model(chunks)\n",
    "\n",
    "        # çµ±è¨ˆæƒ…ç·’\n",
    "        pos_scores = [r['score'] for r in results if r['label'] == 'POSITIVE']\n",
    "        neg_scores = [r['score'] for r in results if r['label'] == 'NEGATIVE']\n",
    "\n",
    "        # å¹³å‡åˆ†æ•¸\n",
    "        avg_pos = sum(pos_scores) / len(pos_scores) if pos_scores else 0\n",
    "        avg_neg = sum(neg_scores) / len(neg_scores) if neg_scores else 0\n",
    "\n",
    "        # æ±ºå®šç¸½é«”æƒ…çºŒ\n",
    "        if avg_pos > avg_neg:\n",
    "            return pd.Series(['POSITIVE', avg_pos])\n",
    "        elif avg_neg > avg_pos:\n",
    "            return pd.Series(['NEGATIVE', avg_neg])\n",
    "        else:\n",
    "            return pd.Series(['NEUTRAL', 0.5])\n",
    "    except Exception:\n",
    "        return pd.Series(['ERROR', 0.0])\n",
    "\n",
    "# åŸ·è¡Œåˆ†æ\n",
    "tqdm.pandas()\n",
    "data[['sentiment_label', 'sentiment_score']] = data['text'].progress_apply(analyze_long_text)\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349aa9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pred_X = data[['sentiment_score']]\n",
    "sentiment_pred_y = data['label']\n",
    "\n",
    "# åˆ†å‰²è¨“ç·´èˆ‡æ¸¬è©¦é›†\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentiment_pred_X, sentiment_pred_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# å»ºç«‹æ¨¡å‹ä¸¦è¨“ç·´\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train, y_train)\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# é æ¸¬èˆ‡è©•ä¼°\n",
    "y_pred = lr_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# é æ¸¬èˆ‡è©•ä¼°\n",
    "rf_y_pred = rf_clf.predict(X_test)\n",
    "print(classification_report(y_test, rf_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcccc87d",
   "metadata": {},
   "source": [
    "### å°çµ: æƒ…ç·’é æ¸¬çœŸå‡æ–°èè¡¨ç¾ä¸å¥½\n",
    "å°çµ:\n",
    "é æ¸¬çœŸæ–°è:  LR      RF\n",
    "precision   0.60    0.52\n",
    "recall      0.37    0.50\n",
    "f1          0.46    0.51\n",
    "\n",
    "é æ¸¬å‡æ–°è:\n",
    "precision   0.54    0.52\n",
    "recall      0.75    0.54\n",
    "f1          0.63    0.53\n",
    "\n",
    "æ•´é«”åˆ†é¡æ•ˆæœåå¼±ï¼Œè·Ÿä¸ŸéŠ…æ¿å·®ä¸å¤š\n",
    "æ¨¡å‹åå¥½é æ¸¬ç‚ºå‡æ–°èï¼ˆrecall é«˜ï¼‰ï¼Œä½†ä¹Ÿå¤šèª¤åˆ¤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e0b838",
   "metadata": {},
   "source": [
    "### å˜—è©¦æ•´åˆå…©è€…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec497be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "combined_X = pd.concat([data_with_ner[['PER', 'ORG', 'LOC']], sentiment_pred_X], axis=1)\n",
    "combined_y = data['label']\n",
    "# åˆ†å‰²è¨“ç·´èˆ‡æ¸¬è©¦é›†\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_X, combined_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression\n",
    "clf_lr = LogisticRegression()\n",
    "clf_lr.fit(X_train, y_train)\n",
    "lr_preds = clf_lr.predict(X_test)\n",
    "\n",
    "# Random Forest\n",
    "clf_rf = RandomForestClassifier(random_state=42)\n",
    "clf_rf.fit(X_train, y_train)\n",
    "rf_preds = clf_rf.predict(X_test)\n",
    "\n",
    "# è©•ä¼°çµæœ\n",
    "print(\"=== Logistic Regression åˆ†é¡çµæœ ===\")\n",
    "print(classification_report(y_test, lr_preds))\n",
    "\n",
    "print(\"=== Random Forest åˆ†é¡çµæœ ===\")\n",
    "print(classification_report(y_test, rf_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f3f453",
   "metadata": {},
   "source": [
    "### æ•´åˆç‰¹å¾µå¾Œæ•´é«”é æ¸¬æº–ç¢ºåº¦ä¸Šå‡\n",
    "\n",
    "NER:\n",
    "=== LogisticRegression åˆ†é¡çµæœ ===\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.71      0.62      0.66       201\n",
    "           1       0.66      0.74      0.70       199\n",
    "\n",
    "    accuracy                           0.68       400\n",
    "   macro avg       0.68      0.68      0.68       400\n",
    "weighted avg       0.68      0.68      0.68       400\n",
    "\n",
    "=== RandomForestClassifier åˆ†é¡çµæœ ===\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.75      0.74      0.74       201\n",
    "           1       0.74      0.75      0.75       199\n",
    "\n",
    "    accuracy                           0.74       400\n",
    "   macro avg       0.75      0.75      0.74       400\n",
    "weighted avg       0.75      0.74      0.74       400\n",
    "\n",
    "\n",
    "\n",
    "æƒ…ç·’åˆ†æ:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.60      0.37      0.46       201\n",
    "           1       0.54      0.75      0.63       199\n",
    "\n",
    "    accuracy                           0.56       400\n",
    "   macro avg       0.57      0.56      0.55       400\n",
    "weighted avg       0.57      0.56      0.55       400\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.52      0.50      0.51       201\n",
    "           1       0.52      0.54      0.53       199\n",
    "\n",
    "    accuracy                           0.52       400\n",
    "   macro avg       0.52      0.52      0.52       400\n",
    "weighted avg       0.52      0.52      0.52       400\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "æ•´åˆå¾Œ:\n",
    "=== Logistic Regression åˆ†é¡çµæœ ===\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.73      0.67      0.70       201\n",
    "           1       0.69      0.75      0.72       199\n",
    "\n",
    "    accuracy                           0.71       400\n",
    "   macro avg       0.71      0.71      0.71       400\n",
    "weighted avg       0.71      0.71      0.71       400\n",
    "\n",
    "=== Random Forest åˆ†é¡çµæœ ===\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.80      0.77      0.79       201\n",
    "           1       0.78      0.81      0.79       199\n",
    "\n",
    "    accuracy                           0.79       400\n",
    "   macro avg       0.79      0.79      0.79       400\n",
    "weighted avg       0.79      0.79      0.79       400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6497f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å˜—è©¦å¢åŠ TF-IDFæ¬„ä½(clean_text)\n",
    "\n",
    "# å»ºç«‹ TF-IDF å‘é‡å™¨ï¼ˆå¯è‡ªè¨‚ ngram ç¯„åœèˆ‡ç¶­åº¦é™åˆ¶ï¼‰\n",
    "tfidf = TfidfVectorizer(max_features=200, ngram_range=(1, 2))\n",
    "tfidf_matrix = tfidf.fit_transform(data['clean_text'].fillna(''))\n",
    "\n",
    "# è½‰ç‚º DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out(), index=data.index)\n",
    "\n",
    "# æ–°å¢tf-idfæ¬„ä½\n",
    "ner_sentiment_df = pd.concat([ner_pred_X, sentiment_pred_X], axis=1)\n",
    "combined_X_full = pd.concat([ner_sentiment_df, tfidf_df], axis=1)\n",
    "\n",
    "# åˆ†å‰²è³‡æ–™\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_X_full, combined_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic\n",
    "clf_lr = LogisticRegression()\n",
    "clf_lr.fit(X_train, y_train)\n",
    "lr_preds = clf_lr.predict(X_test)\n",
    "\n",
    "# Random Forest\n",
    "clf_rf = RandomForestClassifier(random_state=42)\n",
    "clf_rf.fit(X_train, y_train)\n",
    "rf_preds = clf_rf.predict(X_test)\n",
    "\n",
    "# è©•ä¼°\n",
    "print(\"=== Logistic Regression(NER + Sentiment + TF-IDF) ===\")\n",
    "print(classification_report(y_test, lr_preds))\n",
    "\n",
    "print(\"=== Random Forest(NER + Sentiment + TF-IDF) ===\")\n",
    "print(classification_report(y_test, rf_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28223dfe",
   "metadata": {},
   "source": [
    "### éæ“¬åˆ?\n",
    "\n",
    "=== Logistic Regression(NER + Sentiment + TF-IDF) ===\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.97      1.00      0.98       201\n",
    "           1       0.99      0.96      0.98       199\n",
    "\n",
    "    accuracy                           0.98       400\n",
    "   macro avg       0.98      0.98      0.98       400\n",
    "weighted avg       0.98      0.98      0.98       400\n",
    "\n",
    "=== Random Forest(NER + Sentiment + TF-IDF) ===\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00       201\n",
    "           1       0.99      1.00      1.00       199\n",
    "\n",
    "    accuracy                           1.00       400\n",
    "   macro avg       1.00      1.00      1.00       400\n",
    "weighted avg       1.00      1.00      1.00       400\n",
    "\n",
    "- ä¿®æ­£: TfidfVectorizer max_features=1000 -> 200\n",
    "- ä¿®æ­£: è³‡æ–™å„å–1000ç­† -> 10000ç­†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242e57f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(clf_rf, combined_X_full, combined_y, cv=5, scoring='f1')\n",
    "print(f\"5-fold F1 scores: {scores}\")\n",
    "print(f\"Mean F1: {scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa66961",
   "metadata": {},
   "source": [
    "### Topic model: BERTopic ä¸»é¡Œè©ä¾†æºä½¿ç”¨c-TF-IDFé »ç‡å°å‘ï¼Œè¡¨ç¾æ–¹å¼åå‘è©é »é«˜çš„è©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19be7f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# çœŸå‡æ–°èé€²è¡Œä¸»é¡Œå»ºæ¨¡\n",
    "docs = data['text'].astype(str).tolist()\n",
    "\n",
    "# æ¨¡å‹å¯æ›æˆ 'all-MiniLM-L6-v2', 'microsoft/Phi-4-mini-instruct' ç­‰\n",
    "embedding_model = 'all-MiniLM-L6-v2'\n",
    "\n",
    "# å¯èª¿æ•´ æ¸¬è©¦ç”¨2000ç­†\n",
    "# min_cluster_size ç¾¤é›†æœ€å°‘éœ€è¦åŒ…å«nå€‹é»ï¼Œå¦å‰‡æœƒè¢«è¦–ç‚ºé›œè¨Šï¼ˆnoiseï¼‰\n",
    "# min_samples åŒ…å«è‡³å°‘nç¯‡æ–‡ç« çš„ä¸»é¡Œæ‰æœƒè¢«æ‰¿èªç‚ºä¸»é¡Œ\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=10, min_samples=30) # Clustering layer\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\")\n",
    "\n",
    "topic_model = BERTopic(embedding_model=embedding_model, hdbscan_model=hdbscan_model, vectorizer_model=vectorizer_model)\n",
    "topics, probs = topic_model.fit_transform(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3442db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹ä¸€å€‹å„²å­˜æ‰€æœ‰ä¸»é¡Œé—œéµè©èˆ‡ TF-IDF åˆ†æ•¸çš„æ¸…å–®\n",
    "all_topics = []\n",
    "\n",
    "# æŠŠä¸»é¡Œç¸½æ•¸æ‹¿å‡ºä¾†ï¼ˆæ’é™¤ -1 æ˜¯æœªåˆ†é¡ä¸»é¡Œï¼‰\n",
    "valid_topics = [topic for topic in topic_model.get_topic_info().Topic if topic != -1]\n",
    "\n",
    "# å°æ¯å€‹ä¸»é¡Œå–å¾—è©èˆ‡ c-TF-IDF åˆ†æ•¸\n",
    "for topic_id in valid_topics:\n",
    "    topic_words = topic_model.get_topic(topic_id)\n",
    "    for word, score in topic_words:\n",
    "        all_topics.append({\n",
    "            \"Topic\": topic_id,\n",
    "            \"Word\": word,\n",
    "            \"C-TF-IDF\": score\n",
    "        })\n",
    "\n",
    "# è½‰æ›æˆ DataFrame ä¸¦æ’åº\n",
    "topic_tfidf_df = pd.DataFrame(all_topics)\n",
    "topic_tfidf_df = topic_tfidf_df.sort_values(by=[\"Topic\", \"C-TF-IDF\"], ascending=[True, False])\n",
    "\n",
    "# é¡¯ç¤ºå‰å¹¾åˆ—\n",
    "topic_tfidf_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf36076",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# åˆ—å‡ºæ–‡ç« çš„BERTopicè³‡è¨Š\n",
    "topic_model.get_document_info(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4088184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_fake_news_ratio_by_topic(model, docs, labels, title=\"ä¸»é¡Œçš„å‡æ–°èæ¯”ä¾‹\"):\n",
    "    doc_info = model.get_document_info(docs).copy()\n",
    "    doc_info['label'] = labels\n",
    "\n",
    "    # è¨ˆç®—æ¯”ä¾‹èˆ‡æ•¸é‡\n",
    "    topic_fake_ratio = (\n",
    "        doc_info[doc_info['Topic'] != -1]\n",
    "        .groupby('Topic')['label']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={'label': 'fake_news_ratio'})\n",
    "    )\n",
    "    topic_counts = (\n",
    "        doc_info[doc_info['Topic'] != -1]['Topic']\n",
    "        .value_counts()\n",
    "        .rename_axis('Topic')\n",
    "        .reset_index(name='count')\n",
    "    )\n",
    "    topic_stats = pd.merge(topic_fake_ratio, topic_counts, on='Topic')\n",
    "\n",
    "    # åŠ ä¸Šä¸»é¡Œåç¨±\n",
    "    topic_names = model.get_topic_info()[['Topic', 'Name']]\n",
    "    topic_stats_named = topic_stats.merge(topic_names, on='Topic')\n",
    "\n",
    "    # éæ¿¾æ¯”ä¾‹éä½çš„ä¸»é¡Œ\n",
    "    topic_stats_named = topic_stats_named[topic_stats_named['fake_news_ratio'] >= 0.1]\n",
    "\n",
    "    # ç¹ªåœ–\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ax = sns.barplot(\n",
    "        data=topic_stats_named.sort_values(by='fake_news_ratio', ascending=False),\n",
    "        x='fake_news_ratio', y='Name', palette='Reds'\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel('å‡æ–°èæ¯”ä¾‹ (label=1)')\n",
    "    plt.ylabel('ä¸»é¡Œä»£è¡¨è©')\n",
    "    plt.grid(True, axis='x')\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55d4a85",
   "metadata": {},
   "source": [
    "### representation topic model: åŠ ä¸Šèªæ„å°å‘çš„KeyBERT, è¡¨ç¾æ–¹å¼æ˜¯èªæ„å‘é‡ç›¸ä¼¼çš„è© "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281f408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic.representation import KeyBERTInspired\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model_with_st = SentenceTransformer(embedding_model)  # æˆ–å…¶ä»–ä½ æŒ‡å®šçš„æ¨¡å‹\n",
    "embeddings = embedding_model_with_st.encode(docs, show_progress_bar=True)\n",
    "\n",
    "# é—œéµè©è¡¨ç¤ºæ¨¡å‹ï¼ˆéç”Ÿæˆå¼ï¼‰\n",
    "keybert = KeyBERTInspired()\n",
    "\n",
    "# çµ„è£ representation model\n",
    "representation_model = {\n",
    "    \"KeyBERT\": keybert\n",
    "}\n",
    "\n",
    "# å»ºç«‹ BERTopic æ¨¡å‹ï¼ˆç”¨ KeyBERT èª¿æ•´ä¸»é¡Œè¡¨ç¤ºï¼‰\n",
    "representation_topic_model = BERTopic(\n",
    "    embedding_model=embedding_model_with_st,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    representation_model=representation_model,\n",
    "    top_n_words=30,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# è¨“ç·´æ¨¡å‹\n",
    "topics, probs = representation_topic_model.fit_transform(docs, embeddings)\n",
    "\n",
    "# æŸ¥çœ‹æ–°çš„ä¸»é¡Œè¡¨ç¤º\n",
    "representation_topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f35ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¦–è¦ºåŒ–ä¸»é¡Œåˆ†å¸ƒï¼šåœ“åœˆå¤§å°æ˜¯ä¸»é¡Œçš„å¤§å°ï¼Œåœ“åœˆçš„è·é›¢æ˜¯ä¸»é¡Œä¹‹é–“çš„ç›¸ä¼¼åº¦\n",
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c52eb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "representation_topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de5782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸå§‹æ¨¡å‹çš„ä¸»é¡Œ\n",
    "visualize_fake_news_ratio_by_topic(topic_model, docs, data['label'], title=\"åŸå§‹ä¸»é¡Œçš„å‡æ–°èæ¯”ä¾‹\")\n",
    "\n",
    "# ä½¿ç”¨ KeyBERT è¡¨ç¤ºè©çš„æ¨¡å‹ä¸»é¡Œ\n",
    "visualize_fake_news_ratio_by_topic(representation_topic_model, docs, data['label'], title=\"KeyBERT ä¸»é¡Œçš„å‡æ–°èæ¯”ä¾‹\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
