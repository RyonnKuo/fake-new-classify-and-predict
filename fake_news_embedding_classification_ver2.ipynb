{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2077866c",
   "metadata": {
    "id": "2077866c"
   },
   "source": [
    "# BERT (Encoder-only-model)\n",
    "\n",
    "- Token classification\n",
    "    NER\n",
    "- Sequence classification\n",
    "    Sentiment Classification\n",
    "    Relation Extraction (RE)\n",
    "- Text Clustering (BERTopic)\n",
    "    Embedding model\n",
    "    Clustering model\n",
    "    ä½¿ç”¨Representationæ–¹æ³•å»å¾®èª¿ä¸»é¡Œè¡¨ç¤º\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950fcec1",
   "metadata": {
    "id": "950fcec1"
   },
   "source": [
    "### çœŸå‡æ–°èè³‡æ–™é›† ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kHUIVdFdFJao",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 134968,
     "status": "ok",
     "timestamp": 1749735935449,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "kHUIVdFdFJao",
    "outputId": "deb6b7b0-f9dc-46cd-af8d-cd7666eb0045"
   },
   "outputs": [],
   "source": [
    "# ğŸ”§ å®‰è£æ ¸å¿ƒè³‡æ–™è™•ç†èˆ‡æ¨¡å‹å¥—ä»¶\n",
    "!pip install pandas numpy scikit-learn matplotlib seaborn tqdm nltk vaderSentiment empath tabulate\n",
    "# ğŸ” å®‰è£ BERT ç›¸é—œï¼ˆtransformers, pipelineï¼‰\n",
    "!pip install transformers\n",
    "# ğŸ¤– å®‰è£å‘½åå¯¦é«”è¾¨è­˜ç”¨é è¨“ç·´æ¨¡å‹\n",
    "!pip install torch\n",
    "# ğŸ§  å®‰è£æƒ…ç·’åˆ†æå¾®èª¿æ¨¡å‹\n",
    "!pip install sentence-transformers\n",
    "# ğŸ“Š å®‰è£ä¸»é¡Œå»ºæ¨¡ï¼šBERTopic + HDBSCANï¼ˆæ”¯æ´ clusteringï¼‰\n",
    "!pip install bertopic hdbscan\n",
    "# ğŸ—‚ å­—é«”è¨­å®šç”¨ï¼ˆå¦‚ä½ åŠ è¼‰äº†è‡ªè¨‚å­—é«”ï¼‰\n",
    "!pip install fonttools\n",
    "!pip install --upgrade nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6tH_lAhbB52g",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1230,
     "status": "ok",
     "timestamp": 1749735936684,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "6tH_lAhbB52g",
    "outputId": "3ae7508e-acfd-43ae-a181-896ef1253e17"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gznsm2NL0XSg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1126,
     "status": "ok",
     "timestamp": 1749735937812,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "gznsm2NL0XSg",
    "outputId": "f060f513-6ccf-412d-9d36-666a6c67f04b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import nltk, ssl, os\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "nltk.data.path.extend([\n",
    "    '/usr/nltk_data',\n",
    "    '/usr/local/nltk_data',\n",
    "    '/usr/share/nltk_data',\n",
    "    '/usr/local/share/nltk_data',\n",
    "    '/root/nltk_data'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b54574",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 248,
     "status": "ok",
     "timestamp": 1749735938072,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "59b54574",
    "outputId": "5392bcac-87d5-4327-b9fb-5b10e98dc092"
   },
   "outputs": [],
   "source": [
    "# è¼‰å…¥è³‡æ–™é›†\n",
    "fake_df = pd.read_csv('./raw_data/fake.csv')\n",
    "            #/content/drive/MyDrive/Colab Notebooks/æœŸæœ«å°ˆæ¡ˆ/raw_data/Fake.csv #/content/drive/MyDrive/Colab Notebooks/æœŸæœ«å°ˆæ¡ˆ/test_data/Fake_Sample.csv\n",
    "true_df = pd.read_csv('./raw_data/true.csv')\n",
    "            #/content/drive/MyDrive/Colab Notebooks/æœŸæœ«å°ˆæ¡ˆ/raw_data/True.csv #/content/drive/MyDrive/Colab Notebooks/æœŸæœ«å°ˆæ¡ˆ/test_data/True_Sample.csv\n",
    "\n",
    "# åˆä½µ title å’Œ text æˆæ–°çš„ text æ¬„ä½\n",
    "fake_df['text'] = fake_df['title'].astype(str) + \" \" + fake_df['text'].astype(str)\n",
    "true_df['text'] = true_df['title'].astype(str) + \" \" + true_df['text'].astype(str)\n",
    "\n",
    "# åŠ ä¸Š label æ¬„ä½\n",
    "fake_df['label'] = 1\n",
    "true_df['label'] = 0\n",
    "\n",
    "# å–å‰ 1000 ç­†\n",
    "dataNum = 50\n",
    "news_data = pd.concat([fake_df.iloc[:dataNum], true_df.iloc[:dataNum]], ignore_index=True)\n",
    "\n",
    "# ç§»é™¤ç©ºå€¼ä¸¦åªä¿ç•™ text å’Œ label æ¬„ä½\n",
    "news_data = news_data[news_data['text'].notna()].reset_index(drop=True)\n",
    "news_data = news_data[['text', 'label']]\n",
    "\n",
    "# æª¢æŸ¥å„é¡åˆ¥æ•¸é‡\n",
    "print(news_data['label'].value_counts())\n",
    "print(news_data.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a8c468",
   "metadata": {
    "id": "18a8c468"
   },
   "source": [
    "### æ¨ç‰¹çœŸå‡æ¨æ–‡è³‡æ–™é›† ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7806f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1749735938208,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "1c7806f2",
    "outputId": "ef86ba43-79ee-452b-e3c0-ed6fc9b7522e"
   },
   "outputs": [],
   "source": [
    "# æ¨ç‰¹è³‡æ–™è™•ç†(ç‚ºäº†å˜—è©¦è§£æ±º åŠ å…¥TF-IDFéæ“¬åˆ&embedéå¼· å¯èƒ½æ˜¯å› ç‚ºæ–‡æœ¬ç‰¹å¾µå¤ªæ˜é¡¯çš„å•é¡Œ)\n",
    "tweet_df = pd.read_csv('./raw_data/Truth_Seeker_Model_Dataset_unindex.csv', encoding='ISO-8859-1')\n",
    "            #/content/drive/MyDrive/Colab Notebooks/æœŸæœ«å°ˆæ¡ˆ/raw_data/Truth_Seeker_Model_Dataset.csv #/content/drive/MyDrive/Colab Notebooks/æœŸæœ«å°ˆæ¡ˆ/test_data/Truth_Seeker_Model_Dataset_Sample.csv\n",
    "tweet_data = tweet_df[['BinaryNumTarget', 'tweet', '5_label_majority_answer']].copy()\n",
    "\n",
    "# æ¸…ç†\n",
    "tweet_data = tweet_data.dropna()\n",
    "tweet_data = tweet_data[~tweet_data['tweet'].str.contains('#REF!', na=False)]\n",
    "valid_labels = ['Agree', 'Mostly Agree']\n",
    "tweet_data = tweet_data[tweet_data['5_label_majority_answer'].isin(valid_labels)]\n",
    "\n",
    "# ç§»é™¤ 5_label_majority_answer æ¬„ä½ï¼Œä¸¦é‡æ–°å‘½åæ¬„ä½\n",
    "tweet_data = tweet_data.rename(columns={'BinaryNumTarget': 'label', 'tweet': 'text'})\n",
    "tweet_data = tweet_data[['text', 'label']]\n",
    "\n",
    "tweet_data_num = 1000  # å–nç­†\n",
    "tweet_data = tweet_data.groupby('label').apply(\n",
    "    lambda x: x.sample(n=min(len(x), tweet_data_num), random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(tweet_data['label'].value_counts())\n",
    "print(tweet_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3ab861",
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1749735938240,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "4c3ab861"
   },
   "outputs": [],
   "source": [
    "# åˆä½µå…©ä»½ä¸åŒä¾†æºè³‡æ–™é›†\n",
    "data = pd.concat([news_data, tweet_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7b5ac9",
   "metadata": {
    "id": "3b7b5ac9"
   },
   "source": [
    "## éœ€è¦åšæ–‡æœ¬é è™•ç†å—?\n",
    "\n",
    "ç›®çš„:\n",
    "- å»ºç«‹åˆ†é¡å™¨ä¾†é æ¸¬çœŸå‡æ–°è -> (TF-IDF + åˆ†é¡æ¨¡å‹éœ€è¦ä¹¾æ·¨çš„è³‡æ–™ï¼Œæœ‰å¹«åŠ©)\n",
    "- åˆ†æNER çµæœèˆ‡èªæ„åˆ†ä½ˆ -> (æœƒç ´å£èªæ„)\n",
    "- å»ºç«‹ä¸»é¡Œæ¨¡å‹ä¾†æ¢ç´¢èªæ„ä¸»é¡Œï¼ˆBERTopic -> (æœƒç ´å£èªæ„)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e324aa",
   "metadata": {
    "executionInfo": {
     "elapsed": 1467,
     "status": "ok",
     "timestamp": 1749735939726,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "d5e324aa"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "punct_pattern = re.compile(r\"[^a-z ]\")\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = punct_pattern.sub(\" \", text)\n",
    "    # ç”¨ preserve_line=True é¿é–‹ punkt_tab\n",
    "    tokens = word_tokenize(text, preserve_line=True)\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(w)\n",
    "        for w in tokens\n",
    "        if w not in stop_words and len(w) > 1\n",
    "    ]\n",
    "    return tokens\n",
    "\n",
    "data['tokens'] = data['text'].astype(str).apply(preprocess)\n",
    "data['clean_text'] = data['tokens'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270d2b60",
   "metadata": {
    "id": "270d2b60"
   },
   "source": [
    "## 06/10 å˜—è©¦æ–¹å‘ ##\n",
    "\n",
    "1. çœŸå‡æ–°è8æˆè³‡æ–™ä½œç‚ºè¨“ç·´é›†\n",
    "1. åˆ†åˆ¥ç”¨çœŸå‡æ–°èçš„2æˆåšç‚ºæ¸¬è©¦é›†1ã€æ¨ç‰¹çœŸå‡æ¨æ–‡ä½œç‚ºæ¸¬è©¦é›†2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68ddd12",
   "metadata": {
    "id": "e68ddd12"
   },
   "source": [
    "## NER é æ¸¬æ–°èçœŸå‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81df51b6",
   "metadata": {
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1749735939775,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "81df51b6"
   },
   "outputs": [],
   "source": [
    "from matplotlib.font_manager import fontManager\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fontManager.addfont('./public/TaipeiSansTCBeta-Regular.ttf')\n",
    "plt.rcParams['font.sans-serif'] = ['Taipei Sans TC Beta']\n",
    "plt.rcParams['font.size'] = '16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29def4a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 328342,
     "status": "ok",
     "timestamp": 1749736268119,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "29def4a1",
    "outputId": "0c7820aa-6dfd-44e1-be4e-230ce207a5b5"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from hdbscan import HDBSCAN\n",
    "from transformers import BertTokenizerFast, AutoTokenizer, AutoModelForTokenClassification, AutoModelForSequenceClassification, pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# è¼‰å…¥æ¨¡å‹èˆ‡ tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# å»ºç«‹ NER çµæœåˆ—è¡¨\n",
    "ner_rows = []\n",
    "\n",
    "# åˆ†åˆ‡å­—ä¸²\n",
    "def split_text(text, chunk_size=512):\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# é‡å°æ¯ç¯‡æ–‡ç« è·‘ NERï¼ˆå¯ç”¨ tqdm é¡¯ç¤ºé€²åº¦æ¢ï¼‰\n",
    "for idx, text in tqdm(data['text'].astype(str).items()):\n",
    "    try:\n",
    "        chunks = split_text(text)\n",
    "        all_ents = []\n",
    "        for chunk in chunks:\n",
    "            all_ents.extend(ner_pipeline(chunk))  # å°æ¯æ®µè·‘ NER\n",
    "        for ent in all_ents:\n",
    "            ner_rows.append({\n",
    "                \"index\": idx,\n",
    "                \"entity\": ent['entity_group'],  # e.g., PER, LOC\n",
    "                \"word\": ent['word'],\n",
    "                \"score\": ent['score']\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error at idx {idx}: {e}\")\n",
    "\n",
    "# å»ºç«‹ DataFrame\n",
    "ner_df = pd.DataFrame(ner_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875c0283",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1749736268179,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "875c0283",
    "outputId": "46797349-fcf9-44d0-ae61-13cfa2979d86"
   },
   "outputs": [],
   "source": [
    "ner_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbf7dd1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "executionInfo": {
     "elapsed": 454,
     "status": "ok",
     "timestamp": 1749736268646,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "2cbf7dd1",
    "outputId": "210344d9-aaee-415d-f07d-d334f88c2125"
   },
   "outputs": [],
   "source": [
    "# æ•´åˆ label\n",
    "merged_df = ner_df.merge(data[['label']], left_on='index', right_index=True)\n",
    "\n",
    "# èšåˆæ‰€æœ‰ entity é¡å‹çš„å‡ºç¾æ¬¡æ•¸\n",
    "entity_counts_all = (\n",
    "    merged_df.groupby(['index', 'entity'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)  # å¾—åˆ°æ¯ç¯‡æ–‡ç« å„é¡å¯¦é«”æ•¸\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# åˆä½µ label\n",
    "entity_counts_all = entity_counts_all.merge(data[['label']], left_on='index', right_index=True)\n",
    "\n",
    "# å»ºæ¨¡æ¬„ä½é¸æ“‡ï¼šæ‰€æœ‰å¯¦é«”é¡åˆ¥æ¬„ä½ï¼ˆæ’é™¤ index, labelï¼‰\n",
    "feature_cols = [col for col in entity_counts_all.columns if col not in ['index', 'label']]\n",
    "kmeans_fit_pred_data = entity_counts_all[feature_cols]\n",
    "\n",
    "# åš KMeans èšé¡\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "entity_counts_all['cluster'] = kmeans.fit_predict(kmeans_fit_pred_data)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(kmeans_fit_pred_data)\n",
    "entity_counts_all['PC1'] = X_pca[:, 0]\n",
    "entity_counts_all['PC2'] = X_pca[:, 1]\n",
    "# è¦–è¦ºåŒ–èšé¡çµæœ\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=entity_counts_all,\n",
    "    x='PC1', y='PC2', hue='cluster', style='label',\n",
    "    palette='Set2', s=100\n",
    ")\n",
    "\n",
    "plt.title('NER ç‰¹å¾µçš„ä¸»æˆåˆ†åˆ†æ + KMeans èšé¡')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ssZp0y9GbvTr",
   "metadata": {
    "id": "ssZp0y9GbvTr"
   },
   "source": [
    "ä½¿ç”¨NERç‰¹å¾µé€²è¡ŒKMeansèšé¡ï¼Œç„¡ç›£ç£å­¸ç¿’è‡ªå‹•åˆ†æˆå…©ç¾¤ï¼Œä¸Šåœ–ç‚ºæ¨¡å‹å‰çš„æ¢ç´¢æ€§è³‡æ–™åˆ†æ(EDA)çµæœï¼Œè§€å¯Ÿçµæœ:KMeansèšé¡æœ‰éƒ¨åˆ†æˆåŠŸèšå‡ºå‡æ–°èç¾¤ï¼Œæ©˜è‰²cluster1å¹¾ä¹éƒ½æ˜¯å‰å‰ç‚ºå‡æ–°èç¾¤ï¼Œç¶ è‰²cluster0åŒ…å«è¼ƒå¤šçœŸæ–°èèˆ‡éƒ¨åˆ†å‡æ–°èã€‚çµè«–ï¼šåˆ†ç¾¤é‡ç–Šæ˜é¡¯ï¼Œæ•´é«”åˆ†ç¾¤æ•ˆæœä¸ç®—éå¸¸å¥½ï¼Œä½†åˆæ­¥åˆ¤æ–·NERæœ‰å€åˆ¥èƒ½åŠ›ï¼Œéœ€çµåˆæ›´å¤šåˆ†é¡æ–¹å¼é€²è¡Œå¤šæ¨¡æ…‹èšé¡æ¨¡å‹è©•ä¼°!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e6e5bf",
   "metadata": {
    "id": "74e6e5bf"
   },
   "source": [
    "#### å˜—è©¦ç”¨ NER æå–å‡ºçš„'äººå'ã€'çµ„ç¹”'ã€'åœ°åæ•¸é‡'ä½œç‚ºè©å½™ç‰¹å¾µï¼Œå†é¤µçµ¦ TF-IDF + æ¨¡å‹ä¾†é æ¸¬é€™ç¯‡æ–°èæ˜¯çœŸ/å‡\n",
    "\n",
    "Part1. ä½¿ç”¨å‘½åå¯¦é«”è¾¨è­˜(NER)çš„çµæœç•¶ä½œç‰¹å¾µï¼Œä¾†åˆ†é¡çœŸå‡æ–°è(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d807d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1002,
     "status": "ok",
     "timestamp": 1749736269642,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "a8d807d7",
    "outputId": "656d47cd-a303-4e63-d0e1-d01087815c65"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.base import clone\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# === å»ºç«‹ç‰¹å¾µï¼ˆNER ä¾‹å­ï¼‰ ===\n",
    "entity_counts = ner_df.groupby(['index', 'entity']).size().unstack(fill_value=0)\n",
    "data_with_ner = data.copy()\n",
    "data_with_ner = data_with_ner.join(entity_counts, how='left').fillna(0)\n",
    "\n",
    "X = data_with_ner[['PER', 'ORG', 'LOC']]\n",
    "y = data_with_ner['label']\n",
    "\n",
    "# === åˆ†é¡å™¨åˆ—è¡¨ ===\n",
    "classifiers = {\n",
    "    \"LogReg\": LogisticRegression(max_iter=1000),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(),\n",
    "    \"SVM\": svm.SVC(probability=True),\n",
    "    \"RandomForest\": RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# === K-fold è¨­å®š ===\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# å„²å­˜å¹³å‡ f1-score çµæœ\n",
    "results = []\n",
    "\n",
    "# === åŸ·è¡Œäº¤å‰é©—è­‰ä¸¦å°å‡ºæ¯å€‹æ¨¡å‹å ±å‘Š ===\n",
    "for name, model in classifiers.items():\n",
    "    print(f\"\\n=== {name} åˆ†é¡çµæœï¼ˆ5-foldï¼‰ ===\")\n",
    "    y_true_all, y_pred_all = [], []\n",
    "    fold_f1_scores = []\n",
    "\n",
    "    for train_idx, test_idx in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        clf = clone(model)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        y_true_all.extend(y_test)\n",
    "        y_pred_all.extend(y_pred)\n",
    "\n",
    "        fold_f1_scores.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "    avg_f1 = np.mean(fold_f1_scores)\n",
    "    print(classification_report(\n",
    "        y_true_all, y_pred_all,\n",
    "        target_names=[\"çœŸæ–°è\", \"å‡æ–°è\"],\n",
    "        digits=2\n",
    "    ))\n",
    "\n",
    "    results.append({\n",
    "        \"classifier\": name,\n",
    "        \"f1_weighted\": avg_f1\n",
    "    })\n",
    "\n",
    "# === æ¯”è¼ƒçµæœè¡¨æ ¼ ===\n",
    "result_df = pd.DataFrame(results).sort_values(by=\"f1_weighted\", ascending=False).reset_index(drop=True)\n",
    "print(\"ğŸ å„æ¨¡å‹æ¯”è¼ƒï¼š\")\n",
    "print(tabulate(result_df, headers=\"keys\", tablefmt=\"fancy_grid\"))\n",
    "\n",
    "# === æ‰¾å‡ºæœ€ä½³æ¨¡å‹ ===\n",
    "best = result_df.iloc[0]\n",
    "print(f\"\\nğŸ† æœ€ä½³åˆ†é¡å™¨ç‚ºï¼š{best['classifier']}ï¼Œweighted F1 = {best['f1_weighted']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1e1c16",
   "metadata": {
    "id": "7b1e1c16"
   },
   "source": [
    "### NERæå–ç‰¹å¾µé æ¸¬çµæœå°šå¯\n",
    "å°çµ:\n",
    "é æ¸¬çœŸæ–°è:  LR      RF\n",
    "precision   0.71    0.75\n",
    "recall      0.62    0.74\n",
    "f1          0.66    0.74\n",
    "\n",
    "é æ¸¬å‡æ–°è:\n",
    "precision   0.66    0.74\n",
    "recall      0.74    0.75\n",
    "f1          0.70    0.74\n",
    "\n",
    "NER ç‰¹å¾µå°çœŸå‡æ–°èè¾¨è­˜æœ‰ä¸€å®šç¨‹åº¦ä½œç”¨ï¼Œä¸”ç”¨RandomForestçš„çµæœè¼ƒå„ª\n",
    "\n",
    "BY Chuya\n",
    "çµè«–ï¼šç‰¹å¾µå¤ªå°‘ï¼Œåªæœ‰ä¸‰ç¶­(PER,ORG,LOC)ï¼Œåªæä¾›ã€Œäººå/åœ°å/çµ„ç¹”ã€çš„æ•¸é‡ï¼Œè³‡è¨Šé‡å¤ªä½ã€‚å¾ˆå¤šæ–°èæˆ–æ¨æ–‡ä¸ä¸€å®šåŒ…å«é€™ä¸‰é¡å¯¦é«”ï¼Œé€ æˆå¤§é‡ç‚º 0ã€‚\n",
    "\n",
    "ğŸ‘‰ è³‡æ–™å€è¾¨æ€§ä½ï¼Œæ¨¡å‹é›£ä»¥å­¸ç¿’ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400ff9f4",
   "metadata": {
    "id": "400ff9f4"
   },
   "source": [
    "Part2.ä½¿ç”¨æƒ…ç·’åˆ†æè¾¨è­˜çœŸå‡æ–°è\n",
    "\n",
    "\n",
    "1. distilbertï¼šä¸€æ¬¾åŸºæ–¼SST-2å¾®èª¿çš„è¼•é‡ç´šBERTæ¨¡å‹ï¼Œå¸¸ç”¨æ–¼è‹±æ–‡ç”¢å“è©•è«–æˆ–å®¢æœå°è©±ä¸­çš„æƒ…ç·’æ­£è² åˆ†é¡ä»»å‹™ã€‚\n",
    "2. roberta-twitterï¼šå°ˆç‚ºTwitterè³‡æ–™è¨“ç·´çš„RoBERTaæ¨¡å‹ï¼Œå»£æ³›æ‡‰ç”¨æ–¼ç¤¾ç¾¤è²¼æ–‡çš„è¼¿æƒ…åˆ†æèˆ‡ç¤¾æœƒäº‹ä»¶æƒ…ç·’åµæ¸¬ã€‚\n",
    "3. bertweetï¼šä»¥æµ·é‡æ¨æ–‡èªæ–™è¨“ç·´çš„BERTæ¨¡å‹ï¼Œç‰¹åˆ¥é©ç”¨æ–¼ç¤¾ç¾¤åª’é«”ä¸Šçš„å³æ™‚æƒ…ç·’è¿½è¹¤èˆ‡ç”¨æˆ¶åæ‡‰åˆ†æã€‚\n",
    "4. nlptownï¼šä¸€å€‹æ”¯æ´å¤šèªè¨€ã€å¯è¼¸å‡º1ï½5æ˜Ÿç­‰ç´šçš„æƒ…ç·’å¼·åº¦æ¨¡å‹ï¼Œå¸¸ç”¨æ–¼å¤šèªè©•è«–è©•ç­‰ã€é¡§å®¢æ»¿æ„åº¦åˆ†æç­‰ä»»å‹™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d64c13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 899833,
     "status": "ok",
     "timestamp": 1749737169500,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "d3d64c13",
    "outputId": "7eded0ad-c018-4758-8b68-fd6ba73ca4b7"
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Part 2ï¼šæƒ…ç·’ç‰¹å¾µæ¨¡å‹æ¯”è¼ƒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from transformers import pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm import tqdm\n",
    "import pandas as pd, numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "sentiment_models = {\n",
    "    \"distilbert\"      : \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    \"roberta-twitter\" : \"cardiffnlp/twitter-roberta-base-sentiment\",\n",
    "    \"bertweet\"        : \"finiteautomata/bertweet-base-sentiment-analysis\",\n",
    "    \"nlptown\"         : \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "}\n",
    "\n",
    "classifiers = {\n",
    "    \"LogReg\"       : LogisticRegression(max_iter=1000),\n",
    "    \"DecisionTree\" : DecisionTreeClassifier(),\n",
    "    \"LinearSVC\"    : LinearSVC(),\n",
    "    \"RandomForest\" : RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "def split_chunks(txt, size=512):\n",
    "    return [txt[i:i+size] for i in range(0, len(txt), size)]\n",
    "\n",
    "def senti_score(txt, pipe):\n",
    "    try:\n",
    "        res = pipe(split_chunks(txt))\n",
    "        pos = [r['score'] for r in res if 'POS' in r['label'].upper()]\n",
    "        neg = [r['score'] for r in res if 'NEG' in r['label'].upper()]\n",
    "        return (sum(pos)/len(pos)) if pos and sum(pos) > sum(neg) \\\n",
    "               else -(sum(neg)/len(neg)) if neg else 0.0\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "all_results, model_mean = [], []\n",
    "\n",
    "for m_key, m_name in sentiment_models.items():\n",
    "    print(f\"\\nğŸ” Sentiment Model: {m_key}\")\n",
    "    pipe = pipeline(\"sentiment-analysis\", model=m_name, truncation=True)\n",
    "    data_tmp = data.copy()\n",
    "    tqdm.pandas()\n",
    "    data_tmp['sentiment_score'] = data_tmp['text'].astype(str).progress_apply(\n",
    "        lambda t: senti_score(t, pipe)\n",
    "    )\n",
    "\n",
    "    X_senti = data_tmp[['sentiment_score']]\n",
    "    y       = data_tmp['label']\n",
    "\n",
    "    f1_collect = []\n",
    "    for clf_key, clf in classifiers.items():\n",
    "        X_tr, X_te, y_tr, y_te = train_test_split(X_senti, y, test_size=0.2, random_state=42)\n",
    "        clf.fit(X_tr, y_tr)\n",
    "        y_pred = clf.predict(X_te)\n",
    "        f1_val = f1_score(y_te, y_pred, average='weighted')\n",
    "        f1_collect.append(f1_val)\n",
    "        all_results.append({\"model\": m_key, \"classifier\": clf_key, \"f1\": f1_val})\n",
    "\n",
    "    model_mean.append({\"model\": m_key, \"mean_f1\": np.mean(f1_collect)})\n",
    "\n",
    "# âœ æ‰¾å¹³å‡ F1 æœ€é«˜çš„æƒ…ç·’æ¨¡å‹\n",
    "model_df = pd.DataFrame(model_mean).sort_values('mean_f1', ascending=False)\n",
    "best_senti = model_df.iloc[0]['model']\n",
    "print(\"\\nğŸ“Š  æƒ…ç·’æ¨¡å‹å¹³å‡ F1ï¼š\")\n",
    "print(tabulate(model_df, headers=\"keys\", tablefmt=\"fancy_grid\"))\n",
    "print(f\"\\nğŸ† æœ€ä½³æƒ…ç·’æ¨¡å‹ï¼š{best_senti}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349aa9a5",
   "metadata": {
    "id": "349aa9a5"
   },
   "outputs": [],
   "source": [
    "\"\"\"# è¼‰å…¥æƒ…ç·’åˆ†ææ¨¡å‹(å¾®èª¿å¾Œçš„BERT)\n",
    "model = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "# å› ç‚ºé€™å€‹èªè¨€ä¹Ÿæ˜¯BERT = æ•ˆæœä»°è³´'è‡ªç„¶èªè¨€èªåºèˆ‡ä¸Šä¸‹æ–‡' = ä½¿ç”¨data['text']å³å¯\n",
    "\n",
    "# åˆ‡å‰²æ–‡å­— æ¯æ®µä¸è¶…é 512 å­—\n",
    "def split_text(text, chunk_size=512):\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# æ•´åˆæ®µè½çš„æƒ…ç·’åˆ†æ•¸\n",
    "def analyze_long_text(text):\n",
    "    try:\n",
    "        chunks = split_text(text)\n",
    "        results = model(chunks)\n",
    "\n",
    "        # çµ±è¨ˆæƒ…ç·’\n",
    "        pos_scores = [r['score'] for r in results if r['label'] == 'POSITIVE']\n",
    "        neg_scores = [r['score'] for r in results if r['label'] == 'NEGATIVE']\n",
    "\n",
    "        # å¹³å‡åˆ†æ•¸\n",
    "        avg_pos = sum(pos_scores) / len(pos_scores) if pos_scores else 0\n",
    "        avg_neg = sum(neg_scores) / len(neg_scores) if neg_scores else 0\n",
    "\n",
    "        # æ±ºå®šç¸½é«”æƒ…çºŒ\n",
    "        if avg_pos > avg_neg:\n",
    "            return pd.Series(['POSITIVE', avg_pos])\n",
    "        elif avg_neg > avg_pos:\n",
    "            return pd.Series(['NEGATIVE', avg_neg])\n",
    "        else:\n",
    "            return pd.Series(['NEUTRAL', 0.5])\n",
    "    except Exception:\n",
    "        return pd.Series(['ERROR', 0.0])\n",
    "\n",
    "# åŸ·è¡Œåˆ†æ\n",
    "tqdm.pandas()\n",
    "data[['sentiment_label', 'sentiment_score']] = data['text'].progress_apply(analyze_long_text)\n",
    "\n",
    "data.head(10)\n",
    "\n",
    "sentiment_pred_X = data[['sentiment_score']]\n",
    "sentiment_pred_y = data['label']\n",
    "\n",
    "# åˆ†å‰²è¨“ç·´èˆ‡æ¸¬è©¦é›†\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentiment_pred_X, sentiment_pred_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# å»ºç«‹æ¨¡å‹ä¸¦è¨“ç·´\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train, y_train)\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# é æ¸¬èˆ‡è©•ä¼°\n",
    "y_pred = lr_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# é æ¸¬èˆ‡è©•ä¼°\n",
    "rf_y_pred = rf_clf.predict(X_test)\n",
    "print(classification_report(y_test, rf_y_pred))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcccc87d",
   "metadata": {
    "id": "bcccc87d"
   },
   "source": [
    "### å°çµ: æƒ…ç·’é æ¸¬çœŸå‡æ–°èè¡¨ç¾ä¸å¥½\n",
    "å°çµ:\n",
    "é æ¸¬çœŸæ–°è:  LR      RF\n",
    "precision   0.60    0.52\n",
    "recall      0.37    0.50\n",
    "f1          0.46    0.51\n",
    "\n",
    "é æ¸¬å‡æ–°è:\n",
    "precision   0.54    0.52\n",
    "recall      0.75    0.54\n",
    "f1          0.63    0.53\n",
    "\n",
    "æ•´é«”åˆ†é¡æ•ˆæœåå¼±ï¼Œè·Ÿä¸ŸéŠ…æ¿å·®ä¸å¤š\n",
    "æ¨¡å‹åå¥½é æ¸¬ç‚ºå‡æ–°èï¼ˆrecall é«˜ï¼‰ï¼Œä½†ä¹Ÿå¤šèª¤åˆ¤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hZJmy5sYfDL8",
   "metadata": {
    "id": "hZJmy5sYfDL8"
   },
   "source": [
    "æ¡ç”¨HuggingFaceçš„distilbert-base-uncased-finetuned-sst-2-englishæ¨¡å‹ï¼Œé€™æ˜¯ä¸€å€‹å°è‹±æ–‡-é›»å½±è©•è«–åšæƒ…ç·’åˆ†é¡(positive/negative)çš„é è¨“ç·´æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e0b838",
   "metadata": {
    "id": "55e0b838"
   },
   "source": [
    "Part3.ã€€å˜—è©¦æ•´åˆå…©è€…(NER+æƒ…ç·’)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7yrPG8mXmvfH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 232013,
     "status": "ok",
     "timestamp": 1749737401520,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "7yrPG8mXmvfH",
    "outputId": "0fc62356-f179-4a19-aca2-26bef50a20a4"
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Part 3ï¼šNER + Best Sentiment ç‰¹å¾µè¨“ç·´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from transformers import pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.base import clone\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "\n",
    "# 1ï¸âƒ£ ç”¨æœ€ä½³æƒ…ç·’æ¨¡å‹é‡æ–°è¨ˆç®— sentiment_score\n",
    "best_pipe = pipeline(\"sentiment-analysis\", model=sentiment_models[best_senti], truncation=True)\n",
    "tqdm.pandas()\n",
    "data['sentiment_score'] = data['text'].astype(str).progress_apply(lambda t: senti_score(t, best_pipe))\n",
    "\n",
    "# 2ï¸âƒ£ åˆä½µ NER (PER/ORG/LOC) + sentiment_score\n",
    "feature_df = data_with_ner[['PER', 'ORG', 'LOC']].join(data['sentiment_score'])\n",
    "X_full = feature_df\n",
    "y_full = data['label']\n",
    "\n",
    "# 3ï¸âƒ£ å››å€‹åˆ†é¡å™¨\n",
    "final_clfs = {\n",
    "    \"LogReg\"       : LogisticRegression(max_iter=1000),\n",
    "    \"DecisionTree\" : DecisionTreeClassifier(),\n",
    "    \"SVM\"          : svm.SVC(probability=True),\n",
    "    \"RandomForest\" : RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "final_res = []\n",
    "\n",
    "for clf_key, base_clf in final_clfs.items():\n",
    "    y_all_t, y_all_p, f1_list = [], [], []\n",
    "\n",
    "    for tr_idx, te_idx in kf.split(X_full, y_full):\n",
    "        X_tr, X_te = X_full.iloc[tr_idx], X_full.iloc[te_idx]\n",
    "        y_tr, y_te = y_full.iloc[tr_idx], y_full.iloc[te_idx]\n",
    "\n",
    "        clf = clone(base_clf)\n",
    "        clf.fit(X_tr, y_tr)\n",
    "        y_pr = clf.predict(X_te)\n",
    "\n",
    "        y_all_t.extend(y_te); y_all_p.extend(y_pr)\n",
    "        f1_list.append(f1_score(y_te, y_pr, average='weighted'))\n",
    "\n",
    "    print(f\"\\n=== {clf_key} æ•´é«”å ±å‘Š ===\")\n",
    "    print(classification_report(y_all_t, y_all_p, target_names=[\"çœŸæ–°è\",\"å‡æ–°è\"], digits=2))\n",
    "\n",
    "    final_res.append({\"classifier\": clf_key, \"f1_weighted\": np.mean(f1_list)})\n",
    "\n",
    "# 4ï¸âƒ£ æ¯”è¼ƒè¡¨ & æœ€ä½³åˆ†é¡å™¨\n",
    "final_df = pd.DataFrame(final_res).sort_values('f1_weighted', ascending=False).reset_index(drop=True)\n",
    "print(\"\\nğŸ“Š  æœ€çµ‚ 4 åˆ†é¡å™¨æ¯”è¼ƒï¼š\")\n",
    "print(tabulate(final_df, headers=\"keys\", tablefmt=\"fancy_grid\"))\n",
    "\n",
    "best_cls = final_df.iloc[0]\n",
    "print(f\"\\nğŸ† æœ€çµ‚æœ€ä½³çµ„åˆï¼šæƒ…ç·’æ¨¡å‹={best_senti} + åˆ†é¡å™¨={best_cls['classifier']}ï¼Œweighted F1={best_cls['f1_weighted']:.4f}\")\n",
    "\n",
    "# 5ï¸âƒ£ (å¯é¸) è¦–è¦ºåŒ–\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x='classifier', y='f1_weighted', data=final_df, palette='Set2')\n",
    "plt.title(f'NER + Sentiment({best_senti})  4 åˆ†é¡å™¨æ¯”è¼ƒ')\n",
    "plt.ylabel('Weighted F1')\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec497be",
   "metadata": {
    "id": "eec497be"
   },
   "outputs": [],
   "source": [
    "\"\"\"from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "combined_X = pd.concat([data_with_ner[['PER', 'ORG', 'LOC']], sentiment_pred_X], axis=1)\n",
    "combined_y = data['label']\n",
    "# åˆ†å‰²è¨“ç·´èˆ‡æ¸¬è©¦é›†\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_X, combined_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression\n",
    "clf_lr = LogisticRegression()\n",
    "clf_lr.fit(X_train, y_train)\n",
    "lr_preds = clf_lr.predict(X_test)\n",
    "\n",
    "# Random Forest\n",
    "clf_rf = RandomForestClassifier(random_state=42)\n",
    "clf_rf.fit(X_train, y_train)\n",
    "rf_preds = clf_rf.predict(X_test)\n",
    "\n",
    "# è©•ä¼°çµæœ\n",
    "print(\"=== Logistic Regression åˆ†é¡çµæœ ===\")\n",
    "print(classification_report(y_test, lr_preds))\n",
    "\n",
    "print(\"=== Random Forest åˆ†é¡çµæœ ===\")\n",
    "print(classification_report(y_test, rf_preds)) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2OZ3X91gvzZA",
   "metadata": {
    "id": "2OZ3X91gvzZA"
   },
   "source": [
    "Part4.NER+æƒ…ç·’+TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8U7BNkkiv8yI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 230421,
     "status": "ok",
     "timestamp": 1749737631960,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "8U7BNkkiv8yI",
    "outputId": "d121b30c-9bcd-4cbb-b5d0-72ea0d4f5d6e"
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Part 4ï¼šTF-IDF + NER + Best Sentiment æ¨¡å‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import clone\n",
    "from tabulate import tabulate\n",
    "from transformers import pipeline\n",
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# âœ… ä½¿ç”¨ Part 2 æœ€ä½³æƒ…ç·’æ¨¡å‹é‡æ–°ç”Ÿæˆ sentiment_score\n",
    "senti_model_name = sentiment_models[best_senti]\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\", model=senti_model_name, truncation=True)\n",
    "tqdm.pandas()\n",
    "data['sentiment_score'] = data['text'].astype(str).progress_apply(lambda t: senti_score(t, sentiment_pipe))\n",
    "\n",
    "# 1ï¸âƒ£ å»ºç«‹ TF-IDF ç‰¹å¾µ\n",
    "tfidf_vec = TfidfVectorizer(max_features=200, ngram_range=(1, 2))\n",
    "tfidf_mat = tfidf_vec.fit_transform(data['clean_text'].fillna(''))\n",
    "tfidf_df = pd.DataFrame(tfidf_mat.toarray(),\n",
    "                        columns=tfidf_vec.get_feature_names_out(),\n",
    "                        index=data.index)\n",
    "\n",
    "# 2ï¸âƒ£ å–å¾— NER ç‰¹å¾µ + æœ€æ–° sentiment åˆ†æ•¸\n",
    "ner_df     = data_with_ner[['PER', 'ORG', 'LOC']].copy()\n",
    "senti_df   = data[['sentiment_score']]\n",
    "X_features = pd.concat([ner_df, senti_df, tfidf_df], axis=1)\n",
    "y_target   = data['label']\n",
    "\n",
    "# 3ï¸âƒ£ å®šç¾©åˆ†é¡å™¨\n",
    "classifiers = {\n",
    "    \"LogReg\"       : LogisticRegression(max_iter=1000),\n",
    "    \"DecisionTree\" : DecisionTreeClassifier(),\n",
    "    \"SVM\"          : svm.SVC(probability=True),\n",
    "    \"RandomForest\" : RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# 4ï¸âƒ£ Cross-Validation è¨“ç·´\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = []\n",
    "\n",
    "for clf_name, clf_model in classifiers.items():\n",
    "    print(f\"\\n=== {clf_name} åˆ†é¡çµæœï¼ˆ5-foldï¼‰ ===\")\n",
    "    y_all_true, y_all_pred, f1s = [], [], []\n",
    "\n",
    "    for train_idx, test_idx in kf.split(X_features, y_target):\n",
    "        X_train, X_test = X_features.iloc[train_idx], X_features.iloc[test_idx]\n",
    "        y_train, y_test = y_target.iloc[train_idx], y_target.iloc[test_idx]\n",
    "\n",
    "        clf = clone(clf_model)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        y_all_true.extend(y_test)\n",
    "        y_all_pred.extend(y_pred)\n",
    "        f1s.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "    print(classification_report(y_all_true, y_all_pred,\n",
    "                                target_names=[\"çœŸæ–°è\", \"å‡æ–°è\"], digits=2))\n",
    "\n",
    "    results.append({\n",
    "        \"classifier\": clf_name,\n",
    "        \"f1_weighted\": np.mean(f1s)\n",
    "    })\n",
    "\n",
    "# 5ï¸âƒ£ è¼¸å‡ºç¸½çµ\n",
    "result_df = pd.DataFrame(results).sort_values(by='f1_weighted', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nğŸ“Š TF-IDF + NER + Sentiment åˆ†é¡å™¨æ¯”è¼ƒï¼š\")\n",
    "print(tabulate(result_df, headers=\"keys\", tablefmt=\"fancy_grid\"))\n",
    "\n",
    "best = result_df.iloc[0]\n",
    "print(f\"\\nğŸ† æœ€ä½³åˆ†é¡å™¨ç‚ºï¼š{best['classifier']}ï¼Œweighted F1 = {best['f1_weighted']:.4f}\")\n",
    "\n",
    "# â• å¯é¸è¦–è¦ºåŒ–\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x='classifier', y='f1_weighted', data=result_df, palette='Set3')\n",
    "plt.title(f\"TF-IDF + NER + Sentiment({best_senti}) åˆ†é¡å™¨æ¯”è¼ƒ\")\n",
    "plt.ylabel('Weighted F1')\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3xHOBUT211bp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 142,
     "status": "ok",
     "timestamp": 1749737632097,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "3xHOBUT211bp",
    "outputId": "f0ab62f9-a352-44a7-de19-5e04ffaa6af8"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# ä¾ label åˆ†ç¾¤\n",
    "true_texts = data[data['label'] == 0]['clean_text'].fillna('')\n",
    "fake_texts = data[data['label'] == 1]['clean_text'].fillna('')\n",
    "\n",
    "# å»ºç«‹ TF-IDF å‘é‡å™¨ï¼ˆå¯ä½¿ç”¨ç›¸åŒè¨­å®šä»¥ä¾¿æ¯”è¼ƒï¼‰\n",
    "tfidf = TfidfVectorizer(max_features=2000, ngram_range=(1, 2))\n",
    "\n",
    "# æ“¬åˆæ–¼çœŸæ–°è\n",
    "true_tfidf_matrix = tfidf.fit_transform(true_texts)\n",
    "true_feature_names = tfidf.get_feature_names_out()\n",
    "true_scores = true_tfidf_matrix.mean(axis=0).A1\n",
    "true_top30 = sorted(zip(true_feature_names, true_scores), key=lambda x: x[1], reverse=True)[:30]\n",
    "\n",
    "# æ“¬åˆæ–¼å‡æ–°èï¼ˆéœ€é‡æ–°å»ºä¸€å€‹ vectorizer æ‰ä¸æœƒå…±ç”¨å­—å…¸ï¼‰\n",
    "tfidf_fake = TfidfVectorizer(max_features=2000, ngram_range=(1, 2))\n",
    "fake_tfidf_matrix = tfidf_fake.fit_transform(fake_texts)\n",
    "fake_feature_names = tfidf_fake.get_feature_names_out()\n",
    "fake_scores = fake_tfidf_matrix.mean(axis=0).A1\n",
    "fake_top30 = sorted(zip(fake_feature_names, fake_scores), key=lambda x: x[1], reverse=True)[:30]\n",
    "\n",
    "# å°‡å…©å€‹ DataFrame åŠ ä¸Š index ä¸¦ reset\n",
    "true_df = pd.DataFrame(true_top30, columns=[\"çœŸæ–°èè©\", \"çœŸ_TF-IDF\"]).reset_index(drop=True)\n",
    "fake_df = pd.DataFrame(fake_top30, columns=[\"å‡æ–°èè©\", \"å‡_TF-IDF\"]).reset_index(drop=True)\n",
    "\n",
    "# åˆä½µç‚ºä¸€å€‹è¡¨æ ¼ï¼ˆå·¦å³æ¯”å°ï¼‰\n",
    "compare_df = pd.concat([true_df, fake_df], axis=1)\n",
    "\n",
    "# é¡¯ç¤ºçµæœ\n",
    "from IPython.display import display\n",
    "print(\"ğŸ“Š çœŸæ–°è vs å‡æ–°è å‰ 30 å¸¸è¦‹é—œéµè©ï¼ˆTF-IDF åˆ†æ•¸ï¼‰\")\n",
    "display(compare_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6497f1",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "8d6497f1",
    "outputId": "3ba8250f-f1b6-41fc-b790-8d53a8a04ec9"
   },
   "outputs": [],
   "source": [
    "# \"\"\"# å˜—è©¦å¢åŠ TF-IDFæ¬„ä½(clean_text)\n",
    "\n",
    "# # å»ºç«‹ TF-IDF å‘é‡å™¨ï¼ˆå¯è‡ªè¨‚ ngram ç¯„åœèˆ‡ç¶­åº¦é™åˆ¶ï¼‰\n",
    "# tfidf = TfidfVectorizer(max_features=200, ngram_range=(1, 2))\n",
    "# tfidf_matrix = tfidf.fit_transform(data['clean_text'].fillna(''))\n",
    "\n",
    "# # è½‰ç‚º DataFrame\n",
    "# tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out(), index=data.index)\n",
    "\n",
    "# # æ–°å¢tf-idfæ¬„ä½\n",
    "# ner_sentiment_df = pd.concat([ner_pred_X, sentiment_pred_X], axis=1)\n",
    "# combined_X_full = pd.concat([ner_sentiment_df, tfidf_df], axis=1)\n",
    "\n",
    "# # åˆ†å‰²è³‡æ–™\n",
    "# X_train, X_test, y_train, y_test = train_test_split(combined_X_full, combined_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Logistic\n",
    "# clf_lr = LogisticRegression()\n",
    "# clf_lr.fit(X_train, y_train)\n",
    "# lr_preds = clf_lr.predict(X_test)\n",
    "\n",
    "# # Random Forest\n",
    "# clf_rf = RandomForestClassifier(random_state=42)\n",
    "# clf_rf.fit(X_train, y_train)\n",
    "# rf_preds = clf_rf.predict(X_test)\n",
    "\n",
    "# # è©•ä¼°\n",
    "# print(\"=== Logistic Regression(NER + Sentiment + TF-IDF) ===\")\n",
    "# print(classification_report(y_test, lr_preds))\n",
    "\n",
    "# print(\"=== Random Forest(NER + Sentiment + TF-IDF) ===\")\n",
    "# print(classification_report(y_test, rf_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "S1F-x4xHLhyi",
   "metadata": {
    "id": "S1F-x4xHLhyi"
   },
   "source": [
    "Part5. TF-IDF+NER+Sentimentç‰¹å¾µä¸Šï¼Œå†åŠ å…¥VADER(+Empath)èˆ‡æ–‡å­—è¡¨é”Styleç‰¹å¾µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3iJgV5_gMNHs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10054,
     "status": "ok",
     "timestamp": 1749733273778,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "3iJgV5_gMNHs",
    "outputId": "f2697dd2-9025-40f6-f2c3-bd07a5a8966f"
   },
   "outputs": [],
   "source": [
    "!pip install vaderSentiment empath tabulate tqdm seaborn matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFDPH3NGLf7k",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 5075,
     "status": "ok",
     "timestamp": 1749737722134,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "SFDPH3NGLf7k",
    "outputId": "ebc5fc07-1527-4bcd-ea51-d8ca1ad15c32"
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Part 5ï¼šTF-IDF+NER+Sentimentç‰¹å¾µä¸Šï¼Œå†åŠ å…¥VADER(+Empath)èˆ‡æ–‡å­—è¡¨é”Styleç‰¹å¾µ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ------------------------------------------------------------------\n",
    "# 1. é€éå¡æ–¹æª¢å®š (chi-square) æ‰¾å‡ºã€Œå‡æ–°è > çœŸæ–°èã€æœ€å…·å€è¾¨åŠ›çš„ n-gram\n",
    "# ------------------------------------------------------------------\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np, pandas as pd, re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ï¼ˆ1ï¼‰å»ºä¸€å€‹è©è¢‹æ¨¡å‹ï¼ˆunigram+bigramï¼Œéæ¿¾è‹±æ–‡åœç”¨å­—, min_df=5 é¿å…å¤ªç¨€æœ‰ï¼‰\n",
    "cv = CountVectorizer(stop_words='english', ngram_range=(1,2), min_df=5)\n",
    "X_bow = cv.fit_transform(data['clean_text'])\n",
    "feature_names = np.array(cv.get_feature_names_out())\n",
    "\n",
    "# ï¼ˆ2ï¼‰åšå¡æ–¹æª¢å®šï¼›label=1 ä»£è¡¨å‡æ–°è\n",
    "chi_scores, _ = chi2(X_bow, data['label'])\n",
    "\n",
    "# ï¼ˆ3ï¼‰åªä¿ç•™åœ¨å‡æ–°èå‡ºç¾æ¬¡æ•¸ > çœŸæ–°èçš„è©ï¼Œå†å–å‰ 30 å\n",
    "fake_mask = (X_bow[data['label'].values==1].sum(axis=0) >\n",
    "             X_bow[data['label'].values==0].sum(axis=0)).A1\n",
    "candidate_words = feature_names[fake_mask]\n",
    "candidate_scores= chi_scores[fake_mask]\n",
    "\n",
    "top_k = 30\n",
    "top_idx = np.argsort(candidate_scores)[::-1][:top_k]\n",
    "auto_clickbait = set(candidate_words[top_idx])\n",
    "\n",
    "print(f\"ğŸ” è‡ªå‹•åµæ¸¬åˆ° {len(auto_clickbait)} å€‹å‡æ–°èé«˜ç›¸é—œè©ï¼ˆå‰ {top_k}ï¼‰ï¼š\")\n",
    "print(sorted(auto_clickbait))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. å»ºç«‹ VADER / Empath / Style ç‰¹å¾µï¼ˆå«ã€Œå‹•æ…‹ click-baitã€ï¼‰\n",
    "# ------------------------------------------------------------------\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "try:\n",
    "    from empath import Empath\n",
    "    lexicon = Empath(); use_empath = True\n",
    "except ModuleNotFoundError:\n",
    "    use_empath = False\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# 2-1 VADER\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "vader_df = (data['text'].progress_apply(vader.polarity_scores)\n",
    "                       .apply(pd.Series).add_prefix('vader_'))\n",
    "print(\"\\nğŸ§  VADER æƒ…ç·’æ¨å‹•ç‰¹å¾µï¼ˆå‰å¹¾ç­†ï¼‰ï¼š\")\n",
    "print(vader_df.head())\n",
    "\n",
    "# 2-2 Empathï¼ˆé¸æ“‡å¹¾å€‹å¸¸ç”¨æƒ…ç·’ç¤¾æœƒé¢å‘ï¼‰\n",
    "if use_empath:\n",
    "    empath_raw = data['text'].progress_apply(lambda t: lexicon.analyze(t, normalize=True))\n",
    "    empath_keep = ['positive_emotion','negative_emotion','anger','sadness',\n",
    "                   'fear','politics','money','fun','love']\n",
    "    empath_df = (pd.DataFrame(empath_raw.tolist())\n",
    "                   [empath_keep].add_prefix('empath_'))\n",
    "\n",
    "    print(\"\\nğŸ¯ NRC-Empath æƒ…ç·’å‘é‡ï¼ˆå‰å¹¾ç­†ï¼‰ï¼š\")\n",
    "    print(empath_df.head())\n",
    "else:\n",
    "    empath_df = pd.DataFrame(index=data.index)   # ç©º DF\n",
    "    print(\"\\nâš ï¸ æœªå•Ÿç”¨ Empathï¼ˆéœ€ pip install empathï¼‰\")\n",
    "\n",
    "# 2-3 Style featuresï¼ˆå¤§å¯«æ¯”ä¾‹ / ! å¯†åº¦ / è‡ªå‹• click-bait å‘½ä¸­ç‡ï¼‰\n",
    "def style_feats(txt:str):\n",
    "    L = max(len(txt),1)\n",
    "    txt_low = txt.lower()\n",
    "    hit_cnt = sum(1 for w in auto_clickbait if w in txt_low)\n",
    "    return pd.Series({\n",
    "        'caps_ratio'      : sum(c.isupper() for c in txt)/L,\n",
    "        'excl_ratio'      : txt.count('!')/L,\n",
    "        'clickbait_ratio' : hit_cnt / len(auto_clickbait)\n",
    "    })\n",
    "\n",
    "style_df = data['text'].progress_apply(style_feats)\n",
    "\n",
    "print(\"\\nğŸ“ æ–‡å­—è¡¨é”æ–¹å¼ç‰¹å¾µï¼ˆå¤§å¯«æ¯”ä¾‹ / æ„Ÿå˜†è™Ÿå¯†åº¦ / Click-bait å‘½ä¸­ç‡ï¼‰\")\n",
    "print(style_df.describe())\n",
    "\n",
    "print(\"\\nğŸ“Š å‡æ–°èèˆ‡çœŸæ–°èçš„ Style ç‰¹å¾µå¹³å‡æ¯”è¼ƒï¼š\")\n",
    "print(pd.concat([style_df, data['label']], axis=1)\n",
    "        .groupby('label').mean()\n",
    "        .rename(index={0: \"çœŸæ–°è\", 1: \"å‡æ–°è\"}))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. æŠŠæ–°ç‰¹å¾µæ¥åˆ°æ—¢æœ‰ X_featuresï¼ˆTF-IDF + NER + Best-Sentimentï¼‰\n",
    "# ------------------------------------------------------------------\n",
    "X_final = pd.concat([X_features, vader_df, empath_df, style_df], axis=1)\n",
    "y_final = data['label']\n",
    "print(\"ğŸ”¢ æ–°å¢å¾Œç‰¹å¾µç¶­åº¦ :\", X_final.shape)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4. å››å€‹åˆ†é¡å™¨ Ã— 5-fold äº¤å‰é©—è­‰\n",
    "# ------------------------------------------------------------------\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import clone\n",
    "from tabulate import tabulate\n",
    "import seaborn as sns, matplotlib.pyplot as plt\n",
    "\n",
    "clfs = {\n",
    "    \"LogReg\"      : LogisticRegression(max_iter=1000),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(),\n",
    "    \"SVM\"         : SVC(probability=True),\n",
    "    \"RandomForest\": RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "kf = StratifiedKFold(5, shuffle=True, random_state=42)\n",
    "rows = []\n",
    "\n",
    "for name, base in clfs.items():\n",
    "    y_t, y_p, f1s = [], [], []\n",
    "    for tr, te in kf.split(X_final, y_final):\n",
    "        mdl = clone(base).fit(X_final.iloc[tr], y_final.iloc[tr])\n",
    "        pred = mdl.predict(X_final.iloc[te])\n",
    "        y_t.extend(y_final.iloc[te]); y_p.extend(pred)\n",
    "        f1s.append(f1_score(y_final.iloc[te], pred, average='weighted'))\n",
    "    print(f\"\\n=== {name} å ±å‘Š (åŠ  VADER / Style) ===\")\n",
    "    print(classification_report(y_t, y_p, target_names=['çœŸæ–°è','å‡æ–°è'], digits=2))\n",
    "    rows.append({\"classifier\": name, \"f1_weighted\": np.mean(f1s)})\n",
    "\n",
    "res_df = pd.DataFrame(rows).sort_values('f1_weighted', ascending=False)\n",
    "print(\"\\nğŸ“Š  åŠ  VADER / Style / å‹•æ…‹ Click-bait å¾Œåˆ†é¡å™¨æ¯”è¼ƒ\")\n",
    "print(tabulate(res_df, headers=\"keys\", tablefmt=\"fancy_grid\"))\n",
    "\n",
    "best_cls = res_df.iloc[0]\n",
    "print(f\"\\nğŸ†  æ–°æœ€ä½³æ¨¡å‹ï¼š{best_cls['classifier']}  (Weighted F1 = {best_cls['f1_weighted']:.4f})\")\n",
    "\n",
    "# ï¼ˆå¯é¸ï¼‰é•·æ¢åœ–\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.barplot(x='classifier', y='f1_weighted', data=res_df, palette='Set2')\n",
    "plt.title('åŠ å…¥ VADER / Style ç‰¹å¾µå¾Œçš„åˆ†é¡å™¨æ¯”è¼ƒ')\n",
    "plt.ylabel('Weighted F1')\n",
    "plt.ylim(0,1); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kXCGLyxag7DL",
   "metadata": {
    "id": "kXCGLyxag7DL"
   },
   "source": [
    "Part6. NER+Sentimentç‰¹å¾µ+SBERTå‘é‡åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RRiE1c9WgFLd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "1bd81346eb5446ae86c8992ea87a8979",
      "3aa0234db1f4447da06ae0ba78593276",
      "aaefd6793ed644b3afbf183bec859bff",
      "e108b8551f0e41b69ce386e7cbdf3945",
      "9c0fc22fc33a4388b5cde67492782329",
      "a7bd128141e746019bbd9eeed16eda0c",
      "c9da9c06c90a475ab7e45abc95f3dec3",
      "953dd787d72045bc9382fef65869ced1",
      "30cfa6df7235440a8aaf12b38e7c1a24",
      "2ff3e28618eb412c9f1879564137be6c",
      "07b0a05b872c48ec8bc00ec063304866"
     ]
    },
    "executionInfo": {
     "elapsed": 13099,
     "status": "ok",
     "timestamp": 1749738724842,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "RRiE1c9WgFLd",
    "outputId": "3b0c94d7-cadd-4537-9858-96bf4b02f141"
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€ Part 6ï¼šNER+Sentimentç‰¹å¾µ+SBERTå‘é‡åŒ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# â–1. SBERT å‘é‡åŒ–\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')  # å¯æ”¹å…¶ä»–å¦‚ 'paraphrase-MiniLM-L6-v2'\n",
    "sbert_embeddings = model.encode(data['clean_text'].fillna(''), show_progress_bar=True)\n",
    "\n",
    "sbert_df = pd.DataFrame(sbert_embeddings, index=data.index)\n",
    "sbert_df.columns = sbert_df.columns.astype(str)\n",
    "print(\"ğŸ“ å‘é‡ç¶­åº¦ï¼š\", sbert_df.shape)\n",
    "\n",
    "# â–2. åˆä½µå…¶ä»–ç‰¹å¾µï¼ˆNER + Sentimentï¼‰\n",
    "ner_df    = data_with_ner[['PER', 'ORG', 'LOC']].copy()\n",
    "senti_df  = data[['sentiment_score']]\n",
    "X_sbert   = pd.concat([sbert_df, ner_df, senti_df], axis=1)\n",
    "y_target  = data['label']\n",
    "\n",
    "# â–3. å»ºç«‹åˆ†é¡å™¨çµ„åˆ\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.base import clone\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "classifiers = {\n",
    "    \"LogReg\"       : LogisticRegression(max_iter=1000),\n",
    "    \"DecisionTree\" : DecisionTreeClassifier(),\n",
    "    \"SVM\"          : SVC(probability=True),\n",
    "    \"RandomForest\" : RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# â–4. Cross-validation æ¯”è¼ƒè¡¨ç¾\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = []\n",
    "\n",
    "for clf_name, clf_model in classifiers.items():\n",
    "    print(f\"\\n=== {clf_name} åˆ†é¡çµæœï¼ˆ5-foldï¼‰ ===\")\n",
    "    y_all_true, y_all_pred, f1s = [], [], []\n",
    "\n",
    "    for train_idx, test_idx in kf.split(X_sbert, y_target):\n",
    "        X_train, X_test = X_sbert.iloc[train_idx], X_sbert.iloc[test_idx]\n",
    "        y_train, y_test = y_target.iloc[train_idx], y_target.iloc[test_idx]\n",
    "\n",
    "        clf = clone(clf_model)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        y_all_true.extend(y_test)\n",
    "        y_all_pred.extend(y_pred)\n",
    "        f1s.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "    print(classification_report(y_all_true, y_all_pred, target_names=[\"çœŸæ–°è\", \"å‡æ–°è\"], digits=2))\n",
    "\n",
    "    results.append({\n",
    "        \"classifier\": clf_name,\n",
    "        \"f1_weighted\": np.mean(f1s)\n",
    "    })\n",
    "\n",
    "# â–5. é¡¯ç¤ºæ¯”è¼ƒçµæœ\n",
    "result_df = pd.DataFrame(results).sort_values(by='f1_weighted', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nğŸ“Š SBERT + NER + Sentiment åˆ†é¡å™¨æ¯”è¼ƒï¼š\")\n",
    "print(tabulate(result_df, headers=\"keys\", tablefmt=\"fancy_grid\"))\n",
    "\n",
    "best = result_df.iloc[0]\n",
    "print(f\"\\nğŸ† æœ€ä½³åˆ†é¡å™¨ç‚ºï¼š{best['classifier']}ï¼Œweighted F1 = {best['f1_weighted']:.4f}\")\n",
    "\n",
    "# â–6. è¦–è¦ºåŒ–çµæœ\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x='classifier', y='f1_weighted', data=result_df, palette='Set2')\n",
    "plt.title(\"BERT å‘é‡ + NER + Sentiment åˆ†é¡å™¨æ¯”è¼ƒ\")\n",
    "plt.ylabel('Weighted F1')\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HbkktXE-jqDV",
   "metadata": {
    "id": "HbkktXE-jqDV"
   },
   "source": [
    "1. ä½¿ç”¨all-MiniLM-L6-v2ï¼Œæœ€ä½³åˆ†é¡å™¨ç‚ºï¼šRandomForestï¼Œweighted F1 = 0.7197\n",
    "2. ä½¿ç”¨sentence-transformers/paraphrase-MiniLM-L6-v2ï¼Œæœ€ä½³åˆ†é¡å™¨ç‚ºï¼šLogRegï¼Œweighted F1 = 0.7467"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa66961",
   "metadata": {
    "id": "2aa66961"
   },
   "source": [
    "### Topic model: BERTopic ä¸»é¡Œè©ä¾†æºä½¿ç”¨c-TF-IDFé »ç‡å°å‘ï¼ŒæŒ‘å‡ºè©é »é«˜çš„è©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gmCYJY1vm9EW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1611,
     "status": "ok",
     "timestamp": 1749741036530,
     "user": {
      "displayName": "Etta Chuang",
      "userId": "11710828896069713513"
     },
     "user_tz": -480
    },
    "id": "gmCYJY1vm9EW",
    "outputId": "5b1a8b04-f301-4c61-8191-b9a0d51b4ed1"
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 0. å‰ç½®æ¢ä»¶èªªæ˜\n",
    "#   - data['clean_text'] éœ€ç‚ºæ¸…ç†å¾Œæ–‡æœ¬æ¬„\n",
    "#   - data['label'] ç‚ºçœŸå‡æ¨™è¨˜ï¼ˆ0=çœŸæ–°èï¼Œ1=å‡æ–°èï¼‰\n",
    "#   - è‹¥ç”¨ \"tfidf_style\"ï¼Œéœ€å·²å…ˆç®—å¥½ X_final\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# -------- 1. é¸æ“‡å‘é‡åŒ–æ–¹å¼ -------------------------------------Part 4 / 5 / 6 çµæœè¼¸å…¥\n",
    "VEC_CHOICE = \"tfidf\"       # â† è¼¸å…¥ # Part 4- \"tfidf\" / Part 5- \"tfidf_style\" / Part 6- \"sbert\"\n",
    "texts = data['clean_text'].fillna('')\n",
    "\n",
    "if VEC_CHOICE == \"tfidf\":\n",
    "    vec_model = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), stop_words=\"english\")\n",
    "    embeddings = vec_model.fit_transform(texts)\n",
    "\n",
    "elif VEC_CHOICE == \"tfidf_style\":\n",
    "    if \"X_final\" not in globals():\n",
    "        raise RuntimeError(\"âš ï¸ æ‰¾ä¸åˆ° X_finalï¼Œè«‹å…ˆåŸ·è¡Œ Part 5 å»ºç«‹ç‰¹å¾µ\")\n",
    "    embeddings = X_final.values\n",
    "\n",
    "elif VEC_CHOICE == \"sbert\":\n",
    "    emb_model = SentenceTransformer(\"sentence-transformers/paraphrase-MiniLM-L6-v2\")\n",
    "    embeddings = emb_model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"VEC_CHOICE åƒ…èƒ½ç‚º 'tfidf' / 'tfidf_style' / 'sbert'\")\n",
    "\n",
    "# -------- 2. å»ºç«‹ BERTopic æ¨¡å‹ ----------------------------------\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=None if VEC_CHOICE.startswith(\"tfidf\") else emb_model,\n",
    "    hdbscan_model=HDBSCAN(min_cluster_size=10, min_samples=30),\n",
    "    vectorizer_model=CountVectorizer(ngram_range=(1, 2), stop_words=\"english\"),\n",
    "    calculate_probabilities=False,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "topics, _ = topic_model.fit_transform(texts, embeddings)\n",
    "data['topic'] = topics\n",
    "\n",
    "# âœ… é˜²å‘†ï¼šç¢ºèªæ˜¯å¦æœ‰æœ‰æ•ˆä¸»é¡Œï¼ˆé -1ï¼‰\n",
    "valid_topics = [t for t in set(topics) if t != -1]\n",
    "if len(valid_topics) == 0:\n",
    "    print(\"âš ï¸ ç„¡æœ‰æ•ˆä¸»é¡Œï¼ˆå…¨éƒ¨ç‚º outlierï¼‰ï¼Œè«‹æª¢æŸ¥è³‡æ–™ç­†æ•¸æˆ–é™ä½ min_cluster_size è¨­å®šã€‚\")\n",
    "else:\n",
    "    # -------- 3. ä¸»é¡Œ Ã— çœŸï¼å‡ åˆ†ä½ˆ -------------------------------\n",
    "    data['label_name'] = data['label'].map({0: \"True\", 1: \"Fake\"})\n",
    "    topic_dist = (data.groupby(['topic', 'label_name']).size().unstack(fill_value=0))\n",
    "    topic_dist['Total'] = topic_dist.sum(axis=1)\n",
    "    topic_dist['Fake_Ratio'] = topic_dist['Fake'] / topic_dist['Total']\n",
    "\n",
    "    print(\"â–¶ å„ Topic çœŸï¼å‡ç­†æ•¸èˆ‡å‡æ–°èæ¯”ä¾‹ (å‰ 10)ï¼š\")\n",
    "    display(topic_dist.sort_values('Fake_Ratio', ascending=False).head(10))\n",
    "\n",
    "    # -------- 4. å–ä¸»é¡Œé—œéµå­—ä¸¦ä¾çœŸå‡æ¯”ä¾‹æ’åº ----------------------\n",
    "    kw_rows = []\n",
    "    for tid, word_scores in topic_model.get_topics().items():\n",
    "        if tid == -1:\n",
    "            continue\n",
    "        for word, score in word_scores:\n",
    "            kw_rows.append({\"topic\": tid, \"word\": word, \"c_tf_idf\": score})\n",
    "\n",
    "    kw_df = pd.DataFrame(kw_rows)\n",
    "\n",
    "    merged_kw = kw_df.merge(topic_dist.reset_index(), on=\"topic\")\n",
    "\n",
    "    fake_top_kw = (merged_kw.sort_values(['Fake_Ratio', 'c_tf_idf'], ascending=[False, False])\n",
    "                            .groupby('topic')\n",
    "                            .head(30))\n",
    "\n",
    "    true_top_kw = (merged_kw.sort_values(['Fake_Ratio', 'c_tf_idf'], ascending=[True, False])\n",
    "                            .groupby('topic')\n",
    "                            .head(30))\n",
    "\n",
    "    print(\"\\nğŸŸ¥ å‡æ–°èé«˜æ¯”ä¾‹ä¸»é¡Œé—œéµå­— TOP 30\")\n",
    "    display(fake_top_kw[['topic', 'word', 'c_tf_idf', 'Fake_Ratio']])\n",
    "\n",
    "    print(\"\\nğŸŸ¦ çœŸæ–°èé«˜æ¯”ä¾‹ä¸»é¡Œé—œéµå­— TOP 30\")\n",
    "    display(true_top_kw[['topic', 'word', 'c_tf_idf', 'Fake_Ratio']])\n",
    "\n",
    "    # -------- 5. è¦–è¦ºåŒ–æ¯å€‹ä¸»é¡Œçš„å‡æ–°èæ¯”ä¾‹ ------------------------\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    sns.barplot(x=topic_dist.index, y=topic_dist['Fake_Ratio'], palette=\"coolwarm\")\n",
    "    plt.title(\"Fake-News Ratio per Topic\")\n",
    "    plt.ylabel(\"Fake Ratio\")\n",
    "    plt.xlabel(\"Topic ID\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "osakI7pZsZuG",
   "metadata": {
    "id": "osakI7pZsZuG"
   },
   "source": [
    "ã€Œå…ˆç”¨æœ€ä½³å‘é‡åŒ–(SBERT)â†’BERTopicåˆ†ç¾¤â†’ç–ŠåŠ çœŸå‡æ¨™ç±¤â†’çœ‹æ¯å€‹ä¸»é¡Œå“ªé‚Šå‡æ–°èé«˜ã€å“ªé‚ŠçœŸæ–°èé«˜ï¼Œä»¥åŠå°æ‡‰é—œéµè©çš„å…¨æµç¨‹ã€‚\n",
    "\n",
    "å°‡å…©è€…ç–ŠåŠ ï¼Œå°±èƒ½å¾—åˆ°ï¼š\n",
    "ã€Œå‡æ–°èæœ€å¸¸è¦‹çš„ä¸»é¡Œæœ‰å“ªäº›ï¼Ÿã€/ã€ŒçœŸæ–°èè£¡å“ªäº›ä¸»é¡Œç‰¹åˆ¥çªå‡ºï¼Ÿã€/ã€Œå„ä¸»é¡Œçš„ä»£è¡¨é—œéµè©ã€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19be7f71",
   "metadata": {
    "id": "19be7f71"
   },
   "outputs": [],
   "source": [
    "# '''from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# # çœŸå‡æ–°èé€²è¡Œä¸»é¡Œå»ºæ¨¡\n",
    "# docs = data['text'].astype(str).tolist()\n",
    "\n",
    "# # æ¨¡å‹å¯æ›æˆ 'all-MiniLM-L6-v2', 'microsoft/Phi-4-mini-instruct' ç­‰\n",
    "# embedding_model = 'all-MiniLM-L6-v2'\n",
    "\n",
    "# # å¯èª¿æ•´ æ¸¬è©¦ç”¨2000ç­†\n",
    "# # min_cluster_size ç¾¤é›†æœ€å°‘éœ€è¦åŒ…å«nå€‹é»ï¼Œå¦å‰‡æœƒè¢«è¦–ç‚ºé›œè¨Šï¼ˆnoiseï¼‰\n",
    "# # min_samples åŒ…å«è‡³å°‘nç¯‡æ–‡ç« çš„ä¸»é¡Œæ‰æœƒè¢«æ‰¿èªç‚ºä¸»é¡Œ\n",
    "# hdbscan_model = HDBSCAN(min_cluster_size=10, min_samples=30) # Clustering layer\n",
    "# vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\")\n",
    "\n",
    "# topic_model = BERTopic(embedding_model=embedding_model, hdbscan_model=hdbscan_model, vectorizer_model=vectorizer_model)\n",
    "# topics, probs = topic_model.fit_transform(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3442db2",
   "metadata": {
    "id": "c3442db2"
   },
   "outputs": [],
   "source": [
    "# '''# å»ºç«‹ä¸€å€‹å„²å­˜æ‰€æœ‰ä¸»é¡Œé—œéµè©èˆ‡ TF-IDF åˆ†æ•¸çš„æ¸…å–®\n",
    "# all_topics = []\n",
    "\n",
    "# # æŠŠä¸»é¡Œç¸½æ•¸æ‹¿å‡ºä¾†ï¼ˆæ’é™¤ -1 æ˜¯æœªåˆ†é¡ä¸»é¡Œï¼‰\n",
    "# valid_topics = [topic for topic in topic_model.get_topic_info().Topic if topic != -1]\n",
    "\n",
    "# # å°æ¯å€‹ä¸»é¡Œå–å¾—è©èˆ‡ c-TF-IDF åˆ†æ•¸\n",
    "# for topic_id in valid_topics:\n",
    "#     topic_words = topic_model.get_topic(topic_id)\n",
    "#     for word, score in topic_words:\n",
    "#         all_topics.append({\n",
    "#             \"Topic\": topic_id,\n",
    "#             \"Word\": word,\n",
    "#             \"C-TF-IDF\": score\n",
    "#         })\n",
    "\n",
    "# # è½‰æ›æˆ DataFrame ä¸¦æ’åº\n",
    "# topic_tfidf_df = pd.DataFrame(all_topics)\n",
    "# topic_tfidf_df = topic_tfidf_df.sort_values(by=[\"Topic\", \"C-TF-IDF\"], ascending=[True, False])\n",
    "\n",
    "# # é¡¯ç¤ºå‰å¹¾åˆ—\n",
    "# topic_tfidf_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf36076",
   "metadata": {
    "id": "aaf36076"
   },
   "outputs": [],
   "source": [
    "\n",
    "# # åˆ—å‡ºæ–‡ç« çš„BERTopicè³‡è¨Š\n",
    "# topic_model.get_document_info(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4088184f",
   "metadata": {
    "id": "4088184f"
   },
   "outputs": [],
   "source": [
    "# def visualize_fake_news_ratio_by_topic(model, docs, labels, title=\"ä¸»é¡Œçš„å‡æ–°èæ¯”ä¾‹\"):\n",
    "#     doc_info = model.get_document_info(docs).copy()\n",
    "#     doc_info['label'] = labels\n",
    "\n",
    "#     # è¨ˆç®—æ¯”ä¾‹èˆ‡æ•¸é‡\n",
    "#     topic_fake_ratio = (\n",
    "#         doc_info[doc_info['Topic'] != -1]\n",
    "#         .groupby('Topic')['label']\n",
    "#         .mean()\n",
    "#         .reset_index()\n",
    "#         .rename(columns={'label': 'fake_news_ratio'})\n",
    "#     )\n",
    "#     topic_counts = (\n",
    "#         doc_info[doc_info['Topic'] != -1]['Topic']\n",
    "#         .value_counts()\n",
    "#         .rename_axis('Topic')\n",
    "#         .reset_index(name='count')\n",
    "#     )\n",
    "#     topic_stats = pd.merge(topic_fake_ratio, topic_counts, on='Topic')\n",
    "\n",
    "#     # åŠ ä¸Šä¸»é¡Œåç¨±\n",
    "#     topic_names = model.get_topic_info()[['Topic', 'Name']]\n",
    "#     topic_stats_named = topic_stats.merge(topic_names, on='Topic')\n",
    "\n",
    "#     # éæ¿¾æ¯”ä¾‹éä½çš„ä¸»é¡Œ\n",
    "#     topic_stats_named = topic_stats_named[topic_stats_named['fake_news_ratio'] >= 0.1]\n",
    "\n",
    "#     # ç¹ªåœ–\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     ax = sns.barplot(\n",
    "#         data=topic_stats_named.sort_values(by='fake_news_ratio', ascending=False),\n",
    "#         x='fake_news_ratio', y='Name', palette='Reds'\n",
    "#     )\n",
    "#     plt.title(title)\n",
    "#     plt.xlabel('å‡æ–°èæ¯”ä¾‹ (label=1)')\n",
    "#     plt.ylabel('ä¸»é¡Œä»£è¡¨è©')\n",
    "#     plt.grid(True, axis='x')\n",
    "#     ax.set_yticklabels(ax.get_yticklabels(), fontsize=9)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55d4a85",
   "metadata": {
    "id": "b55d4a85"
   },
   "source": [
    "### representation topic model: åŠ ä¸Šèªæ„å°å‘çš„KeyBERT, è¡¨ç¾æ–¹å¼æ˜¯èªæ„å‘é‡ç›¸ä¼¼çš„è©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281f408c",
   "metadata": {
    "id": "281f408c"
   },
   "outputs": [],
   "source": [
    "# from bertopic.representation import KeyBERTInspired\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# embedding_model_with_st = SentenceTransformer(embedding_model)  # æˆ–å…¶ä»–ä½ æŒ‡å®šçš„æ¨¡å‹\n",
    "# embeddings = embedding_model_with_st.encode(docs, show_progress_bar=True)\n",
    "\n",
    "# # é—œéµè©è¡¨ç¤ºæ¨¡å‹ï¼ˆéç”Ÿæˆå¼ï¼‰\n",
    "# keybert = KeyBERTInspired()\n",
    "\n",
    "# # çµ„è£ representation model\n",
    "# representation_model = {\n",
    "#     \"KeyBERT\": keybert\n",
    "# }\n",
    "\n",
    "# # å»ºç«‹ BERTopic æ¨¡å‹ï¼ˆç”¨ KeyBERT èª¿æ•´ä¸»é¡Œè¡¨ç¤ºï¼‰\n",
    "# representation_topic_model = BERTopic(\n",
    "#     embedding_model=embedding_model_with_st,\n",
    "#     vectorizer_model=vectorizer_model,\n",
    "#     hdbscan_model=hdbscan_model,\n",
    "#     representation_model=representation_model,\n",
    "#     top_n_words=30,\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# # è¨“ç·´æ¨¡å‹\n",
    "# topics, probs = representation_topic_model.fit_transform(docs, embeddings)\n",
    "\n",
    "# # æŸ¥çœ‹æ–°çš„ä¸»é¡Œè¡¨ç¤º\n",
    "# representation_topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f35ca8",
   "metadata": {
    "id": "f5f35ca8"
   },
   "outputs": [],
   "source": [
    "# # è¦–è¦ºåŒ–ä¸»é¡Œåˆ†å¸ƒï¼šåœ“åœˆå¤§å°æ˜¯ä¸»é¡Œçš„å¤§å°ï¼Œåœ“åœˆçš„è·é›¢æ˜¯ä¸»é¡Œä¹‹é–“çš„ç›¸ä¼¼åº¦\n",
    "# topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c52eb8b",
   "metadata": {
    "id": "4c52eb8b"
   },
   "outputs": [],
   "source": [
    "# representation_topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de5782e",
   "metadata": {
    "id": "9de5782e"
   },
   "outputs": [],
   "source": [
    "# # åŸå§‹æ¨¡å‹çš„ä¸»é¡Œ\n",
    "# visualize_fake_news_ratio_by_topic(topic_model, docs, data['label'], title=\"åŸå§‹ä¸»é¡Œçš„å‡æ–°èæ¯”ä¾‹\")\n",
    "\n",
    "# # ä½¿ç”¨ KeyBERT è¡¨ç¤ºè©çš„æ¨¡å‹ä¸»é¡Œ\n",
    "# visualize_fake_news_ratio_by_topic(representation_topic_model, docs, data['label'], title=\"KeyBERT ä¸»é¡Œçš„å‡æ–°èæ¯”ä¾‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5762b28",
   "metadata": {},
   "source": [
    "## LLM ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a3a756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama  # ä½¿ç”¨ Ollama å°è£çš„ LLaMA æ¨¡å‹\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# å®šç¾©è¼¸å‡ºçµæ§‹\n",
    "class MessageClassification(BaseModel):\n",
    "    verdict: str = Field(description=\"Verdict whether the message is Real or Fake\")\n",
    "    confidence: str = Field(description=\"Confidence level of the judgment (e.g., High, Medium, Low)\")\n",
    "    reason: str = Field(description=\"Brief explanation of the judgment\")\n",
    "\n",
    "# ä½¿ç”¨æœ¬åœ° LLaMA æ¨¡å‹\n",
    "judge_llm = ChatOllama(model=\"llama3:8B\")\n",
    "logic_llm = ChatOllama(model=\"phi3:3.8B\")\n",
    "debater_llm = ChatOllama(model=\"mistral:7B\")\n",
    "\n",
    "# Json è¼¸å‡ºæ ¼å¼è§£æå™¨\n",
    "parser = JsonOutputParser(pydantic_object=MessageClassification)\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "\n",
    "# å–®ä¸€ LLM æ¨ç†çš„ Prompt\n",
    "llm_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are a professional fact-checker. Analyze the following message and determine if it is real or fake.\n",
    "\n",
    "Message:\n",
    "\\\"\\\"\\\"{message}\\\"\\\"\\\"\n",
    "\n",
    "Fill in this exact JSON format (no extra text!):\n",
    "\n",
    "{{\n",
    "  \"verdict\": \"\",        // \"Real\" or \"Fake\"\n",
    "  \"confidence\": \"\",     // \"High\", \"Medium\", or \"Low\"\n",
    "  \"reason\": \"\"          // Short explanation (1-2 sentences)\n",
    "}}\n",
    "\n",
    "Remember:\n",
    "- DO NOT add anything outside the JSON.\n",
    "- DO NOT wrap it in markdown (e.g., ```json).\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# è®“ judge_llm åŒ¯ç¸½æ‰€æœ‰æ¨¡å‹è§€é»çš„ Prompt\n",
    "summary_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are the final arbiter. Three experts have evaluated the message. Please summarize their opinions and give your final decision.\n",
    "\n",
    "Message:\n",
    "\\\"\\\"\\\"{message}\\\"\\\"\\\"\n",
    "\n",
    "Expert 1 (Logic-focused model):\n",
    "{logic_opinion}\n",
    "\n",
    "Expert 2 (Debate-focused model):\n",
    "{debate_opinion}\n",
    "\n",
    "Expert 3 (Your own opinion):\n",
    "{your_opinion}\n",
    "\n",
    "Now summarize the opinions, resolve any conflicts, and provide a final classification in this JSON format:\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a57b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import re\n",
    "import json\n",
    "\n",
    "def extract_json(text: str) -> dict:\n",
    "    try:\n",
    "        \n",
    "        # æ‰¾å‡ºç¬¬ä¸€çµ„çµæ§‹ç‚º { ... } çš„JSONå€å¡Š\n",
    "        match = re.search(r'{[\\s\\S]*?}', text)\n",
    "        if not match:\n",
    "            raise ValueError(\"No valid JSON object found in output.\")\n",
    "        json_str = match.group()\n",
    "        return json.loads(json_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"\\n JSON è§£æå¤±æ•—ï¼š{e}\")\n",
    "        print(\"åŸå§‹è¼¸å‡ºï¼š\", text)\n",
    "        return {\n",
    "            \"verdict\": \"Unknown\",\n",
    "            \"confidence\": \"Low\",\n",
    "            \"reason\": \"Model did not return valid JSON.\"\n",
    "        }\n",
    "\n",
    "def call_llm(llm, prompt):\n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content if hasattr(response, \"content\") else response\n",
    "\n",
    "# å®šç¾©åˆ†æå‡½å¼\n",
    "def analyze_message_with_multi_llm(message: str):\n",
    "    logic_input = llm_prompt.format(message=message, format_instructions=format_instructions)\n",
    "    debate_input = llm_prompt.format(message=message, format_instructions=format_instructions)\n",
    "    judge_input = llm_prompt.format(message=message, format_instructions=format_instructions)\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = {\n",
    "            executor.submit(call_llm, logic_llm, logic_input): \"logic\",\n",
    "            executor.submit(call_llm, debater_llm, debate_input): \"debate\",\n",
    "            executor.submit(call_llm, judge_llm, judge_input): \"judge\"\n",
    "        }\n",
    "        results = {}\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            key = futures[future]\n",
    "            results[key] = future.result()\n",
    "\n",
    "    summary_input = summary_prompt.format(\n",
    "        message=message,\n",
    "        logic_opinion=results[\"logic\"],\n",
    "        debate_opinion=results[\"debate\"],\n",
    "        your_opinion=results[\"judge\"],\n",
    "        format_instructions=format_instructions\n",
    "    )\n",
    "\n",
    "    final_response = judge_llm.invoke(summary_input)\n",
    "    result = extract_json(final_response.content)\n",
    "    return result\n",
    "\n",
    "def encode_verdict(verdict: str) -> int:\n",
    "    return 1 if verdict.strip().lower() == 'real' else 0\n",
    "\n",
    "def encode_confidence(conf: str) -> int:\n",
    "    mapping = {'low': 0, 'medium': 1, 'high': 2}\n",
    "    return mapping.get(conf.strip().lower(), 1)  # é è¨­çµ¦ä¿¡å¿ƒç¨‹åº¦1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4378c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# # è§€å¯Ÿæ¸¬è©¦ç”¨!!!\n",
    "# sample_texts = data['text'].sample(10, random_state=42)\n",
    "\n",
    "# for i, text in enumerate(sample_texts):\n",
    "#     result = analyze_message_with_multi_llm(text)\n",
    "#     print(\"æ¨è«–çµæœï¼š\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569cba3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-LLM\n",
    "llm_results = data['text'].progress_apply(analyze_message_with_multi_llm)\n",
    "llm_df = pd.DataFrame(llm_results.tolist())\n",
    "\n",
    "# encode\n",
    "llm_df['verdict_encoded'] = llm_df['verdict'].apply(encode_verdict)\n",
    "llm_df['confidence_encoded'] = llm_df['confidence'].apply(encode_confidence)\n",
    "\n",
    "X_final = pd.concat([X_final, llm_df[['verdict_encoded', 'confidence_encoded']]], axis=1)\n",
    "y_final = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf8bfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# classifier models\n",
    "classifier_model = {\n",
    "    \"LogReg\"       : LogisticRegression(max_iter=1000),\n",
    "    \"DecisionTree\" : DecisionTreeClassifier(),\n",
    "    \"SVM\"          : SVC(kernel='linear', probability=True),\n",
    "    \"RandomForest\" : RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'f1': make_scorer(f1_score),\n",
    "    'precision': make_scorer(precision_score),\n",
    "    'recall': make_scorer(recall_score)\n",
    "}\n",
    "\n",
    "# === init result ===\n",
    "results = []\n",
    "\n",
    "# === å»ºç«‹ Stratified K-Fold ===\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# === è¨“ç·´æ¯å€‹æ¨¡å‹ ===\n",
    "for name, model in classifier_model.items():\n",
    "    print(f\"\\n è¨“ç·´æ¨¡å‹: {name}\")\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # å°æ‰€æœ‰ç‰¹å¾µæ¨™æº–åŒ–\n",
    "        ('clf', model)\n",
    "    ])\n",
    "    scores = cross_validate(pipeline, X_final, y, cv=cv, scoring=scoring)\n",
    "    result = {\n",
    "        'model': name,\n",
    "        'accuracy': np.mean(scores['test_accuracy']),\n",
    "        'f1': np.mean(scores['test_f1']),\n",
    "        'precision': np.mean(scores['test_precision']),\n",
    "        'recall': np.mean(scores['test_recall'])\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "# === æ•´ç†æˆ DataFrame é¡¯ç¤º ===\n",
    "result_df = pd.DataFrame(results)\n",
    "print(\"\\nå„æ¨¡å‹è©•ä¼°çµæœï¼š\")\n",
    "print(result_df.sort_values(by='f1', ascending=False))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07b0a05b872c48ec8bc00ec063304866": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1bd81346eb5446ae86c8992ea87a8979": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3aa0234db1f4447da06ae0ba78593276",
       "IPY_MODEL_aaefd6793ed644b3afbf183bec859bff",
       "IPY_MODEL_e108b8551f0e41b69ce386e7cbdf3945"
      ],
      "layout": "IPY_MODEL_9c0fc22fc33a4388b5cde67492782329"
     }
    },
    "2ff3e28618eb412c9f1879564137be6c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30cfa6df7235440a8aaf12b38e7c1a24": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3aa0234db1f4447da06ae0ba78593276": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a7bd128141e746019bbd9eeed16eda0c",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c9da9c06c90a475ab7e45abc95f3dec3",
      "value": "Batches:â€‡100%"
     }
    },
    "953dd787d72045bc9382fef65869ced1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c0fc22fc33a4388b5cde67492782329": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7bd128141e746019bbd9eeed16eda0c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aaefd6793ed644b3afbf183bec859bff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_953dd787d72045bc9382fef65869ced1",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_30cfa6df7235440a8aaf12b38e7c1a24",
      "value": 5
     }
    },
    "c9da9c06c90a475ab7e45abc95f3dec3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e108b8551f0e41b69ce386e7cbdf3945": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2ff3e28618eb412c9f1879564137be6c",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_07b0a05b872c48ec8bc00ec063304866",
      "value": "â€‡5/5â€‡[00:09&lt;00:00,â€‡â€‡1.28s/it]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
